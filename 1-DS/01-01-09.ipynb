{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48ad1db6",
   "metadata": {},
   "source": [
    "# Handling Missing Values in Data Science\n",
    "\n",
    "Missing values are a common challenge in real-world datasets. This notebook explores techniques for detecting, visualizing, and handling missing data in pandas DataFrames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782c69f",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll need several libraries for handling and visualizing missing data:\n",
    "- pandas and numpy for data manipulation\n",
    "- matplotlib and seaborn for standard visualization\n",
    "- missingno for specialized missing data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599026a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msno\n",
    "\n",
    "# Set visualization styles\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Display all dataframe columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2d4ede",
   "metadata": {},
   "source": [
    "## 2. Understanding Missing Values\n",
    "\n",
    "Missing values in datasets can exist for various reasons:\n",
    "\n",
    "1. **MCAR (Missing Completely At Random)**: The missing data has no relationship with any values, observed or missing. This is the ideal case for handling missing data.\n",
    "\n",
    "2. **MAR (Missing At Random)**: The missingness can be explained by other observed variables in the dataset.\n",
    "\n",
    "3. **MNAR (Missing Not At Random)**: The missingness depends on information that hasn't been recorded.\n",
    "\n",
    "In pandas, missing values are represented as:\n",
    "- `NaN` (Not a Number): The standard representation for missing numerical data\n",
    "- `None`: A Python object often converted to NaN\n",
    "- `NaT` (Not a Time): Used for missing datetime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1713c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a sample dataset with different types of missing values\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create a sample dataset\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, 1001),\n",
    "    'age': np.random.normal(35, 10, 1000),\n",
    "    'income': np.random.normal(50000, 15000, 1000),\n",
    "    'gender': np.random.choice(['M', 'F'], 1000),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 1000),\n",
    "    'satisfaction': np.random.choice([1, 2, 3, 4, 5], 1000),\n",
    "    'join_date': pd.date_range(start='2020-01-01', periods=1000)\n",
    "})\n",
    "\n",
    "# Introduce missing values of different types\n",
    "# MCAR - completely random\n",
    "df.loc[np.random.choice(df.index, 100, replace=False), 'age'] = np.nan\n",
    "\n",
    "# MAR - missing based on observed data (e.g., missing income more likely for younger people)\n",
    "young_indices = df[df['age'] < 30].index\n",
    "df.loc[np.random.choice(young_indices, int(len(young_indices) * 0.3), replace=False), 'income'] = np.nan\n",
    "\n",
    "# MNAR - missing not at random (e.g., high income people not reporting satisfaction)\n",
    "high_income_indices = df[df['income'] > 65000].index\n",
    "df.loc[np.random.choice(high_income_indices, int(len(high_income_indices) * 0.4), replace=False), 'satisfaction'] = np.nan\n",
    "\n",
    "# Random missing values in categorical columns\n",
    "df.loc[np.random.choice(df.index, 50, replace=False), 'gender'] = None\n",
    "df.loc[np.random.choice(df.index, 80, replace=False), 'education'] = None\n",
    "\n",
    "# Missing dates\n",
    "df.loc[np.random.choice(df.index, 70, replace=False), 'join_date'] = pd.NaT\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8f10a4",
   "metadata": {},
   "source": [
    "## 3. Detecting Missing Values\n",
    "\n",
    "There are several ways to identify missing values in a pandas DataFrame:\n",
    "\n",
    "- `df.isnull()` or `df.isna()`: Returns a boolean mask where True indicates missing values\n",
    "- `df.isnull().sum()`: Counts missing values in each column\n",
    "- `df.info()`: Shows data types and non-null counts\n",
    "- Calculating percentage of missing values per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0576df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and non-null counts\n",
    "print(\"DataFrame Information:\")\n",
    "df.info()\n",
    "\n",
    "# Get the count of missing values in each column\n",
    "print(\"\\nMissing Value Counts:\")\n",
    "missing_counts = df.isnull().sum()\n",
    "print(missing_counts)\n",
    "\n",
    "# Calculate percentage of missing values\n",
    "print(\"\\nPercentage of Missing Values:\")\n",
    "missing_percentage = (missing_counts / len(df)) * 100\n",
    "missing_summary = pd.DataFrame({\n",
    "    'Missing Count': missing_counts,\n",
    "    'Missing Percentage': missing_percentage\n",
    "})\n",
    "print(missing_summary)\n",
    "\n",
    "# Check if any rows have all values missing\n",
    "print(f\"\\nRows with all values missing: {df.isnull().all(axis=1).sum()}\")\n",
    "\n",
    "# Check if any rows are complete (no missing values)\n",
    "print(f\"Complete rows (no missing values): {(~df.isnull().any(axis=1)).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d6e82d",
   "metadata": {},
   "source": [
    "## 4. Visualizing Missing Data\n",
    "\n",
    "Visualizing missing data patterns can help us better understand:\n",
    "- The extent of missing data\n",
    "- Potential relationships between missing values\n",
    "- Which columns or rows are most affected\n",
    "\n",
    "We'll use several visualization techniques:\n",
    "1. Heatmap of missing values\n",
    "2. Bar chart showing count of missing values per column\n",
    "3. Matrix plot from the missingno package\n",
    "4. Dendrogram to identify relationships between missing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b498b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Heatmap of missing values (for a sample of rows)\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.iloc[:100].isnull(), cmap='viridis', cbar=False)\n",
    "plt.title('Missing Value Heatmap (First 100 rows)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Bar chart of missing value counts\n",
    "plt.figure(figsize=(10, 6))\n",
    "missing_counts.plot(kind='bar')\n",
    "plt.title('Missing Value Count by Column')\n",
    "plt.xlabel('Columns')\n",
    "plt.ylabel('Missing Value Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Missing value matrix with missingno\n",
    "msno.matrix(df, figsize=(12, 8))\n",
    "plt.title('Missing Value Matrix')\n",
    "plt.show()\n",
    "\n",
    "# 4. Missing value correlation heatmap\n",
    "msno.heatmap(df, figsize=(10, 8))\n",
    "plt.title('Missing Value Correlation Heatmap')\n",
    "plt.show()\n",
    "\n",
    "# 5. Missing value dendrogram to identify patterns\n",
    "msno.dendrogram(df, figsize=(12, 8))\n",
    "plt.title('Missing Value Dendrogram')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df891950",
   "metadata": {},
   "source": [
    "## 5. Handling Missing Values\n",
    "\n",
    "There are several strategies for handling missing values:\n",
    "\n",
    "1. **Deletion**:\n",
    "   - Drop rows with missing values\n",
    "   - Drop columns with too many missing values\n",
    "\n",
    "2. **Retention**:\n",
    "   - Keep missing values (some algorithms can handle them directly)\n",
    "   - Mark missing values with a special category or flag\n",
    "\n",
    "3. **Imputation**:\n",
    "   - Fill missing values with calculated or logical values\n",
    "\n",
    "Let's start with simple deletion and filtering approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84ea090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the dataframe to preserve the original\n",
    "df_handled = df.copy()\n",
    "\n",
    "# 1. Drop rows with missing values\n",
    "df_drop_rows = df.dropna()\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"Shape after dropping rows with missing values: {df_drop_rows.shape}\")\n",
    "print(f\"Rows dropped: {df.shape[0] - df_drop_rows.shape[0]}\")\n",
    "\n",
    "# 2. Drop rows with a threshold of missing values\n",
    "df_drop_thresh = df.dropna(thresh=df.shape[1] - 2)  # Drop rows missing more than 2 values\n",
    "print(f\"Shape after dropping rows missing more than 2 values: {df_drop_thresh.shape}\")\n",
    "\n",
    "# 3. Drop columns with more than 10% missing values\n",
    "cols_to_drop = missing_percentage[missing_percentage > 10].index\n",
    "df_drop_cols = df.drop(columns=cols_to_drop)\n",
    "print(f\"Shape after dropping columns with >10% missing values: {df_drop_cols.shape}\")\n",
    "print(f\"Columns dropped: {list(cols_to_drop)}\")\n",
    "\n",
    "# 4. Drop specific columns\n",
    "# df_no_satisfaction = df.drop(columns=['satisfaction'])\n",
    "# print(f\"Shape after dropping 'satisfaction' column: {df_no_satisfaction.shape}\")\n",
    "\n",
    "# 5. Filtering data based on non-missing values in specific columns\n",
    "df_important_cols = df.dropna(subset=['age', 'income'])\n",
    "print(f\"Shape after keeping only rows with non-missing age and income: {df_important_cols.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a5070c",
   "metadata": {},
   "source": [
    "## 6. Imputation Techniques\n",
    "\n",
    "Imputation involves replacing missing values with substituted values. Here are some common techniques:\n",
    "\n",
    "1. **Statistical Imputation**:\n",
    "   - Mean, median, or mode imputation\n",
    "   - Constant value imputation\n",
    "\n",
    "2. **Positional Imputation**:\n",
    "   - Forward fill (carry last valid observation forward)\n",
    "   - Backward fill (use next valid observation)\n",
    "   \n",
    "3. **Group-based Imputation**:\n",
    "   - Impute based on group statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838c5d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies to compare different imputation methods\n",
    "df_mean = df.copy()\n",
    "df_median = df.copy()\n",
    "df_mode = df.copy()\n",
    "df_constant = df.copy()\n",
    "df_ffill = df.copy()\n",
    "df_bfill = df.copy()\n",
    "df_group = df.copy()\n",
    "\n",
    "# 1. Mean imputation for numerical columns\n",
    "df_mean['age'] = df_mean['age'].fillna(df_mean['age'].mean())\n",
    "df_mean['income'] = df_mean['income'].fillna(df_mean['income'].mean())\n",
    "df_mean['satisfaction'] = df_mean['satisfaction'].fillna(df_mean['satisfaction'].mean())\n",
    "\n",
    "# 2. Median imputation for numerical columns\n",
    "df_median['age'] = df_median['age'].fillna(df_median['age'].median())\n",
    "df_median['income'] = df_median['income'].fillna(df_median['income'].median())\n",
    "df_median['satisfaction'] = df_median['satisfaction'].fillna(df_median['satisfaction'].median())\n",
    "\n",
    "# 3. Mode imputation for categorical columns\n",
    "df_mode['gender'] = df_mode['gender'].fillna(df_mode['gender'].mode()[0])\n",
    "df_mode['education'] = df_mode['education'].fillna(df_mode['education'].mode()[0])\n",
    "\n",
    "# 4. Constant value imputation\n",
    "df_constant['age'] = df_constant['age'].fillna(0)\n",
    "df_constant['gender'] = df_constant['gender'].fillna('Unknown')\n",
    "df_constant['satisfaction'] = df_constant['satisfaction'].fillna(0)\n",
    "\n",
    "# 5. Forward fill (use previous value)\n",
    "df_ffill = df_ffill.sort_values('id').fillna(method='ffill')\n",
    "\n",
    "# 6. Backward fill (use next value)\n",
    "df_bfill = df_bfill.sort_values('id').fillna(method='bfill')\n",
    "\n",
    "# 7. Group-based imputation (e.g., fill income based on education level)\n",
    "education_groups = df.groupby('education')['income'].transform('mean')\n",
    "df_group['income'] = df_group['income'].fillna(education_groups)\n",
    "\n",
    "# Compare results of different imputation methods for the income column\n",
    "imputation_comparison = pd.DataFrame({\n",
    "    'Original': df['income'],\n",
    "    'Mean Imputation': df_mean['income'],\n",
    "    'Median Imputation': df_median['income'],\n",
    "    'Forward Fill': df_ffill['income'],\n",
    "    'Backward Fill': df_bfill['income'],\n",
    "    'Group-based': df_group['income']\n",
    "})\n",
    "\n",
    "# Check descriptive statistics of the imputed columns\n",
    "print(\"Descriptive statistics of the imputed income column:\")\n",
    "imputation_comparison.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of different imputation methods on the distribution\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Original distribution (excluding NaNs)\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.histplot(df['income'].dropna(), kde=True)\n",
    "plt.title(\"Original Income Distribution (excluding NaNs)\")\n",
    "\n",
    "# Mean imputation\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.histplot(df_mean['income'], kde=True)\n",
    "plt.title(\"Mean Imputation\")\n",
    "\n",
    "# Median imputation\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.histplot(df_median['income'], kde=True)\n",
    "plt.title(\"Median Imputation\")\n",
    "\n",
    "# Forward fill\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.histplot(df_ffill['income'], kde=True)\n",
    "plt.title(\"Forward Fill\")\n",
    "\n",
    "# Backward fill\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.histplot(df_bfill['income'], kde=True)\n",
    "plt.title(\"Backward Fill\")\n",
    "\n",
    "# Group-based imputation\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.histplot(df_group['income'], kde=True)\n",
    "plt.title(\"Group-based Imputation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892c5521",
   "metadata": {},
   "source": [
    "## 7. Advanced Imputation Methods\n",
    "\n",
    "For more sophisticated imputation, we can use:\n",
    "\n",
    "1. **KNN Imputation**: Imputes values based on k-nearest neighbors\n",
    "2. **Iterative Imputation**: Uses relationships between features to predict missing values\n",
    "3. **MICE (Multiple Imputation by Chained Equations)**: Creates multiple imputations based on the patterns in the data\n",
    "\n",
    "These methods are available in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225d499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer  # needed for IterativeImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create a copy of the dataframe\n",
    "df_advanced = df.copy()\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_cols = ['age', 'income', 'satisfaction']\n",
    "categorical_cols = ['gender', 'education']\n",
    "\n",
    "# Create categorical variables for non-missing data (for visualization)\n",
    "df_num = df[numerical_cols].copy()\n",
    "\n",
    "# 1. Simple Imputer (Mean strategy)\n",
    "simple_imputer = SimpleImputer(strategy='mean')\n",
    "df_simple_imputed = pd.DataFrame(\n",
    "    simple_imputer.fit_transform(df_num),\n",
    "    columns=numerical_cols\n",
    ")\n",
    "\n",
    "# 2. KNN Imputer\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df_knn_imputed = pd.DataFrame(\n",
    "    knn_imputer.fit_transform(df_num),\n",
    "    columns=numerical_cols\n",
    ")\n",
    "\n",
    "# 3. Iterative Imputer (Multivariate imputation)\n",
    "iterative_imputer = IterativeImputer(max_iter=10, random_state=42)\n",
    "df_iterative_imputed = pd.DataFrame(\n",
    "    iterative_imputer.fit_transform(df_num),\n",
    "    columns=numerical_cols\n",
    ")\n",
    "\n",
    "# Compare results for the income column\n",
    "imputation_comparison_advanced = pd.DataFrame({\n",
    "    'Original': df['income'],\n",
    "    'Simple Imputer (Mean)': df_simple_imputed['income'],\n",
    "    'KNN Imputer': df_knn_imputed['income'],\n",
    "    'Iterative Imputer': df_iterative_imputed['income']\n",
    "})\n",
    "\n",
    "# Visualize the effect of different advanced imputation methods\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Original distribution (excluding NaNs)\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(df['income'].dropna(), kde=True)\n",
    "plt.title(\"Original Income Distribution (excluding NaNs)\")\n",
    "\n",
    "# Simple Imputer (Mean)\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(df_simple_imputed['income'], kde=True)\n",
    "plt.title(\"Simple Imputer (Mean)\")\n",
    "\n",
    "# KNN Imputer\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(df_knn_imputed['income'], kde=True)\n",
    "plt.title(\"KNN Imputer\")\n",
    "\n",
    "# Iterative Imputer\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(df_iterative_imputed['income'], kde=True)\n",
    "plt.title(\"Iterative Imputer\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0511488d",
   "metadata": {},
   "source": [
    "## 8. Evaluating Imputation Effectiveness\n",
    "\n",
    "To evaluate the effectiveness of different imputation methods, we can:\n",
    "\n",
    "1. Introduce artificial missing values in a complete dataset\n",
    "2. Apply different imputation methods\n",
    "3. Compare the imputed values with the original \"true\" values\n",
    "\n",
    "Metrics to evaluate imputation:\n",
    "- Mean Absolute Error (MAE)\n",
    "- Root Mean Squared Error (RMSE)\n",
    "- Effect on statistical properties (mean, variance, correlations)\n",
    "- Impact on downstream analysis (e.g., predictive models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59392ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "\n",
    "# Create a complete dataset (only keeping rows with no missing values)\n",
    "df_complete = df.dropna().reset_index(drop=True)\n",
    "\n",
    "if len(df_complete) > 0:\n",
    "    # Make a copy to introduce artificial missing values\n",
    "    df_test = df_complete.copy()\n",
    "    \n",
    "    # Randomly introduce missing values (20% of age values)\n",
    "    np.random.seed(42)\n",
    "    mask = np.random.rand(len(df_test)) < 0.2\n",
    "    \n",
    "    # Save the true values before making them missing\n",
    "    true_values = df_test.loc[mask, 'age'].copy()\n",
    "    \n",
    "    # Set values as missing\n",
    "    df_test.loc[mask, 'age'] = np.nan\n",
    "    \n",
    "    # Apply different imputation methods\n",
    "    # Mean imputation\n",
    "    df_mean_imp = df_test.copy()\n",
    "    df_mean_imp['age'] = df_mean_imp['age'].fillna(df_mean_imp['age'].mean())\n",
    "    \n",
    "    # Median imputation\n",
    "    df_median_imp = df_test.copy()\n",
    "    df_median_imp['age'] = df_median_imp['age'].fillna(df_median_imp['age'].median())\n",
    "    \n",
    "    # Group-based imputation (by gender)\n",
    "    df_group_imp = df_test.copy()\n",
    "    gender_means = df_test.groupby('gender')['age'].transform('mean')\n",
    "    df_group_imp['age'] = df_group_imp['age'].fillna(gender_means)\n",
    "    \n",
    "    # KNN imputation\n",
    "    df_knn_imp = df_test.copy()\n",
    "    # Only impute age column with KNN\n",
    "    cols_for_knn = ['age', 'income', 'satisfaction']  # Using these features for KNN\n",
    "    knn_imputer = KNNImputer(n_neighbors=3)\n",
    "    df_knn_array = knn_imputer.fit_transform(df_test[cols_for_knn])\n",
    "    df_knn_imp['age'] = df_knn_array[:, 0]  # Take imputed age column\n",
    "    \n",
    "    # Evaluate the imputation methods\n",
    "    imputation_results = pd.DataFrame({\n",
    "        'True Values': true_values,\n",
    "        'Mean Imputation': df_mean_imp.loc[mask, 'age'].values,\n",
    "        'Median Imputation': df_median_imp.loc[mask, 'age'].values,\n",
    "        'Group Imputation': df_group_imp.loc[mask, 'age'].values,\n",
    "        'KNN Imputation': df_knn_imp.loc[mask, 'age'].values\n",
    "    })\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    imputation_errors = pd.DataFrame({\n",
    "        'Method': ['Mean', 'Median', 'Group', 'KNN'],\n",
    "        'MAE': [\n",
    "            mean_absolute_error(true_values, df_mean_imp.loc[mask, 'age']),\n",
    "            mean_absolute_error(true_values, df_median_imp.loc[mask, 'age']),\n",
    "            mean_absolute_error(true_values, df_group_imp.loc[mask, 'age']),\n",
    "            mean_absolute_error(true_values, df_knn_imp.loc[mask, 'age'])\n",
    "        ],\n",
    "        'RMSE': [\n",
    "            math.sqrt(mean_squared_error(true_values, df_mean_imp.loc[mask, 'age'])),\n",
    "            math.sqrt(mean_squared_error(true_values, df_median_imp.loc[mask, 'age'])),\n",
    "            math.sqrt(mean_squared_error(true_values, df_group_imp.loc[mask, 'age'])),\n",
    "            math.sqrt(mean_squared_error(true_values, df_knn_imp.loc[mask, 'age']))\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Imputation Method Comparison:\")\n",
    "    print(imputation_errors)\n",
    "    \n",
    "    # Visualize the error comparison\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.barplot(x='Method', y='MAE', data=imputation_errors)\n",
    "    plt.title('Mean Absolute Error by Imputation Method')\n",
    "    plt.ylabel('MAE (lower is better)')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.barplot(x='Method', y='RMSE', data=imputation_errors)\n",
    "    plt.title('Root Mean Squared Error by Imputation Method')\n",
    "    plt.ylabel('RMSE (lower is better)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare original and imputed distribution\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(true_values, kde=True, color='blue', label='True Values')\n",
    "    plt.title('Distribution of Original Values')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    methods = ['Mean Imputation', 'Median Imputation', 'Group Imputation', 'KNN Imputation']\n",
    "    for method in methods:\n",
    "        sns.histplot(imputation_results[method], kde=True, alpha=0.5, label=method)\n",
    "    plt.title('Distribution of Imputed Values')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Not enough complete data to perform imputation evaluation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101abc40",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Summary\n",
    "\n",
    "### Summary of Missing Value Handling Approaches:\n",
    "\n",
    "1. **Understanding missing data**:\n",
    "   - Identify the type of missingness (MCAR, MAR, MNAR)\n",
    "   - Visualize missing data patterns\n",
    "   - Calculate the extent of missingness\n",
    "\n",
    "2. **Consider the impact of missing data**:\n",
    "   - On sample size\n",
    "   - On variable distributions\n",
    "   - On relationships between variables\n",
    "\n",
    "3. **Choose the appropriate handling method**:\n",
    "   - For MCAR: Simple imputation may be sufficient\n",
    "   - For MAR: More advanced imputation using relationships in data\n",
    "   - For MNAR: Requires careful consideration and domain knowledge\n",
    "\n",
    "4. **Evaluate imputation effectiveness**:\n",
    "   - Compare statistical properties before and after imputation\n",
    "   - Assess impact on downstream analysis\n",
    "   - Consider using multiple imputation methods for sensitivity analysis\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Document all decisions made regarding missing data\n",
    "- Consider creating \"missingness indicators\" as additional features\n",
    "- Be aware of the bias that imputation can introduce\n",
    "- Use domain knowledge to guide imputation strategies\n",
    "- For critical analyses, consider how missing data handling affects results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of imputation methods and their pros/cons\n",
    "imputation_methods = pd.DataFrame({\n",
    "    'Method': [\n",
    "        'Deletion (listwise)',\n",
    "        'Deletion (pairwise)',\n",
    "        'Mean/Median/Mode',\n",
    "        'Forward/Backward Fill',\n",
    "        'Group-based',\n",
    "        'KNN Imputation',\n",
    "        'Iterative Imputation',\n",
    "        'Multiple Imputation'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Remove entire rows with any missing values',\n",
    "        'Remove specific columns or use available data for each analysis',\n",
    "        'Replace with column average (mean/median) or most common value (mode)',\n",
    "        'Use adjacent values (temporal or sequential)',\n",
    "        'Impute based on similar groups (e.g., by gender, age group)',\n",
    "        'Impute based on similarity to other samples',\n",
    "        'Use relationships between features to predict missing values',\n",
    "        'Create multiple complete datasets with different imputations'\n",
    "    ],\n",
    "    'Best Used When': [\n",
    "        'Very few missing values, MCAR assumption holds',\n",
    "        'Different analyses can use different variables',\n",
    "        'MCAR data, simple approach needed, numerical data',\n",
    "        'Time series data, sequential data with trends',\n",
    "        'Strong group patterns exist, categorical predictors',\n",
    "        'Complex relationships, enough complete cases for similarity',\n",
    "        'Strong relationships between variables, complex patterns',\n",
    "        'Accounting for uncertainty in imputation, formal statistical inference'\n",
    "    ],\n",
    "    'Limitations': [\n",
    "        'Reduces sample size, potential loss of information, bias if not MCAR',\n",
    "        'Complex to implement, different N for different analyses',\n",
    "        'Reduces variance, doesn\\'t preserve relationships, unrealistic',\n",
    "        'Requires sensible ordering, may not be realistic',\n",
    "        'Requires good grouping variables, may have small group sizes',\n",
    "        'Computationally intensive, requires parameter tuning',\n",
    "        'Assumes relationships between variables, complex to implement',\n",
    "        'Computationally intensive, complex to implement and analyze'\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Display the summary table\n",
    "imputation_methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310c256",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Handling missing values is a critical step in the data preprocessing pipeline. The approach you choose depends on:\n",
    "\n",
    "1. The amount and pattern of missingness\n",
    "2. The underlying mechanism of the missing data\n",
    "3. The requirements of your analysis\n",
    "4. The computational resources available\n",
    "\n",
    "Remember that there is no one-size-fits-all solution. Each dataset and analysis might require a different approach, and it's often valuable to try multiple methods and compare their effects on your results.\n",
    "\n",
    "By properly addressing missing values, you can build more robust and reliable models while minimizing bias and preserving as much information as possible from your data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
