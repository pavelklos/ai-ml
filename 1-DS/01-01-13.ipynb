{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ed14a3",
   "metadata": {},
   "source": [
    "# Working with Categorical Data\n",
    "\n",
    "This notebook provides a comprehensive guide to working with categorical data in data science, including creation, encoding, visualization, and advanced techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceed869",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952796e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from category_encoders import TargetEncoder, BinaryEncoder, WOEEncoder\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Display all columns in pandas dataframes\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38771b35",
   "metadata": {},
   "source": [
    "## 2. Understanding Categorical Data\n",
    "\n",
    "Categorical data represents characteristics or qualities that can be divided into groups or categories. They are typically non-numeric, although they can be represented numerically in some cases.\n",
    "\n",
    "### Types of Categorical Data\n",
    "\n",
    "1. **Nominal Data**: Categories with no inherent order (e.g., colors, gender, country)\n",
    "2. **Ordinal Data**: Categories with a meaningful order (e.g., education level, customer satisfaction ratings)\n",
    "\n",
    "### Why Categorical Data Matters\n",
    "\n",
    "- Much of real-world data comes in categorical form\n",
    "- Proper handling of categorical data is essential for accurate modeling\n",
    "- Categorical features often contain significant predictive information\n",
    "- Machine learning algorithms typically require numerical input, so categorical data needs special handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset with categorical variables\n",
    "data = {\n",
    "    'Gender': ['Male', 'Female', 'Male', 'Female', 'Male', 'Female', 'Female', 'Male', 'Female', 'Male'],\n",
    "    'Education': ['High School', 'Bachelor', 'Master', 'PhD', 'Bachelor', 'Master', 'High School', 'Bachelor', 'PhD', 'Master'],\n",
    "    'Country': ['USA', 'Canada', 'UK', 'Australia', 'USA', 'Canada', 'UK', 'Australia', 'USA', 'Canada'],\n",
    "    'Satisfaction': ['Low', 'Medium', 'High', 'Very High', 'Low', 'Medium', 'High', 'Very High', 'Medium', 'High'],\n",
    "    'Age': [25, 30, 35, 40, 45, 50, 55, 60, 65, 70],\n",
    "    'Salary': [50000, 60000, 70000, 80000, 90000, 100000, 110000, 120000, 130000, 140000]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Dataset overview:\")\n",
    "display(df)\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Identify categorical columns automatically\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nCategorical columns identified: {categorical_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822aa3ef",
   "metadata": {},
   "source": [
    "## 3. Creating Categorical Data\n",
    "\n",
    "Pandas provides a special `category` data type that offers memory efficiency and specialized functionality for categorical data. Converting appropriate columns to this type can significantly reduce memory usage, especially for large datasets with repetitive categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8590ef5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string columns to category type\n",
    "df_cat = df.copy()\n",
    "for col in categorical_columns:\n",
    "    df_cat[col] = df_cat[col].astype('category')\n",
    "\n",
    "# Check data types after conversion\n",
    "print(\"Data types after conversion to category:\")\n",
    "print(df_cat.dtypes)\n",
    "\n",
    "# Check memory usage before and after conversion\n",
    "print(\"\\nMemory usage comparison:\")\n",
    "print(f\"Original DataFrame memory usage: {df.memory_usage(deep=True).sum()} bytes\")\n",
    "print(f\"Category DataFrame memory usage: {df_cat.memory_usage(deep=True).sum()} bytes\")\n",
    "\n",
    "# Create an ordered categorical variable for Satisfaction\n",
    "satisfaction_order = ['Low', 'Medium', 'High', 'Very High']\n",
    "df_cat['Satisfaction'] = pd.Categorical(df_cat['Satisfaction'], \n",
    "                                       categories=satisfaction_order, \n",
    "                                       ordered=True)\n",
    "\n",
    "# Demonstrate properties of categorical data\n",
    "print(\"\\nCategories in 'Education':\", df_cat['Education'].cat.categories.tolist())\n",
    "print(\"\\nCategory codes for 'Education':\")\n",
    "display(pd.DataFrame({'Education': df_cat['Education'], 'Code': df_cat['Education'].cat.codes}))\n",
    "\n",
    "# Adding a new category\n",
    "df_cat['Education'] = df_cat['Education'].cat.add_categories(['Associate'])\n",
    "print(\"\\nCategories after adding 'Associate':\", df_cat['Education'].cat.categories.tolist())\n",
    "\n",
    "# Removing unused categories\n",
    "df_cat['Education'] = df_cat['Education'].cat.remove_unused_categories()\n",
    "print(\"\\nCategories after removing unused:\", df_cat['Education'].cat.categories.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243fc4d",
   "metadata": {},
   "source": [
    "## 4. Encoding Categorical Variables\n",
    "\n",
    "Machine learning algorithms typically require numerical input. Encoding is the process of converting categorical data into numerical format. Several encoding techniques are available, each with its advantages and disadvantages:\n",
    "\n",
    "1. **Label Encoding**: Assigns a unique integer to each category\n",
    "2. **One-Hot Encoding**: Creates binary columns for each category\n",
    "3. **Ordinal Encoding**: Assigns integers based on an ordered relationship\n",
    "4. **Target Encoding**: Replaces categories with the mean of the target variable\n",
    "\n",
    "Let's implement and compare these methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb65521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of our dataframe for encoding examples\n",
    "df_encode = df.copy()\n",
    "\n",
    "# 1. Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "df_encode['Gender_Label'] = label_encoder.fit_transform(df_encode['Gender'])\n",
    "df_encode['Country_Label'] = label_encoder.fit_transform(df_encode['Country'])\n",
    "\n",
    "# 2. One-Hot Encoding \n",
    "# Using pandas get_dummies\n",
    "df_onehot = pd.get_dummies(df_encode, columns=['Education', 'Country'], prefix=['Edu', 'Country'])\n",
    "\n",
    "# 3. Ordinal Encoding\n",
    "# Define the order for our ordinal variable\n",
    "satisfaction_order = {'Low': 0, 'Medium': 1, 'High': 2, 'Very High': 3}\n",
    "df_encode['Satisfaction_Ordinal'] = df_encode['Satisfaction'].map(satisfaction_order)\n",
    "\n",
    "# Alternative with scikit-learn\n",
    "ordinal_encoder = OrdinalEncoder(categories=[satisfaction_order.keys()])\n",
    "df_encode['Satisfaction_Ordinal2'] = ordinal_encoder.fit_transform(df_encode[['Satisfaction']])\n",
    "\n",
    "# 4. Target Encoding (using salary as target for demonstration)\n",
    "# For realistic use, we'd split the data first to avoid data leakage\n",
    "encoder = TargetEncoder()\n",
    "df_encode['Country_Target'] = encoder.fit_transform(df_encode['Country'], df_encode['Salary'])\n",
    "\n",
    "# Display the results\n",
    "print(\"Label Encoding Results:\")\n",
    "display(df_encode[['Gender', 'Gender_Label', 'Country', 'Country_Label']])\n",
    "\n",
    "print(\"\\nOrdinal Encoding Results:\")\n",
    "display(df_encode[['Satisfaction', 'Satisfaction_Ordinal']])\n",
    "\n",
    "print(\"\\nTarget Encoding Results:\")\n",
    "display(df_encode[['Country', 'Salary', 'Country_Target']])\n",
    "\n",
    "print(\"\\nOne-Hot Encoding Results (partial view):\")\n",
    "display(df_onehot.head(3))\n",
    "\n",
    "# Compare encoding methods\n",
    "print(\"\\nComparison of Encoding Methods:\")\n",
    "print(\"1. Label Encoding:\")\n",
    "print(\"   + Simple and straightforward\")\n",
    "print(\"   + Maintains a single column\")\n",
    "print(\"   - Implies an ordinal relationship that may not exist\")\n",
    "print(\"   - Can mislead algorithms to infer numerical relationships\")\n",
    "\n",
    "print(\"\\n2. One-Hot Encoding:\")\n",
    "print(\"   + No ordinal relationship implied\")\n",
    "print(\"   + Works well with most algorithms\")\n",
    "print(\"   - Creates many columns (curse of dimensionality)\")\n",
    "print(\"   - Memory intensive for high-cardinality features\")\n",
    "\n",
    "print(\"\\n3. Ordinal Encoding:\")\n",
    "print(\"   + Preserves order information\")\n",
    "print(\"   + Memory efficient\")\n",
    "print(\"   - Only appropriate when order matters\")\n",
    "print(\"   - Requires domain knowledge to define order\")\n",
    "\n",
    "print(\"\\n4. Target Encoding:\")\n",
    "print(\"   + Captures relationship with target variable\")\n",
    "print(\"   + Handles high-cardinality well\")\n",
    "print(\"   - Risk of target leakage\")\n",
    "print(\"   - Requires careful validation strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8bd103",
   "metadata": {},
   "source": [
    "## 5. Working with Categorical Data in Pandas\n",
    "\n",
    "Pandas provides powerful functionality for working with categorical data, including filtering, grouping, and sorting operations specific to categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70997da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with categorical data in pandas\n",
    "# Create a sample dataset\n",
    "data = {\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Laptop', 'Phone', 'Tablet', 'Laptop', 'Phone', 'Tablet', 'Laptop'],\n",
    "    'Color': ['Red', 'Blue', 'Green', 'Red', 'Blue', 'Green', 'Red', 'Blue', 'Green', 'Red'],\n",
    "    'Size': ['Small', 'Medium', 'Large', 'Small', 'Medium', 'Large', 'Medium', 'Large', 'Small', 'Large'],\n",
    "    'Rating': [4, 3, 5, 2, 4, 3, 5, 4, 3, 5],\n",
    "    'Price': [1200, 800, 600, 1500, 700, 550, 1300, 850, 500, 1400]\n",
    "}\n",
    "\n",
    "cat_df = pd.DataFrame(data)\n",
    "cat_df['Size'] = pd.Categorical(cat_df['Size'], categories=['Small', 'Medium', 'Large'], ordered=True)\n",
    "cat_df['Color'] = cat_df['Color'].astype('category')\n",
    "cat_df['Product'] = cat_df['Product'].astype('category')\n",
    "\n",
    "print(\"Dataset with categorical variables:\")\n",
    "display(cat_df)\n",
    "print(cat_df.dtypes)\n",
    "\n",
    "# Filtering by categorical value\n",
    "print(\"\\nFiltering laptops only:\")\n",
    "display(cat_df[cat_df['Product'] == 'Laptop'])\n",
    "\n",
    "# Grouping by categorical variable\n",
    "print(\"\\nAverage price by product type:\")\n",
    "display(cat_df.groupby('Product')['Price'].mean())\n",
    "\n",
    "print(\"\\nAverage rating by size:\")\n",
    "display(cat_df.groupby('Size')['Rating'].mean())\n",
    "\n",
    "# Sorting by categorical variable (using ordered category)\n",
    "print(\"\\nData sorted by Size (ordered categorical):\")\n",
    "display(cat_df.sort_values('Size'))\n",
    "\n",
    "# Cross-tabulation of categorical variables\n",
    "print(\"\\nCross-tabulation of Product vs. Color:\")\n",
    "display(pd.crosstab(cat_df['Product'], cat_df['Color']))\n",
    "\n",
    "# Aggregation with multiple functions\n",
    "print(\"\\nAggregation by Product:\")\n",
    "display(cat_df.groupby('Product').agg({\n",
    "    'Price': ['mean', 'min', 'max'],\n",
    "    'Rating': ['mean', 'count']\n",
    "}))\n",
    "\n",
    "# Filtering with categorical comparisons (for ordered categories)\n",
    "print(\"\\nItems that are Medium or larger:\")\n",
    "display(cat_df[cat_df['Size'] >= 'Medium'])\n",
    "\n",
    "# Value counts and frequency analysis\n",
    "print(\"\\nFrequency of each product:\")\n",
    "display(cat_df['Product'].value_counts())\n",
    "\n",
    "print(\"\\nRelative frequency (percentage):\")\n",
    "display(cat_df['Product'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655fc810",
   "metadata": {},
   "source": [
    "## 6. Visualization of Categorical Data\n",
    "\n",
    "Visualizing categorical data helps uncover patterns and relationships. We'll use matplotlib and seaborn to create various plots specifically designed for categorical data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93e4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for better visualizations\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "\n",
    "categories = {\n",
    "    'Department': ['Sales', 'Marketing', 'IT', 'HR', 'Finance', 'Engineering'],\n",
    "    'Education': ['High School', 'Associate', 'Bachelor', 'Master', 'PhD'],\n",
    "    'Region': ['North', 'South', 'East', 'West'],\n",
    "    'Performance': ['Poor', 'Average', 'Good', 'Excellent']\n",
    "}\n",
    "\n",
    "vis_data = {\n",
    "    'Department': np.random.choice(categories['Department'], size=n),\n",
    "    'Education': np.random.choice(categories['Education'], size=n),\n",
    "    'Region': np.random.choice(categories['Region'], size=n),\n",
    "    'Performance': np.random.choice(categories['Performance'], size=n),\n",
    "    'Age': np.random.randint(22, 65, size=n),\n",
    "    'Salary': np.random.normal(60000, 15000, size=n),\n",
    "    'Experience': np.random.randint(0, 30, size=n),\n",
    "    'Satisfaction': np.random.randint(1, 11, size=n)\n",
    "}\n",
    "\n",
    "vis_df = pd.DataFrame(vis_data)\n",
    "\n",
    "# Convert categorical columns to category type with appropriate ordering\n",
    "vis_df['Performance'] = pd.Categorical(vis_df['Performance'], \n",
    "                                      categories=['Poor', 'Average', 'Good', 'Excellent'], \n",
    "                                      ordered=True)\n",
    "\n",
    "vis_df['Education'] = pd.Categorical(vis_df['Education'],\n",
    "                                   categories=['High School', 'Associate', 'Bachelor', 'Master', 'PhD'],\n",
    "                                   ordered=True)\n",
    "\n",
    "# 1. Bar Chart for frequency of categories\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(data=vis_df, x='Department', order=vis_df['Department'].value_counts().index)\n",
    "plt.title('Frequency of Employees by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 2. Stacked bar chart for two categorical variables\n",
    "plt.figure(figsize=(12, 8))\n",
    "pd.crosstab(vis_df['Department'], vis_df['Education']).plot(kind='bar', stacked=True, colormap='viridis')\n",
    "plt.title('Education Level by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Education Level', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Boxplot for categorical vs. numerical data\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.boxplot(data=vis_df, x='Department', y='Salary')\n",
    "plt.title('Salary Distribution by Department')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Salary ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 4. Violin plot for categorical vs. numerical data\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.violinplot(data=vis_df, x='Performance', y='Satisfaction')\n",
    "plt.title('Satisfaction Distribution by Performance Level')\n",
    "plt.xlabel('Performance Rating')\n",
    "plt.ylabel('Satisfaction Score (1-10)')\n",
    "plt.show()\n",
    "\n",
    "# 5. Heatmap for correlation between categorical variables\n",
    "plt.figure(figsize=(12, 10))\n",
    "crosstab = pd.crosstab(vis_df['Region'], vis_df['Department'], normalize='index')\n",
    "sns.heatmap(crosstab, annot=True, cmap='YlGnBu', fmt='.2f')\n",
    "plt.title('Proportion of Departments across Regions')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Point plot for categorical comparisons\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.pointplot(data=vis_df, x='Education', y='Salary', hue='Performance', palette='viridis')\n",
    "plt.title('Average Salary by Education and Performance')\n",
    "plt.xlabel('Education Level')\n",
    "plt.ylabel('Average Salary ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Performance', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Categorical scatterplot (stripplot)\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.stripplot(data=vis_df, x='Department', y='Age', hue='Education', jitter=True, alpha=0.6)\n",
    "plt.title('Age Distribution by Department and Education')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Age')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Education', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Facetgrid for multi-categorical analysis\n",
    "g = sns.FacetGrid(vis_df, col=\"Region\", row=\"Performance\", height=3.5, aspect=1.2)\n",
    "g.map_dataframe(sns.barplot, x=\"Department\", y=\"Salary\")\n",
    "g.set_axis_labels(\"Department\", \"Average Salary ($)\")\n",
    "g.set_titles(col_template=\"{col_name} Region\", row_template=\"{row_name} Performance\")\n",
    "g.fig.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('Salary by Department, Region, and Performance', fontsize=16)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1c89f4",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis with Categorical Data\n",
    "\n",
    "Categorical data requires specific statistical tests to analyze relationships:\n",
    "\n",
    "1. **Chi-Square Test**: Tests association between categorical variables\n",
    "2. **ANOVA**: Compares means across categorical groups\n",
    "3. **T-test**: Compares means between two groups\n",
    "4. **Correlation Analysis**: Measures relationships with categorical variables\n",
    "\n",
    "Let's implement these techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a4f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis with categorical data\n",
    "stats_df = vis_df.copy()\n",
    "\n",
    "# 1. Chi-Square Test of Independence\n",
    "# Create a contingency table\n",
    "contingency = pd.crosstab(stats_df['Department'], stats_df['Performance'])\n",
    "print(\"Contingency Table (Department vs Performance):\")\n",
    "display(contingency)\n",
    "\n",
    "# Perform chi-square test\n",
    "chi2, p, dof, expected = stats.chi2_contingency(contingency)\n",
    "print(f\"Chi-Square Test Results:\")\n",
    "print(f\"Chi2 value: {chi2:.4f}\")\n",
    "print(f\"p-value: {p:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"Interpretation: {'Reject null hypothesis (variables are dependent)' if p < 0.05 else 'Fail to reject null hypothesis (variables are independent)'}\")\n",
    "\n",
    "# 2. ANOVA - Comparing means across groups\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Group salary by department\n",
    "groups = [stats_df[stats_df['Department'] == dept]['Salary'].values for dept in stats_df['Department'].unique()]\n",
    "f_stat, p_val = f_oneway(*groups)\n",
    "print(\"\\nOne-way ANOVA Results (Salary across Departments):\")\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "print(f\"Interpretation: {'Reject null hypothesis (means are different)' if p_val < 0.05 else 'Fail to reject null hypothesis (means are equal)'}\")\n",
    "\n",
    "# 3. T-test between two groups\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# Compare salary between two regions\n",
    "group1 = stats_df[stats_df['Region'] == 'North']['Salary']\n",
    "group2 = stats_df[stats_df['Region'] == 'South']['Salary']\n",
    "t_stat, p_val = ttest_ind(group1, group2)\n",
    "print(\"\\nT-test Results (Salary: North vs South):\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_val:.4f}\")\n",
    "print(f\"Interpretation: {'Reject null hypothesis (means are different)' if p_val < 0.05 else 'Fail to reject null hypothesis (means are equal)'}\")\n",
    "\n",
    "# 4. Point-Biserial Correlation (for binary categorical vs numerical)\n",
    "# Create a binary variable for demonstration\n",
    "stats_df['SeniorRole'] = (stats_df['Experience'] > 15).astype(int)\n",
    "from scipy.stats import pointbiserialr\n",
    "r, p = pointbiserialr(stats_df['SeniorRole'], stats_df['Salary'])\n",
    "print(\"\\nPoint-Biserial Correlation (Senior Role vs Salary):\")\n",
    "print(f\"Correlation: {r:.4f}\")\n",
    "print(f\"p-value: {p:.4f}\")\n",
    "print(f\"Interpretation: {'Significant correlation' if p < 0.05 else 'No significant correlation'}\")\n",
    "\n",
    "# 5. Visualize statistical comparisons\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(data=stats_df, x='Department', y='Salary', errorbar=('ci', 95))\n",
    "plt.title('Average Salary by Department with 95% Confidence Intervals')\n",
    "plt.xlabel('Department')\n",
    "plt.ylabel('Average Salary ($)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6. Correlation matrix with encoded categorical variables\n",
    "# One-hot encode categorical variables\n",
    "encoded_df = pd.get_dummies(stats_df, columns=['Department', 'Region', 'Performance'], drop_first=True)\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = encoded_df.corr()\n",
    "\n",
    "# Plot correlation heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(corr_matrix)\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix Including Encoded Categorical Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614e82a",
   "metadata": {},
   "source": [
    "## 8. Feature Engineering with Categorical Variables\n",
    "\n",
    "Feature engineering is crucial for extracting value from categorical data. Advanced encoding techniques can significantly improve model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814905f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering with categorical data\n",
    "from category_encoders import WOEEncoder, TargetEncoder, BinaryEncoder, HashingEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create a dataset for classification\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "\n",
    "# Generate data\n",
    "fe_data = {\n",
    "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse', 'Headphones', 'Speakers'], size=n),\n",
    "    'Brand': np.random.choice(['Apple', 'Samsung', 'Dell', 'HP', 'Lenovo', 'Asus', 'Acer', 'Microsoft'], size=n),\n",
    "    'Store': np.random.choice(['Amazon', 'BestBuy', 'Walmart', 'Target', 'Costco', 'Newegg'], size=n),\n",
    "    'Shipping': np.random.choice(['Standard', 'Express', 'Next Day', 'In-Store Pickup'], size=n),\n",
    "    'Day': np.random.choice(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'], size=n),\n",
    "    'Price': np.random.gamma(5, 100, n)\n",
    "}\n",
    "\n",
    "fe_df = pd.DataFrame(fe_data)\n",
    "\n",
    "# Create a target variable (simulated purchase amount)\n",
    "base_values = {'Laptop': 1200, 'Phone': 800, 'Tablet': 500, 'Monitor': 300, \n",
    "              'Keyboard': 100, 'Mouse': 50, 'Headphones': 150, 'Speakers': 200}\n",
    "brand_factors = {'Apple': 1.5, 'Samsung': 1.3, 'Dell': 1.0, 'HP': 0.9, \n",
    "                'Lenovo': 0.95, 'Asus': 0.92, 'Acer': 0.85, 'Microsoft': 1.1}\n",
    "\n",
    "fe_df['Purchase'] = fe_df.apply(lambda row: \n",
    "                                base_values[row['Product']] * \n",
    "                                brand_factors[row['Brand']] * \n",
    "                                (1 + np.random.normal(0, 0.1)), axis=1)\n",
    "\n",
    "print(\"Feature engineering dataset:\")\n",
    "display(fe_df.head())\n",
    "\n",
    "# Split into train and test sets\n",
    "X = fe_df.drop('Purchase', axis=1)\n",
    "y = fe_df['Purchase']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Frequency Encoding\n",
    "def frequency_encoder(df, col):\n",
    "    freq_map = df[col].value_counts(normalize=True).to_dict()\n",
    "    return df[col].map(freq_map)\n",
    "\n",
    "# Apply frequency encoding\n",
    "X_train_freq = X_train.copy()\n",
    "X_test_freq = X_test.copy()\n",
    "\n",
    "for col in ['Product', 'Brand', 'Store', 'Shipping', 'Day']:\n",
    "    X_train_freq[f'{col}_freq'] = frequency_encoder(X_train, col)\n",
    "    # Use the training frequencies for the test set to avoid data leakage\n",
    "    X_test_freq[f'{col}_freq'] = X_test[col].map(X_train[col].value_counts(normalize=True).to_dict())\n",
    "\n",
    "print(\"\\nFrequency Encoding Example:\")\n",
    "display(X_train_freq[['Product', 'Product_freq']].drop_duplicates().sort_values('Product_freq', ascending=False))\n",
    "\n",
    "# 2. Target Encoding\n",
    "target_encoder = TargetEncoder()\n",
    "X_train_target = X_train.copy()\n",
    "X_test_target = X_test.copy()\n",
    "\n",
    "for col in ['Product', 'Brand', 'Store', 'Shipping', 'Day']:\n",
    "    X_train_target[f'{col}_target'] = target_encoder.fit_transform(X_train[col], y_train)\n",
    "    X_test_target[f'{col}_target'] = target_encoder.transform(X_test[col])\n",
    "\n",
    "print(\"\\nTarget Encoding Example:\")\n",
    "display(X_train_target[['Product', 'Product_target']].drop_duplicates().sort_values('Product_target', ascending=False))\n",
    "\n",
    "# 3. Binary Encoding\n",
    "binary_encoder = BinaryEncoder()\n",
    "encoded_cols = binary_encoder.fit_transform(X_train[['Product', 'Brand']])\n",
    "print(\"\\nBinary Encoding Example:\")\n",
    "display(pd.concat([X_train[['Product', 'Brand']], encoded_cols], axis=1).head(10))\n",
    "\n",
    "# 4. Weight of Evidence (WOE) Encoding\n",
    "# For WOE, we need a binary target. Let's create one for demonstration\n",
    "y_train_binary = (y_train > y_train.median()).astype(int)\n",
    "y_test_binary = (y_test > y_test.median()).astype(int)\n",
    "\n",
    "woe_encoder = WOEEncoder()\n",
    "X_train_woe = X_train.copy()\n",
    "X_test_woe = X_test.copy()\n",
    "\n",
    "for col in ['Product', 'Brand', 'Store']:\n",
    "    X_train_woe[f'{col}_woe'] = woe_encoder.fit_transform(X_train[col], y_train_binary)\n",
    "    X_test_woe[f'{col}_woe'] = woe_encoder.transform(X_test[col])\n",
    "\n",
    "print(\"\\nWeight of Evidence Encoding Example:\")\n",
    "display(X_train_woe[['Product', 'Product_woe']].drop_duplicates().sort_values('Product_woe'))\n",
    "\n",
    "# 5. Entity Embedding (concept explanation)\n",
    "print(\"\\nEntity Embedding:\")\n",
    "print(\"Entity embedding is a technique where categorical variables are embedded in a\")\n",
    "print(\"lower-dimensional space, similar to word embeddings in NLP.\")\n",
    "print(\"This is typically implemented in neural networks by:\")\n",
    "print(\"1. Converting categories to integers\")\n",
    "print(\"2. Creating an embedding layer\")\n",
    "print(\"3. Learning embeddings during model training\")\n",
    "print(\"\\nAdvantages:\")\n",
    "print(\"- Captures complex relationships between categories\")\n",
    "print(\"- Efficient for high-cardinality features\")\n",
    "print(\"- Can transfer knowledge between similar categories\")\n",
    "\n",
    "# 6. Compare different encoding techniques with a simple model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "# Prepare different feature sets\n",
    "# One-hot encoding\n",
    "X_train_onehot = pd.get_dummies(X_train, columns=['Product', 'Brand', 'Store', 'Shipping', 'Day'], drop_first=True)\n",
    "X_test_onehot = pd.get_dummies(X_test, columns=['Product', 'Brand', 'Store', 'Shipping', 'Day'], drop_first=True)\n",
    "\n",
    "# Ensure train and test have the same columns\n",
    "missing_cols = set(X_train_onehot.columns) - set(X_test_onehot.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_onehot[col] = 0\n",
    "X_test_onehot = X_test_onehot[X_train_onehot.columns]\n",
    "\n",
    "# Create models with different encodings\n",
    "models = {\n",
    "    'Frequency Encoding': (X_train_freq.drop(['Product', 'Brand', 'Store', 'Shipping', 'Day'], axis=1), \n",
    "                         X_test_freq.drop(['Product', 'Brand', 'Store', 'Shipping', 'Day'], axis=1)),\n",
    "    'Target Encoding': (X_train_target.drop(['Product', 'Brand', 'Store', 'Shipping', 'Day'], axis=1), \n",
    "                      X_test_target.drop(['Product', 'Brand', 'Store', 'Shipping', 'Day'], axis=1)),\n",
    "    'One-Hot Encoding': (X_train_onehot, X_test_onehot),\n",
    "    'WOE Encoding': (X_train_woe.drop(['Product', 'Brand', 'Store', 'Shipping', 'Day'], axis=1), \n",
    "                    X_test_woe.drop(['Product', 'Brand', 'Store', 'Shipping', 'Day'], axis=1))\n",
    "}\n",
    "\n",
    "# Evaluate each model\n",
    "results = {}\n",
    "for name, (X_train_enc, X_test_enc) in models.items():\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_enc, y_train)\n",
    "    y_pred = model.predict(X_test_enc)\n",
    "    rmse = math.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {'RMSE': rmse, 'R²': r2}\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\nEncoding Method Comparison (Linear Regression):\")\n",
    "display(results_df)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 8))\n",
    "ax1 = plt.subplot(211)\n",
    "results_df['RMSE'].plot(kind='bar', ax=ax1, color='salmon')\n",
    "plt.title('RMSE by Encoding Method (Lower is Better)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "ax2 = plt.subplot(212)\n",
    "results_df['R²'].plot(kind='bar', ax=ax2, color='skyblue')\n",
    "plt.title('R² by Encoding Method (Higher is Better)')\n",
    "plt.ylabel('R²')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaeed215",
   "metadata": {},
   "source": [
    "## 9. Handling High Cardinality Categorical Features\n",
    "\n",
    "High cardinality categorical features (those with many unique values) pose a challenge for modeling. Let's look at techniques to handle them effectively:\n",
    "\n",
    "1. **Grouping Rare Categories**: Combine infrequent categories into an \"Other\" category\n",
    "2. **Hierarchical Grouping**: Group categories based on domain knowledge\n",
    "3. **Similarity-Based Grouping**: Group similar categories based on feature behavior\n",
    "4. **Dimensionality Reduction**: Use techniques like PCA on encoded categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abfd84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with high cardinality features\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generate data with a high-cardinality feature (ZIP code)\n",
    "zipcode_range = range(10000, 99999)\n",
    "sample_zipcodes = np.random.choice(zipcode_range, size=100, replace=False)\n",
    "zipcodes = np.random.choice(sample_zipcodes, size=n)\n",
    "\n",
    "high_card_data = {\n",
    "    'Customer_ID': range(1, n+1),\n",
    "    'ZIP_Code': zipcodes,\n",
    "    'Product_ID': np.random.choice(range(1000, 9999), size=n),\n",
    "    'Store_ID': np.random.choice(range(100, 999), size=n, p=np.random.dirichlet(np.ones(899), 1)[0]),\n",
    "    'Transaction_Amount': np.random.gamma(5, 20, n)\n",
    "}\n",
    "\n",
    "hc_df = pd.DataFrame(high_card_data)\n",
    "\n",
    "# Add target for ML examples\n",
    "hc_df['Purchase_Again'] = (hc_df['Transaction_Amount'] > 100).astype(int)\n",
    "hc_df['Purchase_Again'] = hc_df['Purchase_Again'] ^ (np.random.random(n) > 0.7).astype(int)\n",
    "\n",
    "print(\"High Cardinality Dataset:\")\n",
    "display(hc_df.head())\n",
    "\n",
    "print(\"\\nCardinality of Categorical Features:\")\n",
    "print(f\"ZIP_Code: {hc_df['ZIP_Code'].nunique()} unique values\")\n",
    "print(f\"Product_ID: {hc_df['Product_ID'].nunique()} unique values\")\n",
    "print(f\"Store_ID: {hc_df['Store_ID'].nunique()} unique values\")\n",
    "\n",
    "# 1. Grouping Rare Categories\n",
    "def group_rare_categories(df, col, threshold=0.05):\n",
    "    \"\"\"Group categories that appear less than threshold% of the time\"\"\"\n",
    "    counts = df[col].value_counts(normalize=True)\n",
    "    mask = counts >= threshold\n",
    "    categories_to_keep = counts[mask].index\n",
    "    df_new = df.copy()\n",
    "    df_new[f'{col}_grouped'] = df_new[col].apply(lambda x: x if x in categories_to_keep else 'Other')\n",
    "    return df_new\n",
    "\n",
    "# Apply to Store_ID\n",
    "hc_df = group_rare_categories(hc_df, 'Store_ID', threshold=0.02)\n",
    "print(\"\\nStore_ID After Grouping Rare Categories:\")\n",
    "display(hc_df['Store_ID_grouped'].value_counts().head(10))\n",
    "\n",
    "# Visualize effect of grouping\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "hc_df['Store_ID'].value_counts().head(10).plot(kind='bar')\n",
    "plt.title('Original Store_ID (Top 10)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(122)\n",
    "hc_df['Store_ID_grouped'].value_counts().plot(kind='bar')\n",
    "plt.title('Grouped Store_ID')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Hierarchical Grouping (by prefix, simulating geographic information in ZIP codes)\n",
    "hc_df['ZIP_Prefix'] = hc_df['ZIP_Code'].astype(str).str[:3]\n",
    "print(\"\\nHierarchical Grouping of ZIP Codes (by prefix):\")\n",
    "display(hc_df['ZIP_Prefix'].value_counts().head(10))\n",
    "\n",
    "# 3. Group by Target Behavior (Group by statistical property related to target)\n",
    "def group_by_target_mean(df, cat_col, target_col, bins=5):\n",
    "    # Calculate the mean of the target for each category\n",
    "    means = df.groupby(cat_col)[target_col].mean().to_dict()\n",
    "    # Map each category to its mean\n",
    "    df[f'{cat_col}_target_mean'] = df[cat_col].map(means)\n",
    "    # Create bins based on the mean values\n",
    "    df[f'{cat_col}_binned'] = pd.qcut(df[f'{cat_col}_target_mean'], \n",
    "                                     q=bins, \n",
    "                                     labels=[f'Bin_{i+1}' for i in range(bins)])\n",
    "    return df\n",
    "\n",
    "# Apply to Product_ID\n",
    "hc_df = group_by_target_mean(hc_df, 'Product_ID', 'Purchase_Again', bins=5)\n",
    "print(\"\\nProduct_ID Grouped by Target Behavior:\")\n",
    "display(hc_df['Product_ID_binned'].value_counts())\n",
    "\n",
    "# Visualize relationship between original and binned categories\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(data=hc_df, x='Product_ID_binned', y='Purchase_Again')\n",
    "plt.title('Purchase Again Rate by Product_ID Bins')\n",
    "plt.show()\n",
    "\n",
    "# 4. Dimensionality Reduction on Encoded Categories\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# One-hot encode ZIP_Prefix\n",
    "zip_prefix_encoded = pd.get_dummies(hc_df['ZIP_Prefix'], prefix='ZIP')\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=5)\n",
    "zip_pca = pca.fit_transform(zip_prefix_encoded)\n",
    "\n",
    "# Add PCA components to dataframe\n",
    "for i in range(5):\n",
    "    hc_df[f'ZIP_PCA_{i+1}'] = zip_pca[:, i]\n",
    "\n",
    "print(\"\\nPCA on Encoded ZIP Prefixes:\")\n",
    "display(hc_df[['ZIP_Prefix'] + [f'ZIP_PCA_{i+1}' for i in range(5)]].head())\n",
    "\n",
    "print(\"\\nPCA Explained Variance Ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n",
    "print(f\"Total explained variance: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "# 5. Hashing (Feature Hashing / The Hashing Trick)\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "# Apply feature hashing to Product_ID\n",
    "hasher = FeatureHasher(n_features=10, input_type='string')\n",
    "product_id_str = hc_df['Product_ID'].astype(str).values.reshape(-1, 1)\n",
    "hashed_features = hasher.transform(product_id_str).toarray()\n",
    "\n",
    "# Add hashed features to the dataframe\n",
    "for i in range(10):\n",
    "    hc_df[f'Product_Hash_{i+1}'] = hashed_features[:, i]\n",
    "\n",
    "print(\"\\nFeature Hashing for Product_ID:\")\n",
    "display(hc_df[[f'Product_Hash_{i+1}' for i in range(5)]].head())\n",
    "\n",
    "# 6. Compare methods with a classification model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# Split data\n",
    "X = hc_df.drop(['Customer_ID', 'Purchase_Again'], axis=1)\n",
    "y = hc_df['Purchase_Again']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Prepare different feature sets\n",
    "feature_sets = {\n",
    "    'Original High Cardinality': ['ZIP_Code', 'Product_ID', 'Store_ID', 'Transaction_Amount'],\n",
    "    'Grouped Rare Categories': ['ZIP_Code', 'Product_ID', 'Store_ID_grouped', 'Transaction_Amount'],\n",
    "    'Hierarchical Grouping': ['ZIP_Prefix', 'Product_ID', 'Store_ID', 'Transaction_Amount'],\n",
    "    'Target-Based Binning': ['ZIP_Code', 'Product_ID_binned', 'Store_ID', 'Transaction_Amount'],\n",
    "    'PCA Features': [f'ZIP_PCA_{i+1}' for i in range(5)] + ['Product_ID', 'Store_ID', 'Transaction_Amount'],\n",
    "    'Hashing Features': [f'Product_Hash_{i+1}' for i in range(10)] + ['ZIP_Code', 'Store_ID', 'Transaction_Amount']\n",
    "}\n",
    "\n",
    "# Evaluate each feature set\n",
    "model_results = {}\n",
    "for name, features in feature_sets.items():\n",
    "    # Handle categorical features for each approach\n",
    "    X_train_set = X_train[features].copy()\n",
    "    X_test_set = X_test[features].copy()\n",
    "    \n",
    "    # Convert remaining categorical columns\n",
    "    for col in X_train_set.select_dtypes(include=['object', 'int64']).columns:\n",
    "        if X_train_set[col].nunique() < 50:  # Only one-hot encode if cardinality is manageable\n",
    "            X_train_dummies = pd.get_dummies(X_train_set[col], prefix=col)\n",
    "            X_test_dummies = pd.get_dummies(X_test_set[col], prefix=col)\n",
    "            \n",
    "            # Handle potential difference in columns\n",
    "            for c in X_train_dummies.columns:\n",
    "                if c not in X_test_dummies.columns:\n",
    "                    X_test_dummies[c] = 0\n",
    "            X_test_dummies = X_test_dummies[X_train_dummies.columns]\n",
    "            \n",
    "            # Replace original column with dummy variables\n",
    "            X_train_set = pd.concat([X_train_set.drop(col, axis=1), X_train_dummies], axis=1)\n",
    "            X_test_set = pd.concat([X_test_set.drop(col, axis=1), X_test_dummies], axis=1)\n",
    "        else:\n",
    "            # For very high cardinality, use label encoding instead\n",
    "            le = LabelEncoder()\n",
    "            X_train_set[col] = le.fit_transform(X_train_set[col])\n",
    "            X_test_set[col] = le.transform(X_test_set[col])\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_set, y_train)\n",
    "    y_pred = model.predict(X_test_set)\n",
    "    y_pred_proba = model.predict_proba(X_test_set)[:, 1]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    model_results[name] = {'Accuracy': accuracy, 'AUC': auc}\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "print(\"\\nComparison of High Cardinality Handling Methods:\")\n",
    "display(results_df)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.subplot(211)\n",
    "results_df['Accuracy'].plot(kind='bar', color='skyblue')\n",
    "plt.title('Accuracy by Method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.subplot(212)\n",
    "results_df['AUC'].plot(kind='bar', color='salmon')\n",
    "plt.title('AUC by Method')\n",
    "plt.ylabel('AUC')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.ylim(0.5, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary of techniques\n",
    "print(\"\\nSummary of High Cardinality Handling Techniques:\")\n",
    "print(\"1. Grouping Rare Categories:\")\n",
    "print(\"   + Reduces dimensionality\")\n",
    "print(\"   + Handles infrequent values\")\n",
    "print(\"   - May lose information\")\n",
    "print(\"   - Requires setting a threshold\")\n",
    "\n",
    "print(\"\\n2. Hierarchical Grouping:\")\n",
    "print(\"   + Preserves information structure\")\n",
    "print(\"   + Uses domain knowledge\")\n",
    "print(\"   - Requires understanding of the feature\")\n",
    "print(\"   - May not always be possible\")\n",
    "\n",
    "print(\"\\n3. Target-Based Binning:\")\n",
    "print(\"   + Directly relates to target variable\")\n",
    "print(\"   + Creates meaningful groups\")\n",
    "print(\"   - Risk of target leakage\")\n",
    "print(\"   - Requires careful validation\")\n",
    "\n",
    "print(\"\\n4. Dimensionality Reduction:\")\n",
    "print(\"   + Efficiently compresses information\")\n",
    "print(\"   + Can capture complex relationships\")\n",
    "print(\"   - Less interpretable\")\n",
    "print(\"   - Requires preprocessing\")\n",
    "\n",
    "print(\"\\n5. Feature Hashing:\")\n",
    "print(\"   + Handles any number of categories\")\n",
    "print(\"   + Memory efficient\")\n",
    "print(\"   - Hash collisions\")\n",
    "print(\"   - Loss of interpretability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753306f8",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Working with categorical data is a fundamental skill in data science. In this notebook, we've explored:\n",
    "\n",
    "1. **Understanding categorical data types** - nominal vs. ordinal data\n",
    "2. **Creating and manipulating categorical data** in pandas  \n",
    "3. **Encoding techniques** - label, one-hot, ordinal, target encoding and more\n",
    "4. **Pandas functions** for categorical data analysis\n",
    "5. **Visualization techniques** for categorical variables\n",
    "6. **Statistical analysis** methods for categorical data\n",
    "7. **Advanced feature engineering** approaches\n",
    "8. **Solutions for high cardinality** categorical variables\n",
    "\n",
    "The most appropriate technique depends on your specific data, domain knowledge, and modeling goals. Experimenting with different approaches is often necessary to find the optimal solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
