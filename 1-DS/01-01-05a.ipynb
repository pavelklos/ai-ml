{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7e776f",
   "metadata": {},
   "source": [
    "# Loading Data from Various Sources (CSV, Excel, JSON)\n",
    "\n",
    "This notebook demonstrates how to load data from various sources including CSV, Excel, and JSON files in data science applications. These are fundamental skills for any data scientist as data acquisition is the first step in any data analysis project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1074155",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6c788a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for JSON handling\n",
    "import json\n",
    "\n",
    "# Libraries for web requests\n",
    "import requests\n",
    "\n",
    "# Library for database connections\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Other useful imports\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ca8db",
   "metadata": {},
   "source": [
    "## Loading CSV Data\n",
    "\n",
    "CSV (Comma-Separated Values) files are one of the most common data formats. Pandas provides powerful functions for importing these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabad8a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CSV loading:\n",
      "   id         name  age           city\n",
      "0   1   John Smith   34       New York\n",
      "1   2     Jane Doe   28  San Francisco\n",
      "2   3  Bob Johnson   45        Chicago\n",
      "3   4  Alice Brown   32         Boston\n"
     ]
    }
   ],
   "source": [
    "# Basic CSV loading\n",
    "# For this demonstration, we'll create a simple CSV in memory\n",
    "csv_data = \"\"\"id,name,age,city\n",
    "1,John Smith,34,New York\n",
    "2,Jane Doe,28,San Francisco\n",
    "3,Bob Johnson,45,Chicago\n",
    "4,Alice Brown,32,Boston\"\"\"\n",
    "\n",
    "# Write to a file\n",
    "with open(\"sample_csv_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Basic read_csv usage\n",
    "df_csv = pd.read_csv(\"sample_csv_data.csv\")\n",
    "print(\"Basic CSV loading:\")\n",
    "print(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ee25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with tab delimiter:\n",
      "   id         name  age           city\n",
      "0   1   John Smith   34       New York\n",
      "1   2     Jane Doe   28  San Francisco\n",
      "2   3  Bob Johnson   45        Chicago\n",
      "3   4  Alice Brown   32         Boston\n",
      "\n",
      "CSV with missing values:\n",
      "   id         name   age      city\n",
      "0   1   John Smith   NaN  New York\n",
      "1   2     Jane Doe  28.0       NaN\n",
      "2   3          NaN  45.0   Chicago\n",
      "3   4  Alice Brown  32.0    Boston\n",
      "\n",
      "CSV with skipped rows and custom types:\n",
      "   id         name   age           city\n",
      "0   1   John Smith  34.0       New York\n",
      "1   2     Jane Doe  28.0  San Francisco\n",
      "2   3  Bob Johnson  45.0        Chicago\n",
      "3   4  Alice Brown  32.0         Boston\n"
     ]
    }
   ],
   "source": [
    "# Advanced CSV loading options\n",
    "\n",
    "# Custom delimiters\n",
    "csv_tab_data = \"\"\"id\\tname\\tage\\tcity\n",
    "1\\tJohn Smith\\t34\\tNew York\n",
    "2\\tJane Doe\\t28\\tSan Francisco\n",
    "3\\tBob Johnson\\t45\\tChicago\n",
    "4\\tAlice Brown\\t32\\tBoston\"\"\"\n",
    "\n",
    "with open(\"sample_csv_tab_data.csv\", \"w\") as f:\n",
    "    f.write(csv_tab_data)\n",
    "\n",
    "df_tab = pd.read_csv(\"sample_csv_tab_data.csv\", delimiter=\"\\t\")\n",
    "print(\"CSV with tab delimiter:\")\n",
    "print(df_tab)\n",
    "\n",
    "# Handling missing values\n",
    "csv_missing_data = \"\"\"id,name,age,city\n",
    "1,John Smith,,New York\n",
    "2,Jane Doe,28,\n",
    "3,,45,Chicago\n",
    "4,Alice Brown,32,Boston\"\"\"\n",
    "\n",
    "with open(\"missing_data.csv\", \"w\") as f:\n",
    "    f.write(csv_missing_data)\n",
    "\n",
    "df_missing = pd.read_csv(\"missing_data.csv\", na_values=[\"\", \"NA\", \"N/A\"])\n",
    "print(\"\\nCSV with missing values:\")\n",
    "print(df_missing)\n",
    "\n",
    "# Skip rows and specify column types\n",
    "csv_messy_data = \"\"\"This is a header line we want to skip\n",
    "This is another line we want to skip\n",
    "id,name,age,city\n",
    "1,John Smith,34,New York\n",
    "2,Jane Doe,28,San Francisco\n",
    "3,Bob Johnson,45,Chicago\n",
    "4,Alice Brown,32,Boston\"\"\"\n",
    "\n",
    "with open(\"messy_data.csv\", \"w\") as f:\n",
    "    f.write(csv_messy_data)\n",
    "\n",
    "df_skipped = pd.read_csv(\"messy_data.csv\", \n",
    "                        skiprows=2,\n",
    "                        dtype={\"id\": int, \"age\": float})\n",
    "print(\"\\nCSV with skipped rows and custom types:\")\n",
    "print(df_skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d5b46",
   "metadata": {},
   "source": [
    "## Loading Excel Data\n",
    "\n",
    "Excel files are ubiquitous in business environments. Pandas can read Excel files (both .xls and .xlsx formats) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457e121c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file created with multiple sheets\n"
     ]
    }
   ],
   "source": [
    "# Create a simple Excel file for demonstration\n",
    "# First, let's create multiple DataFrames to represent different sheets\n",
    "\n",
    "# Sheet 1 - Employee data\n",
    "employee_data = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Alice Brown'],\n",
    "    'Department': ['HR', 'Engineering', 'Marketing', 'Engineering'],\n",
    "    'Salary': [65000, 85000, 72000, 88000]\n",
    "})\n",
    "\n",
    "# Sheet 2 - Department data\n",
    "department_data = pd.DataFrame({\n",
    "    'Department': ['HR', 'Engineering', 'Marketing', 'Sales'],\n",
    "    'Manager': ['Michael Scott', 'Jim Halpert', 'Dwight Schrute', 'Pam Beesly'],\n",
    "    'Budget': [250000, 750000, 400000, 650000]\n",
    "})\n",
    "\n",
    "# Create an Excel file with multiple sheets\n",
    "with pd.ExcelWriter('company_data.xlsx') as writer:\n",
    "    employee_data.to_excel(writer, sheet_name='Employees', index=False)\n",
    "    department_data.to_excel(writer, sheet_name='Departments', index=False)\n",
    "\n",
    "print(\"Excel file created with multiple sheets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7954ef55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Excel reading (first sheet):\n",
      "   ID         Name   Department  Salary\n",
      "0   1   John Smith           HR   65000\n",
      "1   2     Jane Doe  Engineering   85000\n",
      "2   3  Bob Johnson    Marketing   72000\n",
      "3   4  Alice Brown  Engineering   88000\n",
      "\n",
      "Reading specific sheet 'Departments':\n",
      "    Department         Manager  Budget\n",
      "0           HR   Michael Scott  250000\n",
      "1  Engineering     Jim Halpert  750000\n",
      "2    Marketing  Dwight Schrute  400000\n",
      "3        Sales      Pam Beesly  650000\n",
      "\n",
      "All sheets names in the Excel file:\n",
      " - Employees\n",
      " - Departments\n",
      "\n",
      "Customized Excel import (skipping 1 row, only cols A,C,D, first 3 rows):\n",
      "   1           HR  65000\n",
      "0  2  Engineering  85000\n",
      "1  3    Marketing  72000\n",
      "2  4  Engineering  88000\n"
     ]
    }
   ],
   "source": [
    "# Basic reading from Excel\n",
    "df_excel = pd.read_excel('company_data.xlsx')\n",
    "print(\"Default Excel reading (first sheet):\")\n",
    "print(df_excel)\n",
    "\n",
    "# Reading a specific sheet\n",
    "df_excel_dept = pd.read_excel('company_data.xlsx', sheet_name='Departments')\n",
    "print(\"\\nReading specific sheet 'Departments':\")\n",
    "print(df_excel_dept)\n",
    "\n",
    "# Reading multiple sheets\n",
    "all_sheets = pd.read_excel('company_data.xlsx', sheet_name=None)\n",
    "print(\"\\nAll sheets names in the Excel file:\")\n",
    "for sheet_name in all_sheets.keys():\n",
    "    print(f\" - {sheet_name}\")\n",
    "    \n",
    "# Working with specific rows and columns\n",
    "df_excel_subset = pd.read_excel('company_data.xlsx',\n",
    "                               skiprows=1,\n",
    "                               usecols=\"A,C,D\",\n",
    "                               nrows=3)\n",
    "print(\"\\nCustomized Excel import (skipping 1 row, only cols A,C,D, first 3 rows):\")\n",
    "print(df_excel_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c2cc8",
   "metadata": {},
   "source": [
    "## Loading JSON Data\n",
    "\n",
    "JSON (JavaScript Object Notation) is a common format for web APIs and configuration files. Pandas has built-in support for JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c510712f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created\n"
     ]
    }
   ],
   "source": [
    "# Create a sample JSON file\n",
    "json_data = {\n",
    "    \"employees\": [\n",
    "        {\"id\": 1, \"name\": \"John Smith\", \"department\": \"HR\", \"projects\": [\"Recruitment\", \"Training\"]},\n",
    "        {\"id\": 2, \"name\": \"Jane Doe\", \"department\": \"Engineering\", \"projects\": [\"Database\", \"API\"]},\n",
    "        {\"id\": 3, \"name\": \"Bob Johnson\", \"department\": \"Marketing\", \"projects\": [\"Campaign\", \"Social Media\"]},\n",
    "        {\"id\": 4, \"name\": \"Alice Brown\", \"department\": \"Engineering\", \"projects\": [\"Frontend\", \"Mobile App\"]}\n",
    "    ],\n",
    "    \"company_info\": {\n",
    "        \"name\": \"Tech Corp\",\n",
    "        \"founded\": 2005,\n",
    "        \"locations\": [\"New York\", \"San Francisco\", \"London\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write to a JSON file\n",
    "with open(\"company_data.json\", \"w\") as f:\n",
    "    json.dump(json_data, f)\n",
    "\n",
    "print(\"JSON file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8422dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reading JSON with pandas\n",
    "# df_json = pd.read_json(\"company_data.json\")\n",
    "# print(\"Basic JSON reading:\")\n",
    "# print(df_json)\n",
    "\n",
    "# # The above doesn't work well with nested structures, so let's use the json module\n",
    "# with open(\"company_data.json\", \"r\") as f:\n",
    "#     json_data = json.load(f)\n",
    "\n",
    "# # Convert the employees list to a DataFrame\n",
    "# df_employees = pd.DataFrame(json_data['employees'])\n",
    "# print(\"\\nEmployees data from nested JSON:\")\n",
    "# print(df_employees)\n",
    "\n",
    "# # Handling the projects list (which is an array in each record)\n",
    "# # Let's normalize this nested data\n",
    "# df_with_projects = pd.json_normalize(json_data['employees'])\n",
    "# print(\"\\nNormalized JSON data with project arrays:\")\n",
    "# print(df_with_projects)\n",
    "\n",
    "# # Exploding arrays into separate rows\n",
    "# df_exploded = df_with_projects.explode('projects')\n",
    "# print(\"\\nExploded projects into separate rows:\")\n",
    "# print(df_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc2c96-f3ae-4c71-b783-b8d7d0beebf2",
   "metadata": {},
   "source": [
    "```\n",
    "ValueError: Mixing dicts with non-Series may lead to ambiguous ordering.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f8b91f-a74e-4cb8-bb0c-a6e2a74b6ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Employees data from nested JSON:\n",
      "   id         name   department                  projects\n",
      "0   1   John Smith           HR   [Recruitment, Training]\n",
      "1   2     Jane Doe  Engineering           [Database, API]\n",
      "2   3  Bob Johnson    Marketing  [Campaign, Social Media]\n",
      "3   4  Alice Brown  Engineering    [Frontend, Mobile App]\n",
      "\n",
      "Normalized JSON data with project arrays:\n",
      "   id         name   department                  projects\n",
      "0   1   John Smith           HR   [Recruitment, Training]\n",
      "1   2     Jane Doe  Engineering           [Database, API]\n",
      "2   3  Bob Johnson    Marketing  [Campaign, Social Media]\n",
      "3   4  Alice Brown  Engineering    [Frontend, Mobile App]\n",
      "\n",
      "Exploded projects into separate rows:\n",
      "   id         name   department      projects\n",
      "0   1   John Smith           HR   Recruitment\n",
      "0   1   John Smith           HR      Training\n",
      "1   2     Jane Doe  Engineering      Database\n",
      "1   2     Jane Doe  Engineering           API\n",
      "2   3  Bob Johnson    Marketing      Campaign\n",
      "2   3  Bob Johnson    Marketing  Social Media\n",
      "3   4  Alice Brown  Engineering      Frontend\n",
      "3   4  Alice Brown  Engineering    Mobile App\n"
     ]
    }
   ],
   "source": [
    "# Safely open and load the JSON\n",
    "with open(\"company_data.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Convert top-level employees list to a DataFrame\n",
    "df_employees = pd.DataFrame(json_data['employees'])\n",
    "print(\"\\nEmployees data from nested JSON:\")\n",
    "print(df_employees)\n",
    "\n",
    "# Normalize nested structures\n",
    "df_with_projects = pd.json_normalize(json_data['employees'])\n",
    "print(\"\\nNormalized JSON data with project arrays:\")\n",
    "print(df_with_projects)\n",
    "\n",
    "# Explode the list of projects into individual rows\n",
    "df_exploded = df_with_projects.explode('projects')\n",
    "print(\"\\nExploded projects into separate rows:\")\n",
    "print(df_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09648590",
   "metadata": {},
   "source": [
    "## Working with Web APIs\n",
    "\n",
    "Many data sources are available via APIs. We'll use the requests library to fetch data from public APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6109df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response:\n",
      "{'ISBN:9780140328721': {'bib_key': 'ISBN:9780140328721', 'info_url': 'https://openlibrary.org/books/OL7353617M/Fantastic_Mr._Fox', 'preview': 'restricted', 'preview_url': 'https://archive.org/details/fantasticmrfoxpu00roal', 'thumbnail_url': 'https://covers.openlibrary.org/b/id/8739161-S.jpg'}}\n",
      "\n",
      "Book data as DataFrame:\n",
      "              bib_key                                           info_url  \\\n",
      "0  ISBN:9780140328721  https://openlibrary.org/books/OL7353617M/Fanta...   \n",
      "\n",
      "      preview                                        preview_url  \\\n",
      "0  restricted  https://archive.org/details/fantasticmrfoxpu00...   \n",
      "\n",
      "                                       thumbnail_url  \n",
      "0  https://covers.openlibrary.org/b/id/8739161-S.jpg  \n"
     ]
    }
   ],
   "source": [
    "# Example API request - using a public API for demonstration\n",
    "# Let's get book data from the Open Library API\n",
    "\n",
    "try:\n",
    "    # Get information about a book\n",
    "    response = requests.get('https://openlibrary.org/api/books?bibkeys=ISBN:9780140328721&format=json')\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        book_data = response.json()\n",
    "        print(\"API Response:\")\n",
    "        print(book_data)\n",
    "        \n",
    "        # Convert to DataFrame (this is simple data, for more complex nested data \n",
    "        # you might need to flatten it first)\n",
    "        df_book = pd.DataFrame(list(book_data.values()))\n",
    "        print(\"\\nBook data as DataFrame:\")\n",
    "        print(df_book)\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d65efe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users data from API:\n",
      "   id              name   username                      email  \\\n",
      "0   1     Leanne Graham       Bret          Sincere@april.biz   \n",
      "1   2      Ervin Howell  Antonette          Shanna@melissa.tv   \n",
      "2   3  Clementine Bauch   Samantha         Nathan@yesenia.net   \n",
      "3   4  Patricia Lebsack   Karianne  Julianne.OConner@kory.org   \n",
      "4   5  Chelsey Dietrich     Kamren   Lucio_Hettinger@annie.ca   \n",
      "\n",
      "                   phone        website     address.street address.suite  \\\n",
      "0  1-770-736-8031 x56442  hildegard.org        Kulas Light      Apt. 556   \n",
      "1    010-692-6593 x09125  anastasia.net      Victor Plains     Suite 879   \n",
      "2         1-463-123-4447    ramiro.info  Douglas Extension     Suite 847   \n",
      "3      493-170-9623 x156       kale.biz        Hoeger Mall      Apt. 692   \n",
      "4          (254)954-1289   demarco.info       Skiles Walks     Suite 351   \n",
      "\n",
      "    address.city address.zipcode address.geo.lat address.geo.lng  \\\n",
      "0    Gwenborough      92998-3874        -37.3159         81.1496   \n",
      "1    Wisokyburgh      90566-7771        -43.9509        -34.4618   \n",
      "2  McKenziehaven      59590-4157        -68.6102        -47.0653   \n",
      "3    South Elvis      53919-4257         29.4572       -164.2990   \n",
      "4     Roscoeview           33263        -31.8129         62.5342   \n",
      "\n",
      "         company.name                       company.catchPhrase  \\\n",
      "0     Romaguera-Crona    Multi-layered client-server neural-net   \n",
      "1        Deckow-Crist            Proactive didactic contingency   \n",
      "2  Romaguera-Jacobson         Face to face bifurcated interface   \n",
      "3       Robel-Corkery  Multi-tiered zero tolerance productivity   \n",
      "4         Keebler LLC      User-centric fault-tolerant solution   \n",
      "\n",
      "                             company.bs  \n",
      "0           harness real-time e-markets  \n",
      "1      synergize scalable supply-chains  \n",
      "2       e-enable strategic applications  \n",
      "3  transition cutting-edge web services  \n",
      "4      revolutionize end-to-end systems  \n",
      "\n",
      "Selected user information:\n",
      "   id              name                      email        company.name\n",
      "0   1     Leanne Graham          Sincere@april.biz     Romaguera-Crona\n",
      "1   2      Ervin Howell          Shanna@melissa.tv        Deckow-Crist\n",
      "2   3  Clementine Bauch         Nathan@yesenia.net  Romaguera-Jacobson\n",
      "3   4  Patricia Lebsack  Julianne.OConner@kory.org       Robel-Corkery\n",
      "4   5  Chelsey Dietrich   Lucio_Hettinger@annie.ca         Keebler LLC\n"
     ]
    }
   ],
   "source": [
    "# Getting a list of items from a JSON API\n",
    "try:\n",
    "    # Request a list of users from JSONPlaceholder API\n",
    "    response = requests.get('https://jsonplaceholder.typicode.com/users')\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        users_data = response.json()\n",
    "        \n",
    "        # Convert the list of users to a DataFrame\n",
    "        df_users = pd.json_normalize(users_data)\n",
    "        print(\"Users data from API:\")\n",
    "        print(df_users.head())\n",
    "        \n",
    "        # We can select specific columns\n",
    "        df_users_subset = df_users[['id', 'name', 'email', 'company.name']]\n",
    "        print(\"\\nSelected user information:\")\n",
    "        print(df_users_subset.head())\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c061b60",
   "metadata": {},
   "source": [
    "## Loading Data from Databases\n",
    "\n",
    "Connecting to databases is a core skill for data scientists. Pandas can directly interface with SQL databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75eb4770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SQLite database created with sample data\n"
     ]
    }
   ],
   "source": [
    "# Create an in-memory SQLite database for demonstration\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE employees (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    department TEXT,\n",
    "    salary REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE departments (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    manager TEXT,\n",
    "    budget REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert data\n",
    "employees_data = [\n",
    "    (1, 'John Smith', 'HR', 65000),\n",
    "    (2, 'Jane Doe', 'Engineering', 85000),\n",
    "    (3, 'Bob Johnson', 'Marketing', 72000),\n",
    "    (4, 'Alice Brown', 'Engineering', 88000)\n",
    "]\n",
    "\n",
    "departments_data = [\n",
    "    (1, 'HR', 'Michael Scott', 250000),\n",
    "    (2, 'Engineering', 'Jim Halpert', 750000),\n",
    "    (3, 'Marketing', 'Dwight Schrute', 400000),\n",
    "    (4, 'Sales', 'Pam Beesly', 650000)\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT INTO employees VALUES (?, ?, ?, ?)', employees_data)\n",
    "cursor.executemany('INSERT INTO departments VALUES (?, ?, ?, ?)', departments_data)\n",
    "conn.commit()\n",
    "\n",
    "print(\"SQLite database created with sample data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "677a6e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees from SQL:\n",
      "   id         name   department   salary\n",
      "0   1   John Smith           HR  65000.0\n",
      "1   2     Jane Doe  Engineering  85000.0\n",
      "2   3  Bob Johnson    Marketing  72000.0\n",
      "3   4  Alice Brown  Engineering  88000.0\n",
      "\n",
      "Joined data from SQL:\n",
      "  employee_name   salary   department         manager\n",
      "0    John Smith  65000.0           HR   Michael Scott\n",
      "1      Jane Doe  85000.0  Engineering     Jim Halpert\n",
      "2   Bob Johnson  72000.0    Marketing  Dwight Schrute\n",
      "3   Alice Brown  88000.0  Engineering     Jim Halpert\n",
      "\n",
      "Data written to and read from database using SQLAlchemy:\n",
      "   id           name department  salary\n",
      "0   5  Charlie Davis      Sales   67000\n",
      "1   6    Diana Evans         IT   92000\n"
     ]
    }
   ],
   "source": [
    "# Reading data from SQLite with pandas\n",
    "df_employees_sql = pd.read_sql_query(\"SELECT * FROM employees\", conn)\n",
    "print(\"Employees from SQL:\")\n",
    "print(df_employees_sql)\n",
    "\n",
    "# Join tables with SQL\n",
    "df_joined = pd.read_sql_query('''\n",
    "    SELECT e.name as employee_name, \n",
    "           e.salary, \n",
    "           e.department,\n",
    "           d.manager\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.name\n",
    "''', conn)\n",
    "\n",
    "print(\"\\nJoined data from SQL:\")\n",
    "print(df_joined)\n",
    "\n",
    "# Using SQLAlchemy for database connections\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# We can also write DataFrames back to the database\n",
    "df_new = pd.DataFrame({\n",
    "    'id': [5, 6],\n",
    "    'name': ['Charlie Davis', 'Diana Evans'],\n",
    "    'department': ['Sales', 'IT'],\n",
    "    'salary': [67000, 92000]\n",
    "})\n",
    "\n",
    "df_new.to_sql('new_employees', engine, index=False, if_exists='replace')\n",
    "\n",
    "# And read it back\n",
    "df_read_back = pd.read_sql('new_employees', engine)\n",
    "print(\"\\nData written to and read from database using SQLAlchemy:\")\n",
    "print(df_read_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afb0e3",
   "metadata": {},
   "source": [
    "## Handling Different File Encodings\n",
    "\n",
    "Working with international data often means dealing with different character encodings. Pandas can handle various encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ff122e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with international characters read with UTF-8 encoding:\n",
      "   id               name  country     city\n",
      "0   1        José García    Spain   Madrid\n",
      "1   2    François Dupont   France    Paris\n",
      "2   3      Jürgen Müller  Germany   Berlin\n",
      "3   4                黄小明    China  Beijing\n",
      "4   5  Екатерина Иванова   Russia   Moscow\n",
      "\n",
      "CSV with Latin-1 encoding:\n",
      "   id             name  country     city\n",
      "0   1      José García    Spain   Madrid\n",
      "1   2  François Dupont   France    Paris\n",
      "2   3    Jurgen Muller  Germany   Berlin\n",
      "3   4          Unknown    China  Beijing\n",
      "4   5        Ekaterina   Russia   Moscow\n",
      "\n",
      "Error reading with wrong encoding: 'ascii' codec can't decode byte 0xc3 in position 27: ordinal not in range(128)\n"
     ]
    }
   ],
   "source": [
    "# Create a CSV with non-ASCII characters\n",
    "csv_international = \"\"\"id,name,country,city\n",
    "1,José García,Spain,Madrid\n",
    "2,François Dupont,France,Paris\n",
    "3,Jürgen Müller,Germany,Berlin\n",
    "4,黄小明,China,Beijing\n",
    "5,Екатерина Иванова,Russia,Moscow\"\"\"\n",
    "\n",
    "# Write with UTF-8 encoding\n",
    "with open(\"international_data.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(csv_international)\n",
    "\n",
    "# Read with UTF-8 encoding\n",
    "df_utf8 = pd.read_csv(\"international_data.csv\", encoding=\"utf-8\")\n",
    "print(\"CSV with international characters read with UTF-8 encoding:\")\n",
    "print(df_utf8)\n",
    "\n",
    "# Let's simulate a file with different encoding\n",
    "with open(\"international_data_latin1.csv\", \"w\", encoding=\"latin1\") as f:\n",
    "    # Note: This will lose some characters that aren't in latin1\n",
    "    try:\n",
    "        f.write(csv_international)\n",
    "    except UnicodeEncodeError:\n",
    "        # This is expected, we'll modify the data to make it fit latin1\n",
    "        simplified_csv = \"\"\"id,name,country,city\n",
    "1,José García,Spain,Madrid\n",
    "2,François Dupont,France,Paris\n",
    "3,Jurgen Muller,Germany,Berlin\n",
    "4,Unknown,China,Beijing\n",
    "5,Ekaterina,Russia,Moscow\"\"\"\n",
    "        f.write(simplified_csv)\n",
    "\n",
    "# Reading with correct encoding \n",
    "df_latin1 = pd.read_csv(\"international_data_latin1.csv\", encoding=\"latin1\")\n",
    "print(\"\\nCSV with Latin-1 encoding:\")\n",
    "print(df_latin1)\n",
    "\n",
    "# Trying to read a file with the wrong encoding can lead to errors or garbled text\n",
    "try:\n",
    "    # This might fail or produce garbled text\n",
    "    df_wrong = pd.read_csv(\"international_data.csv\", encoding=\"ascii\")\n",
    "    print(\"\\nReading UTF-8 file with ASCII encoding (may show garbled text):\")\n",
    "    print(df_wrong)\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"\\nError reading with wrong encoding: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1dedd",
   "metadata": {},
   "source": [
    "## Comparing Data Loading Methods\n",
    "\n",
    "Different data formats have different advantages and performance characteristics. Let's compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4e158c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 100000 rows for performance testing\n",
      "Saved in csv format\n",
      "Saved in excel format\n",
      "Saved in json format\n",
      "Saved in pickle format\n",
      "Saved in parquet format\n"
     ]
    }
   ],
   "source": [
    "# Create a larger dataset for performance comparison\n",
    "rows = 100000\n",
    "ids = list(range(1, rows + 1))\n",
    "names = [f\"Person_{i}\" for i in range(1, rows + 1)]\n",
    "values = np.random.randn(rows)\n",
    "categories = np.random.choice(['A', 'B', 'C', 'D'], rows)\n",
    "\n",
    "big_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'name': names,\n",
    "    'value': values,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "print(f\"Created dataset with {rows} rows for performance testing\")\n",
    "\n",
    "# Save in different formats for comparison\n",
    "formats = {\n",
    "    'csv': {'func': big_df.to_csv, 'args': {'path_or_buf': 'big_data.csv', 'index': False}},\n",
    "    'excel': {'func': big_df.to_excel, 'args': {'excel_writer': 'big_data.xlsx', 'index': False}},\n",
    "    'json': {'func': big_df.to_json, 'args': {'path_or_buf': 'big_data.json'}},\n",
    "    'pickle': {'func': big_df.to_pickle, 'args': {'path': 'big_data.pkl'}},\n",
    "    'parquet': {'func': big_df.to_parquet, 'args': {'path': 'big_data.parquet', 'index': False}}\n",
    "}\n",
    "\n",
    "# Save in different formats\n",
    "for fmt, config in formats.items():\n",
    "    try:\n",
    "        config['func'](**config['args'])\n",
    "        print(f\"Saved in {fmt} format\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving in {fmt} format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f861667e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded csv in 0.1795 seconds - 100000 rows\n",
      "Loaded excel in 7.5136 seconds - 100000 rows\n",
      "Loaded json in 0.3067 seconds - 100000 rows\n",
      "Loaded pickle in 0.0294 seconds - 100000 rows\n",
      "Loaded parquet in 0.0761 seconds - 100000 rows\n",
      "\n",
      "Performance comparison:\n",
      "         duration    rows  columns\n",
      "pickle   0.029425  100000        4\n",
      "parquet  0.076087  100000        4\n",
      "csv      0.179482  100000        4\n",
      "json     0.306654  100000        4\n",
      "excel    7.513570  100000        4\n",
      "\n",
      "File size comparison:\n",
      "         Size (MB)\n",
      "parquet   2.183290\n",
      "excel     3.176146\n",
      "csv       3.949280\n",
      "json      6.649483\n"
     ]
    }
   ],
   "source": [
    "# Compare loading speeds\n",
    "load_functions = {\n",
    "    'csv': {'func': pd.read_csv, 'args': {'filepath_or_buffer': 'big_data.csv'}},\n",
    "    'excel': {'func': pd.read_excel, 'args': {'io': 'big_data.xlsx'}},\n",
    "    'json': {'func': pd.read_json, 'args': {'path_or_buf': 'big_data.json'}},\n",
    "    'pickle': {'func': pd.read_pickle, 'args': {'filepath_or_buffer': 'big_data.pkl'}},\n",
    "    'parquet': {'func': pd.read_parquet, 'args': {'path': 'big_data.parquet'}}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for fmt, config in load_functions.items():\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        df = config['func'](**config['args'])\n",
    "        duration = time.time() - start_time\n",
    "        results[fmt] = {'duration': duration, 'rows': len(df), 'columns': len(df.columns)}\n",
    "        print(f\"Loaded {fmt} in {duration:.4f} seconds - {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {fmt}: {e}\")\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "comparison_df = comparison_df.sort_values(by='duration')\n",
    "\n",
    "print(\"\\nPerformance comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# # Show file sizes\n",
    "# file_sizes = {}\n",
    "# for fmt in formats.keys():\n",
    "#     try:\n",
    "#         file_path = f\"big_data.{fmt}\"\n",
    "#         if fmt == 'parquet':\n",
    "#             file_path = 'big_data.parquet'\n",
    "        \n",
    "#         if os.path.exists(file_path):\n",
    "#             size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "#             file_sizes[fmt] = size_mb\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error getting file size for {fmt}: {e}\")\n",
    "\n",
    "# Show file sizes\n",
    "file_sizes = {}\n",
    "for fmt in load_functions.keys():\n",
    "    try:\n",
    "        file_path = f\"big_data.{fmt if fmt != 'excel' else 'xlsx'}\"\n",
    "        if os.path.exists(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            file_sizes[fmt] = size_mb\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file size for {fmt}: {e}\")\n",
    "\n",
    "file_sizes_df = pd.DataFrame.from_dict(file_sizes, orient='index', columns=['Size (MB)'])\n",
    "file_sizes_df = file_sizes_df.sort_values(by='Size (MB)')\n",
    "\n",
    "print(\"\\nFile size comparison:\")\n",
    "print(file_sizes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4d0da",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored various methods to load data from different sources:\n",
    "\n",
    "1. **CSV files** - The most common format, easy to read and write, but lacks type information\n",
    "2. **Excel files** - Great for business data, supports multiple sheets, but slower to process\n",
    "3. **JSON data** - Standard for web APIs, handles nested structures, but can be complex to normalize\n",
    "4. **Databases** - Scalable, support complex queries, but require connection setup\n",
    "5. **Web APIs** - Access to external data, but may require authentication and handling rate limits\n",
    "\n",
    "Key takeaways:\n",
    "- Choose the right format based on your data size, structure, and processing needs\n",
    "- Parquet and Pickle formats are much faster for large datasets\n",
    "- Consider file size if storage is a concern\n",
    "- Pay attention to encodings when working with international data\n",
    "- Always inspect your data after loading to verify it was loaded correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
