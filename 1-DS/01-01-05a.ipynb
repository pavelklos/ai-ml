{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7e776f",
   "metadata": {},
   "source": [
    "# Loading Data from Various Sources (CSV, Excel, JSON)\n",
    "\n",
    "This notebook demonstrates how to load data from various sources including CSV, Excel, and JSON files in data science applications. These are fundamental skills for any data scientist as data acquisition is the first step in any data analysis project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1074155",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c788a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for JSON handling\n",
    "import json\n",
    "\n",
    "# Libraries for web requests\n",
    "import requests\n",
    "\n",
    "# Library for database connections\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Other useful imports\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113ca8db",
   "metadata": {},
   "source": [
    "## Loading CSV Data\n",
    "\n",
    "CSV (Comma-Separated Values) files are one of the most common data formats. Pandas provides powerful functions for importing these files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabad8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CSV loading\n",
    "# For this demonstration, we'll create a simple CSV in memory\n",
    "csv_data = \"\"\"id,name,age,city\n",
    "1,John Smith,34,New York\n",
    "2,Jane Doe,28,San Francisco\n",
    "3,Bob Johnson,45,Chicago\n",
    "4,Alice Brown,32,Boston\"\"\"\n",
    "\n",
    "# Write to a file\n",
    "with open(\"sample_data.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Basic read_csv usage\n",
    "df_csv = pd.read_csv(\"sample_data.csv\")\n",
    "print(\"Basic CSV loading:\")\n",
    "print(df_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ee25f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced CSV loading options\n",
    "\n",
    "# Custom delimiters\n",
    "csv_tab_data = \"\"\"id\\tname\\tage\\tcity\n",
    "1\\tJohn Smith\\t34\\tNew York\n",
    "2\\tJane Doe\\t28\\tSan Francisco\n",
    "3\\tBob Johnson\\t45\\tChicago\n",
    "4\\tAlice Brown\\t32\\tBoston\"\"\"\n",
    "\n",
    "with open(\"sample_tab_data.csv\", \"w\") as f:\n",
    "    f.write(csv_tab_data)\n",
    "\n",
    "df_tab = pd.read_csv(\"sample_tab_data.csv\", delimiter=\"\\t\")\n",
    "print(\"CSV with tab delimiter:\")\n",
    "print(df_tab)\n",
    "\n",
    "# Handling missing values\n",
    "csv_missing_data = \"\"\"id,name,age,city\n",
    "1,John Smith,,New York\n",
    "2,Jane Doe,28,\n",
    "3,,45,Chicago\n",
    "4,Alice Brown,32,Boston\"\"\"\n",
    "\n",
    "with open(\"missing_data.csv\", \"w\") as f:\n",
    "    f.write(csv_missing_data)\n",
    "\n",
    "df_missing = pd.read_csv(\"missing_data.csv\", na_values=[\"\", \"NA\", \"N/A\"])\n",
    "print(\"\\nCSV with missing values:\")\n",
    "print(df_missing)\n",
    "\n",
    "# Skip rows and specify column types\n",
    "csv_messy_data = \"\"\"This is a header line we want to skip\n",
    "This is another line we want to skip\n",
    "id,name,age,city\n",
    "1,John Smith,34,New York\n",
    "2,Jane Doe,28,San Francisco\n",
    "3,Bob Johnson,45,Chicago\n",
    "4,Alice Brown,32,Boston\"\"\"\n",
    "\n",
    "with open(\"messy_data.csv\", \"w\") as f:\n",
    "    f.write(csv_messy_data)\n",
    "\n",
    "df_skipped = pd.read_csv(\"messy_data.csv\", \n",
    "                        skiprows=2,\n",
    "                        dtype={\"id\": int, \"age\": float})\n",
    "print(\"\\nCSV with skipped rows and custom types:\")\n",
    "print(df_skipped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640d5b46",
   "metadata": {},
   "source": [
    "## Loading Excel Data\n",
    "\n",
    "Excel files are ubiquitous in business environments. Pandas can read Excel files (both .xls and .xlsx formats) directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457e121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple Excel file for demonstration\n",
    "# First, let's create multiple DataFrames to represent different sheets\n",
    "\n",
    "# Sheet 1 - Employee data\n",
    "employee_data = pd.DataFrame({\n",
    "    'ID': [1, 2, 3, 4],\n",
    "    'Name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Alice Brown'],\n",
    "    'Department': ['HR', 'Engineering', 'Marketing', 'Engineering'],\n",
    "    'Salary': [65000, 85000, 72000, 88000]\n",
    "})\n",
    "\n",
    "# Sheet 2 - Department data\n",
    "department_data = pd.DataFrame({\n",
    "    'Department': ['HR', 'Engineering', 'Marketing', 'Sales'],\n",
    "    'Manager': ['Michael Scott', 'Jim Halpert', 'Dwight Schrute', 'Pam Beesly'],\n",
    "    'Budget': [250000, 750000, 400000, 650000]\n",
    "})\n",
    "\n",
    "# Create an Excel file with multiple sheets\n",
    "with pd.ExcelWriter('company_data.xlsx') as writer:\n",
    "    employee_data.to_excel(writer, sheet_name='Employees', index=False)\n",
    "    department_data.to_excel(writer, sheet_name='Departments', index=False)\n",
    "\n",
    "print(\"Excel file created with multiple sheets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7954ef55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic reading from Excel\n",
    "df_excel = pd.read_excel('company_data.xlsx')\n",
    "print(\"Default Excel reading (first sheet):\")\n",
    "print(df_excel)\n",
    "\n",
    "# Reading a specific sheet\n",
    "df_excel_dept = pd.read_excel('company_data.xlsx', sheet_name='Departments')\n",
    "print(\"\\nReading specific sheet 'Departments':\")\n",
    "print(df_excel_dept)\n",
    "\n",
    "# Reading multiple sheets\n",
    "all_sheets = pd.read_excel('company_data.xlsx', sheet_name=None)\n",
    "print(\"\\nAll sheets names in the Excel file:\")\n",
    "for sheet_name in all_sheets.keys():\n",
    "    print(f\" - {sheet_name}\")\n",
    "    \n",
    "# Working with specific rows and columns\n",
    "df_excel_subset = pd.read_excel('company_data.xlsx',\n",
    "                               skiprows=1,\n",
    "                               usecols=\"A,C,D\",\n",
    "                               nrows=3)\n",
    "print(\"\\nCustomized Excel import (skipping 1 row, only cols A,C,D, first 3 rows):\")\n",
    "print(df_excel_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c2cc8",
   "metadata": {},
   "source": [
    "## Loading JSON Data\n",
    "\n",
    "JSON (JavaScript Object Notation) is a common format for web APIs and configuration files. Pandas has built-in support for JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c510712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample JSON file\n",
    "json_data = {\n",
    "    \"employees\": [\n",
    "        {\"id\": 1, \"name\": \"John Smith\", \"department\": \"HR\", \"projects\": [\"Recruitment\", \"Training\"]},\n",
    "        {\"id\": 2, \"name\": \"Jane Doe\", \"department\": \"Engineering\", \"projects\": [\"Database\", \"API\"]},\n",
    "        {\"id\": 3, \"name\": \"Bob Johnson\", \"department\": \"Marketing\", \"projects\": [\"Campaign\", \"Social Media\"]},\n",
    "        {\"id\": 4, \"name\": \"Alice Brown\", \"department\": \"Engineering\", \"projects\": [\"Frontend\", \"Mobile App\"]}\n",
    "    ],\n",
    "    \"company_info\": {\n",
    "        \"name\": \"Tech Corp\",\n",
    "        \"founded\": 2005,\n",
    "        \"locations\": [\"New York\", \"San Francisco\", \"London\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write to a JSON file\n",
    "with open(\"company_data.json\", \"w\") as f:\n",
    "    json.dump(json_data, f)\n",
    "\n",
    "print(\"JSON file created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8422dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON with pandas\n",
    "df_json = pd.read_json(\"company_data.json\")\n",
    "print(\"Basic JSON reading:\")\n",
    "print(df_json)\n",
    "\n",
    "# The above doesn't work well with nested structures, so let's use the json module\n",
    "with open(\"company_data.json\", \"r\") as f:\n",
    "    json_data = json.load(f)\n",
    "\n",
    "# Convert the employees list to a DataFrame\n",
    "df_employees = pd.DataFrame(json_data['employees'])\n",
    "print(\"\\nEmployees data from nested JSON:\")\n",
    "print(df_employees)\n",
    "\n",
    "# Handling the projects list (which is an array in each record)\n",
    "# Let's normalize this nested data\n",
    "df_with_projects = pd.json_normalize(json_data['employees'])\n",
    "print(\"\\nNormalized JSON data with project arrays:\")\n",
    "print(df_with_projects)\n",
    "\n",
    "# Exploding arrays into separate rows\n",
    "df_exploded = df_with_projects.explode('projects')\n",
    "print(\"\\nExploded projects into separate rows:\")\n",
    "print(df_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09648590",
   "metadata": {},
   "source": [
    "## Working with Web APIs\n",
    "\n",
    "Many data sources are available via APIs. We'll use the requests library to fetch data from public APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6109df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example API request - using a public API for demonstration\n",
    "# Let's get book data from the Open Library API\n",
    "\n",
    "try:\n",
    "    # Get information about a book\n",
    "    response = requests.get('https://openlibrary.org/api/books?bibkeys=ISBN:9780140328721&format=json')\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        book_data = response.json()\n",
    "        print(\"API Response:\")\n",
    "        print(book_data)\n",
    "        \n",
    "        # Convert to DataFrame (this is simple data, for more complex nested data \n",
    "        # you might need to flatten it first)\n",
    "        df_book = pd.DataFrame(list(book_data.values()))\n",
    "        print(\"\\nBook data as DataFrame:\")\n",
    "        print(df_book)\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65efe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting a list of items from a JSON API\n",
    "try:\n",
    "    # Request a list of users from JSONPlaceholder API\n",
    "    response = requests.get('https://jsonplaceholder.typicode.com/users')\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        users_data = response.json()\n",
    "        \n",
    "        # Convert the list of users to a DataFrame\n",
    "        df_users = pd.json_normalize(users_data)\n",
    "        print(\"Users data from API:\")\n",
    "        print(df_users.head())\n",
    "        \n",
    "        # We can select specific columns\n",
    "        df_users_subset = df_users[['id', 'name', 'email', 'company.name']]\n",
    "        print(\"\\nSelected user information:\")\n",
    "        print(df_users_subset.head())\n",
    "    else:\n",
    "        print(f\"Error fetching data: {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c061b60",
   "metadata": {},
   "source": [
    "## Loading Data from Databases\n",
    "\n",
    "Connecting to databases is a core skill for data scientists. Pandas can directly interface with SQL databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eb4770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an in-memory SQLite database for demonstration\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create tables\n",
    "cursor.execute('''\n",
    "CREATE TABLE employees (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    department TEXT,\n",
    "    salary REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "cursor.execute('''\n",
    "CREATE TABLE departments (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    manager TEXT,\n",
    "    budget REAL\n",
    ")\n",
    "''')\n",
    "\n",
    "# Insert data\n",
    "employees_data = [\n",
    "    (1, 'John Smith', 'HR', 65000),\n",
    "    (2, 'Jane Doe', 'Engineering', 85000),\n",
    "    (3, 'Bob Johnson', 'Marketing', 72000),\n",
    "    (4, 'Alice Brown', 'Engineering', 88000)\n",
    "]\n",
    "\n",
    "departments_data = [\n",
    "    (1, 'HR', 'Michael Scott', 250000),\n",
    "    (2, 'Engineering', 'Jim Halpert', 750000),\n",
    "    (3, 'Marketing', 'Dwight Schrute', 400000),\n",
    "    (4, 'Sales', 'Pam Beesly', 650000)\n",
    "]\n",
    "\n",
    "cursor.executemany('INSERT INTO employees VALUES (?, ?, ?, ?)', employees_data)\n",
    "cursor.executemany('INSERT INTO departments VALUES (?, ?, ?, ?)', departments_data)\n",
    "conn.commit()\n",
    "\n",
    "print(\"SQLite database created with sample data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677a6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from SQLite with pandas\n",
    "df_employees_sql = pd.read_sql_query(\"SELECT * FROM employees\", conn)\n",
    "print(\"Employees from SQL:\")\n",
    "print(df_employees_sql)\n",
    "\n",
    "# Join tables with SQL\n",
    "df_joined = pd.read_sql_query('''\n",
    "    SELECT e.name as employee_name, \n",
    "           e.salary, \n",
    "           e.department,\n",
    "           d.manager\n",
    "    FROM employees e\n",
    "    JOIN departments d ON e.department = d.name\n",
    "''', conn)\n",
    "\n",
    "print(\"\\nJoined data from SQL:\")\n",
    "print(df_joined)\n",
    "\n",
    "# Using SQLAlchemy for database connections\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "\n",
    "# We can also write DataFrames back to the database\n",
    "df_new = pd.DataFrame({\n",
    "    'id': [5, 6],\n",
    "    'name': ['Charlie Davis', 'Diana Evans'],\n",
    "    'department': ['Sales', 'IT'],\n",
    "    'salary': [67000, 92000]\n",
    "})\n",
    "\n",
    "df_new.to_sql('new_employees', engine, index=False, if_exists='replace')\n",
    "\n",
    "# And read it back\n",
    "df_read_back = pd.read_sql('new_employees', engine)\n",
    "print(\"\\nData written to and read from database using SQLAlchemy:\")\n",
    "print(df_read_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6afb0e3",
   "metadata": {},
   "source": [
    "## Handling Different File Encodings\n",
    "\n",
    "Working with international data often means dealing with different character encodings. Pandas can handle various encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff122e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV with non-ASCII characters\n",
    "csv_international = \"\"\"id,name,country,city\n",
    "1,José García,Spain,Madrid\n",
    "2,François Dupont,France,Paris\n",
    "3,Jürgen Müller,Germany,Berlin\n",
    "4,黄小明,China,Beijing\n",
    "5,Екатерина Иванова,Russia,Moscow\"\"\"\n",
    "\n",
    "# Write with UTF-8 encoding\n",
    "with open(\"international_data.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(csv_international)\n",
    "\n",
    "# Read with UTF-8 encoding\n",
    "df_utf8 = pd.read_csv(\"international_data.csv\", encoding=\"utf-8\")\n",
    "print(\"CSV with international characters read with UTF-8 encoding:\")\n",
    "print(df_utf8)\n",
    "\n",
    "# Let's simulate a file with different encoding\n",
    "with open(\"international_data_latin1.csv\", \"w\", encoding=\"latin1\") as f:\n",
    "    # Note: This will lose some characters that aren't in latin1\n",
    "    try:\n",
    "        f.write(csv_international)\n",
    "    except UnicodeEncodeError:\n",
    "        # This is expected, we'll modify the data to make it fit latin1\n",
    "        simplified_csv = \"\"\"id,name,country,city\n",
    "1,José García,Spain,Madrid\n",
    "2,François Dupont,France,Paris\n",
    "3,Jurgen Muller,Germany,Berlin\n",
    "4,Unknown,China,Beijing\n",
    "5,Ekaterina,Russia,Moscow\"\"\"\n",
    "        f.write(simplified_csv)\n",
    "\n",
    "# Reading with correct encoding \n",
    "df_latin1 = pd.read_csv(\"international_data_latin1.csv\", encoding=\"latin1\")\n",
    "print(\"\\nCSV with Latin-1 encoding:\")\n",
    "print(df_latin1)\n",
    "\n",
    "# Trying to read a file with the wrong encoding can lead to errors or garbled text\n",
    "try:\n",
    "    # This might fail or produce garbled text\n",
    "    df_wrong = pd.read_csv(\"international_data.csv\", encoding=\"ascii\")\n",
    "    print(\"\\nReading UTF-8 file with ASCII encoding (may show garbled text):\")\n",
    "    print(df_wrong)\n",
    "except UnicodeDecodeError as e:\n",
    "    print(f\"\\nError reading with wrong encoding: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e1dedd",
   "metadata": {},
   "source": [
    "## Comparing Data Loading Methods\n",
    "\n",
    "Different data formats have different advantages and performance characteristics. Let's compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e158c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a larger dataset for performance comparison\n",
    "rows = 100000\n",
    "ids = list(range(1, rows + 1))\n",
    "names = [f\"Person_{i}\" for i in range(1, rows + 1)]\n",
    "values = np.random.randn(rows)\n",
    "categories = np.random.choice(['A', 'B', 'C', 'D'], rows)\n",
    "\n",
    "big_df = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'name': names,\n",
    "    'value': values,\n",
    "    'category': categories\n",
    "})\n",
    "\n",
    "print(f\"Created dataset with {rows} rows for performance testing\")\n",
    "\n",
    "# Save in different formats for comparison\n",
    "formats = {\n",
    "    'csv': {'func': big_df.to_csv, 'args': {'path_or_buf': 'big_data.csv', 'index': False}},\n",
    "    'excel': {'func': big_df.to_excel, 'args': {'excel_writer': 'big_data.xlsx', 'index': False}},\n",
    "    'json': {'func': big_df.to_json, 'args': {'path_or_buf': 'big_data.json'}},\n",
    "    'pickle': {'func': big_df.to_pickle, 'args': {'path': 'big_data.pkl'}},\n",
    "    'parquet': {'func': big_df.to_parquet, 'args': {'path': 'big_data.parquet', 'index': False}}\n",
    "}\n",
    "\n",
    "# Save in different formats\n",
    "for fmt, config in formats.items():\n",
    "    try:\n",
    "        config['func'](**config['args'])\n",
    "        print(f\"Saved in {fmt} format\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving in {fmt} format: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f861667e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare loading speeds\n",
    "load_functions = {\n",
    "    'csv': {'func': pd.read_csv, 'args': {'filepath_or_buffer': 'big_data.csv'}},\n",
    "    'excel': {'func': pd.read_excel, 'args': {'io': 'big_data.xlsx'}},\n",
    "    'json': {'func': pd.read_json, 'args': {'path_or_buf': 'big_data.json'}},\n",
    "    'pickle': {'func': pd.read_pickle, 'args': {'filepath_or_buffer': 'big_data.pkl'}},\n",
    "    'parquet': {'func': pd.read_parquet, 'args': {'path': 'big_data.parquet'}}\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for fmt, config in load_functions.items():\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        df = config['func'](**config['args'])\n",
    "        duration = time.time() - start_time\n",
    "        results[fmt] = {'duration': duration, 'rows': len(df), 'columns': len(df.columns)}\n",
    "        print(f\"Loaded {fmt} in {duration:.4f} seconds - {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {fmt}: {e}\")\n",
    "\n",
    "# Create a comparison DataFrame\n",
    "comparison_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "comparison_df = comparison_df.sort_values(by='duration')\n",
    "\n",
    "print(\"\\nPerformance comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Show file sizes\n",
    "file_sizes = {}\n",
    "for fmt in formats.keys():\n",
    "    try:\n",
    "        file_path = f\"big_data.{fmt}\"\n",
    "        if fmt == 'parquet':\n",
    "            file_path = 'big_data.parquet'\n",
    "        \n",
    "        if os.path.exists(file_path):\n",
    "            size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            file_sizes[fmt] = size_mb\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting file size for {fmt}: {e}\")\n",
    "\n",
    "file_sizes_df = pd.DataFrame.from_dict(file_sizes, orient='index', columns=['Size (MB)'])\n",
    "file_sizes_df = file_sizes_df.sort_values(by='Size (MB)')\n",
    "\n",
    "print(\"\\nFile size comparison:\")\n",
    "print(file_sizes_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e4d0da",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored various methods to load data from different sources:\n",
    "\n",
    "1. **CSV files** - The most common format, easy to read and write, but lacks type information\n",
    "2. **Excel files** - Great for business data, supports multiple sheets, but slower to process\n",
    "3. **JSON data** - Standard for web APIs, handles nested structures, but can be complex to normalize\n",
    "4. **Databases** - Scalable, support complex queries, but require connection setup\n",
    "5. **Web APIs** - Access to external data, but may require authentication and handling rate limits\n",
    "\n",
    "Key takeaways:\n",
    "- Choose the right format based on your data size, structure, and processing needs\n",
    "- Parquet and Pickle formats are much faster for large datasets\n",
    "- Consider file size if storage is a concern\n",
    "- Pay attention to encodings when working with international data\n",
    "- Always inspect your data after loading to verify it was loaded correctly"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
