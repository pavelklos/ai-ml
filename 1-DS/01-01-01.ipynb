{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b2c1e41",
   "metadata": {},
   "source": [
    "# Introduction to Data Science Concepts\n",
    "\n",
    "This notebook provides a comprehensive introduction to data science concepts, covering fundamental definitions, processes, and applications. We'll explore the interdisciplinary nature of data science and understand its key components through practical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ab786a",
   "metadata": {},
   "source": [
    "## What is Data Science?\n",
    "\n",
    "Data Science is an interdisciplinary field that combines elements from:\n",
    "- Statistics\n",
    "- Computer Science\n",
    "- Domain Expertise\n",
    "- Mathematics\n",
    "\n",
    "It involves extracting knowledge and insights from structured and unstructured data using scientific methods, algorithms, and systems.\n",
    "\n",
    "Key aspects:\n",
    "- Uses both statistical and computational approaches\n",
    "- Focuses on extracting actionable insights from data\n",
    "- Requires strong analytical and critical thinking\n",
    "- Involves communicating findings to stakeholders\n",
    "\n",
    "Let's explore the Python ecosystem for data science:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cce9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common data science libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Display versions to show our data science ecosystem\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.__version__}\")\n",
    "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
    "\n",
    "# Create a simple array with NumPy\n",
    "array = np.array([1, 2, 3, 4, 5])\n",
    "print(\"\\nNumPy array:\", array)\n",
    "print(\"Array mean:\", np.mean(array))\n",
    "\n",
    "# Create a simple DataFrame with pandas\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, 3, 4, 5],\n",
    "    'B': [10, 20, 30, 40, 50]\n",
    "})\n",
    "print(\"\\nPandas DataFrame:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352fcb76",
   "metadata": {},
   "source": [
    "## The Data Science Process\n",
    "\n",
    "The data science process typically follows these steps:\n",
    "\n",
    "1. **Problem Definition**: Clearly define the question or problem to be solved\n",
    "2. **Data Collection**: Gather relevant data from various sources\n",
    "3. **Data Cleaning**: Handle missing values, outliers, and inconsistencies\n",
    "4. **Exploratory Data Analysis**: Understand patterns and relationships in the data\n",
    "5. **Feature Engineering**: Create new features or transform existing ones\n",
    "6. **Modeling**: Build predictive or descriptive models\n",
    "7. **Evaluation**: Assess model performance against metrics\n",
    "8. **Deployment**: Implement the solution in a production environment\n",
    "9. **Monitoring**: Track the performance and update as needed\n",
    "\n",
    "Let's demonstrate a simple workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3cf2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Data Science Workflow Example\n",
    "\n",
    "# 1. Data Collection - Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "print(f\"Target names: {target_names}\")\n",
    "\n",
    "# 2. Data Preparation - Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"\\nTraining set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# 3. Build a simple model\n",
    "model = linear_model.LogisticRegression(max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Evaluate the model\n",
    "score = model.score(X_test, y_test)\n",
    "print(f\"\\nModel accuracy: {score:.4f}\")\n",
    "\n",
    "# 5. Make predictions\n",
    "sample = X_test[:3]\n",
    "predictions = model.predict(sample)\n",
    "print(\"\\nPredictions for 3 samples:\")\n",
    "for i, prediction in enumerate(predictions):\n",
    "    print(f\"Sample {i+1}: Predicted as {target_names[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c94b564",
   "metadata": {},
   "source": [
    "## Key Components of Data Science\n",
    "\n",
    "Data science integrates several key components:\n",
    "\n",
    "### 1. Statistics\n",
    "- Descriptive statistics\n",
    "- Inferential statistics\n",
    "- Hypothesis testing\n",
    "- Probability theory\n",
    "\n",
    "### 2. Programming\n",
    "- Data manipulation\n",
    "- Algorithm implementation\n",
    "- Database management\n",
    "- Data visualization\n",
    "\n",
    "### 3. Domain Knowledge\n",
    "- Understanding the business context\n",
    "- Formulating relevant questions\n",
    "- Interpreting results properly\n",
    "- Providing actionable recommendations\n",
    "\n",
    "### 4. Communication\n",
    "- Data storytelling\n",
    "- Visualization\n",
    "- Presentation skills\n",
    "- Technical writing\n",
    "\n",
    "Let's see how these components interact in practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0eed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showing components interacting\n",
    "\n",
    "# 1. Programming - Load and prepare the data\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings\n",
    "\n",
    "boston = load_boston()\n",
    "boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "boston_df['PRICE'] = boston.target\n",
    "\n",
    "print(\"Boston Housing Dataset:\")\n",
    "print(boston_df.head())\n",
    "\n",
    "# 2. Statistics - Perform statistical analysis\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "print(boston_df[['PRICE', 'RM', 'LSTAT']].describe())\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = boston_df['PRICE'].corr(boston_df['RM'])\n",
    "print(f\"\\nCorrelation between PRICE and RM (avg number of rooms): {correlation:.4f}\")\n",
    "\n",
    "# 3. Visualization - Create informative plots\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(boston_df['RM'], boston_df['PRICE'], alpha=0.7)\n",
    "plt.title('Price vs. Room Count')\n",
    "plt.xlabel('Average Number of Rooms')\n",
    "plt.ylabel('House Price ($1000s)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(boston_df['LSTAT'], boston_df['PRICE'], alpha=0.7)\n",
    "plt.title('Price vs. Lower Status Population')\n",
    "plt.xlabel('% Lower Status Population')\n",
    "plt.ylabel('House Price ($1000s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Domain Knowledge - Interpret findings\n",
    "print(\"\\nDomain Knowledge Insights:\")\n",
    "print(\"- Houses with more rooms tend to be more expensive (positive correlation)\")\n",
    "print(\"- Areas with higher percentage of lower status population have lower house prices (negative correlation)\")\n",
    "print(\"- Understanding local real estate factors would help refine analysis further\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11b3596",
   "metadata": {},
   "source": [
    "## Types of Data\n",
    "\n",
    "Understanding different types of data is fundamental to data science:\n",
    "\n",
    "### Data Structure\n",
    "- **Structured data**: Organized in a predefined format (e.g., tables, spreadsheets)\n",
    "- **Unstructured data**: No predefined format (e.g., text, images, audio)\n",
    "- **Semi-structured data**: Some organizational properties but not rigid (e.g., JSON, XML)\n",
    "\n",
    "### Data Types\n",
    "- **Numerical**:\n",
    "  - Continuous (e.g., height, temperature)\n",
    "  - Discrete (e.g., count of items)\n",
    "- **Categorical**:\n",
    "  - Nominal (no order, e.g., colors)\n",
    "  - Ordinal (ordered, e.g., education level)\n",
    "- **Time series**: Data points indexed in time order\n",
    "- **Text data**: Unstructured linguistic information\n",
    "- **Geospatial data**: Information tied to geographic locations\n",
    "\n",
    "Let's explore different data types and how to handle them in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9ab268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating and manipulating different types of data\n",
    "\n",
    "# Create a dataset with various data types\n",
    "data = {\n",
    "    # Numerical data\n",
    "    'age': [25, 30, 35, 40, 45],\n",
    "    'income': [50000, 60000, 75000, 90000, 120000],\n",
    "    'years_experience': [2, 5, 7, 10, 15],\n",
    "    \n",
    "    # Categorical data\n",
    "    'education': ['Bachelors', 'Masters', 'PhD', 'Bachelors', 'Masters'],\n",
    "    'department': ['Sales', 'Engineering', 'Marketing', 'HR', 'Engineering'],\n",
    "    \n",
    "    # Date/time data\n",
    "    'hire_date': pd.date_range(start='2018-01-01', periods=5, freq='6M'),\n",
    "    \n",
    "    # Text data\n",
    "    'feedback': [\n",
    "        'Excellent team player',\n",
    "        'Strong analytical skills',\n",
    "        'Good communication',\n",
    "        'Detail oriented',\n",
    "        'Creative problem solver'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "employee_df = pd.DataFrame(data)\n",
    "print(\"Employee DataFrame with Multiple Data Types:\")\n",
    "print(employee_df)\n",
    "\n",
    "# Working with different data types\n",
    "print(\"\\n1. Numerical Data Summary:\")\n",
    "print(employee_df[['age', 'income']].describe())\n",
    "\n",
    "print(\"\\n2. Categorical Data:\")\n",
    "# Convert to categorical type\n",
    "employee_df['education'] = employee_df['education'].astype('category')\n",
    "print(employee_df['education'].value_counts())\n",
    "\n",
    "print(\"\\n3. Date/Time Data:\")\n",
    "# Extract year and month from date\n",
    "employee_df['hire_year'] = employee_df['hire_date'].dt.year\n",
    "employee_df['hire_month'] = employee_df['hire_date'].dt.month\n",
    "print(employee_df[['hire_date', 'hire_year', 'hire_month']].head())\n",
    "\n",
    "print(\"\\n4. Text Data:\")\n",
    "# Simple text analysis - count words\n",
    "employee_df['feedback_word_count'] = employee_df['feedback'].apply(lambda x: len(x.split()))\n",
    "print(employee_df[['feedback', 'feedback_word_count']])\n",
    "\n",
    "# Visualize categorical and numerical data\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Categorical data\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.countplot(x='education', data=employee_df)\n",
    "plt.title('Education Distribution')\n",
    "\n",
    "# Numerical data\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(employee_df['age'], kde=True)\n",
    "plt.title('Age Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bac895",
   "metadata": {},
   "source": [
    "## Statistical Foundations\n",
    "\n",
    "Statistics forms the backbone of data science. Here are key statistical concepts every data scientist should understand:\n",
    "\n",
    "### Descriptive Statistics\n",
    "- Central tendency (mean, median, mode)\n",
    "- Variability (variance, standard deviation, range)\n",
    "- Distributions (normal, skewed, bimodal)\n",
    "- Percentiles and quartiles\n",
    "\n",
    "### Inferential Statistics\n",
    "- Hypothesis testing\n",
    "- Confidence intervals\n",
    "- p-values and statistical significance\n",
    "- Regression analysis\n",
    "\n",
    "### Probability\n",
    "- Probability distributions\n",
    "- Conditional probability\n",
    "- Bayes' theorem\n",
    "- Random variables\n",
    "\n",
    "Let's explore these concepts with code examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7929a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data for statistical analysis\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create two samples with different distributions\n",
    "sample1 = np.random.normal(loc=50, scale=10, size=1000)  # Normal distribution\n",
    "sample2 = np.random.exponential(scale=10, size=1000)     # Exponential distribution\n",
    "\n",
    "# Combine into a DataFrame\n",
    "stats_df = pd.DataFrame({\n",
    "    'normal': sample1,\n",
    "    'exponential': sample2\n",
    "})\n",
    "\n",
    "# Central tendency measures\n",
    "mean_values = stats_df.mean()\n",
    "median_values = stats_df.median()\n",
    "mode_values = stats_df.mode().iloc[0]  # Mode can have multiple values\n",
    "\n",
    "print(\"Central Tendency:\")\n",
    "print(f\"Mean: {mean_values}\")\n",
    "print(f\"Median: {median_values}\")\n",
    "print(f\"Mode (first value): {mode_values}\")\n",
    "\n",
    "# Measures of variability\n",
    "std_values = stats_df.std()\n",
    "var_values = stats_df.var()\n",
    "range_values = stats_df.max() - stats_df.min()\n",
    "\n",
    "print(\"\\nVariability:\")\n",
    "print(f\"Standard Deviation: {std_values}\")\n",
    "print(f\"Variance: {var_values}\")\n",
    "print(f\"Range: {range_values}\")\n",
    "\n",
    "# Percentiles\n",
    "percentiles = stats_df.quantile([0.25, 0.5, 0.75])\n",
    "print(\"\\nPercentiles:\")\n",
    "print(percentiles)\n",
    "\n",
    "# Visualize distributions\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Histograms\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.hist(sample1, bins=30, alpha=0.7, color='blue')\n",
    "plt.title('Normal Distribution Histogram')\n",
    "plt.axvline(np.mean(sample1), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(np.median(sample1), color='green', linestyle='dashed', linewidth=1)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(sample2, bins=30, alpha=0.7, color='orange')\n",
    "plt.title('Exponential Distribution Histogram')\n",
    "plt.axvline(np.mean(sample2), color='red', linestyle='dashed', linewidth=1)\n",
    "plt.axvline(np.median(sample2), color='green', linestyle='dashed', linewidth=1)\n",
    "\n",
    "# Box plots\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(data=stats_df)\n",
    "plt.title('Box Plots Comparison')\n",
    "\n",
    "# Q-Q Plot for normality check\n",
    "plt.subplot(2, 2, 4)\n",
    "from scipy import stats\n",
    "stats.probplot(sample1, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot (Normal Distribution)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation\n",
    "correlation = np.corrcoef(sample1, sample2)[0, 1]\n",
    "print(f\"\\nCorrelation between samples: {correlation:.4f}\")\n",
    "\n",
    "# Perform t-test between the two samples\n",
    "t_stat, p_value = stats.ttest_ind(sample1, sample2)\n",
    "print(\"\\nIndependent t-test results:\")\n",
    "print(f\"t-statistic: {t_stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.10f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"The two samples are significantly different (p < 0.05)\")\n",
    "else:\n",
    "    print(\"No significant difference between samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f00ce0e",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a critical step in the data science process that involves:\n",
    "\n",
    "1. **Understanding the structure** of your data\n",
    "2. **Discovering patterns** and relationships\n",
    "3. **Identifying anomalies** and outliers\n",
    "4. **Generating hypotheses** for further investigation\n",
    "5. **Visualizing key relationships** between variables\n",
    "\n",
    "EDA helps guide subsequent data preprocessing and modeling steps. Let's demonstrate some EDA techniques:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d0c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's perform EDA on a real dataset\n",
    "# We'll use the Titanic dataset, a classic for data exploration\n",
    "\n",
    "# Load the dataset\n",
    "titanic_df = sns.load_dataset('titanic')\n",
    "\n",
    "# First look at the data\n",
    "print(\"Titanic Dataset Sample:\")\n",
    "print(titanic_df.head())\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(\"\\nDataset Information:\")\n",
    "print(f\"Number of rows and columns: {titanic_df.shape}\")\n",
    "print(\"\\nColumn types:\")\n",
    "print(titanic_df.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(titanic_df.isnull().sum())\n",
    "\n",
    "# Basic summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(titanic_df.describe())\n",
    "\n",
    "# Categorical variable distributions\n",
    "print(\"\\nPassenger Class Distribution:\")\n",
    "print(titanic_df['class'].value_counts())\n",
    "\n",
    "print(\"\\nGender Distribution:\")\n",
    "print(titanic_df['sex'].value_counts())\n",
    "\n",
    "print(\"\\nSurvival Distribution:\")\n",
    "print(titanic_df['survived'].value_counts())\n",
    "print(f\"Survival rate: {titanic_df['survived'].mean():.2%}\")\n",
    "\n",
    "# Visualize relationships\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Survival rate by passenger class\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.countplot(x='class', hue='survived', data=titanic_df)\n",
    "plt.title('Survival by Passenger Class')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Survival rate by gender\n",
    "plt.subplot(2, 3, 2)\n",
    "sns.countplot(x='sex', hue='survived', data=titanic_df)\n",
    "plt.title('Survival by Gender')\n",
    "\n",
    "# Survival rate by age group\n",
    "plt.subplot(2, 3, 3)\n",
    "# Create age groups\n",
    "titanic_df['age_group'] = pd.cut(titanic_df['age'], \n",
    "                                bins=[0, 12, 18, 35, 60, 100], \n",
    "                                labels=['Child', 'Teen', 'Young Adult', 'Adult', 'Senior'])\n",
    "sns.countplot(x='age_group', hue='survived', data=titanic_df)\n",
    "plt.title('Survival by Age Group')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Age distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.histplot(data=titanic_df, x='age', hue='survived', multiple='stack', bins=20)\n",
    "plt.title('Age Distribution by Survival')\n",
    "\n",
    "# Fare vs. Class\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.boxplot(x='class', y='fare', data=titanic_df)\n",
    "plt.title('Fare by Passenger Class')\n",
    "\n",
    "# Correlation heatmap of numerical variables\n",
    "plt.subplot(2, 3, 6)\n",
    "numerical_df = titanic_df.select_dtypes(include=['float64', 'int64'])\n",
    "sns.heatmap(numerical_df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Advanced analysis: Survival rate by combination of features\n",
    "print(\"\\nSurvival Rate by Gender and Class:\")\n",
    "survival_by_gender_class = titanic_df.groupby(['sex', 'class'])['survived'].mean()\n",
    "print(survival_by_gender_class)\n",
    "\n",
    "# Visualize this relationship\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='class', y='survived', hue='sex', data=titanic_df)\n",
    "plt.title('Survival Rate by Gender and Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3255aa5",
   "metadata": {},
   "source": [
    "## Machine Learning Overview\n",
    "\n",
    "Machine learning is a subset of data science focused on building models that can learn from data and make predictions or decisions. Here's a brief overview:\n",
    "\n",
    "### Types of Machine Learning:\n",
    "\n",
    "1. **Supervised Learning**: Models learn from labeled data to predict outcomes\n",
    "   - Classification: Predicting categories (e.g., spam detection)\n",
    "   - Regression: Predicting continuous values (e.g., price prediction)\n",
    "\n",
    "2. **Unsupervised Learning**: Models find patterns in unlabeled data\n",
    "   - Clustering: Grouping similar data points (e.g., customer segmentation)\n",
    "   - Dimensionality Reduction: Reducing features while preserving information\n",
    "   \n",
    "3. **Reinforcement Learning**: Models learn by interacting with an environment\n",
    "   - Agents learn optimal actions through trial and error\n",
    "\n",
    "Let's demonstrate basic examples of supervised and unsupervised learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba02507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning Examples\n",
    "\n",
    "# 1. Supervised Learning - Classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Load a dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Classification Results (Random Forest on Iris):\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': clf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# 2. Supervised Learning - Regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create simple regression data\n",
    "np.random.seed(42)\n",
    "X_reg = np.random.rand(100, 1) * 10\n",
    "y_reg = 2 * X_reg.squeeze() + 1 + np.random.randn(100) * 2\n",
    "\n",
    "# Split data\n",
    "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "reg_model = LinearRegression()\n",
    "reg_model.fit(X_train_reg, y_train_reg)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_reg = reg_model.predict(X_test_reg)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg)\n",
    "\n",
    "print(\"\\nRegression Results:\")\n",
    "print(f\"Coefficient: {reg_model.coef_[0]:.4f}\")\n",
    "print(f\"Intercept: {reg_model.intercept_:.4f}\")\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# 3. Unsupervised Learning - Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Use the first two features from iris for visualization\n",
    "X_cluster = iris.data[:, :2]\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_cluster)\n",
    "\n",
    "# Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Supervised learning - regression plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test_reg, y_test_reg, color='blue', label='Actual')\n",
    "plt.plot(X_test_reg, y_pred_reg, color='red', linewidth=2, label='Predicted')\n",
    "plt.title('Linear Regression: Actual vs Predicted')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "\n",
    "# Unsupervised learning - clustering plot\n",
    "plt.subplot(1, 2, 2)\n",
    "for i in range(3):\n",
    "    plt.scatter(X_scaled[clusters == i, 0], X_scaled[clusters == i, 1], \n",
    "                label=f'Cluster {i}', alpha=0.7)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "           s=100, c='black', marker='X', label='Centroids')\n",
    "plt.title('K-Means Clustering of Iris Data')\n",
    "plt.xlabel(f'Scaled {feature_names[0]}')\n",
    "plt.ylabel(f'Scaled {feature_names[1]}')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49d1b77",
   "metadata": {},
   "source": [
    "## Data Visualization Basics\n",
    "\n",
    "Data visualization is a crucial skill for data scientists to communicate findings effectively. Good visualizations:\n",
    "\n",
    "- **Reveal insights** hidden in the data\n",
    "- Make complex information **more accessible**\n",
    "- Help stakeholders **understand the story** in the data\n",
    "- Guide **decision-making processes**\n",
    "\n",
    "Let's explore some fundamental visualization techniques using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb0cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization Examples\n",
    "\n",
    "# Create some data to visualize\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2023-01-01', periods=12, freq='M')\n",
    "sales_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'store_a': np.random.randint(100, 200, size=12) * 100,\n",
    "    'store_b': np.random.randint(80, 160, size=12) * 100,\n",
    "    'store_c': np.random.randint(120, 220, size=12) * 100,\n",
    "    'temperature': np.random.normal(70, 10, size=12),\n",
    "    'promotion': np.random.choice([0, 1], size=12, p=[0.7, 0.3])\n",
    "})\n",
    "\n",
    "sales_data['month'] = sales_data['date'].dt.month_name()\n",
    "sales_data['total_sales'] = sales_data[['store_a', 'store_b', 'store_c']].sum(axis=1)\n",
    "\n",
    "print(\"Sales Data Sample:\")\n",
    "print(sales_data.head())\n",
    "\n",
    "# Create a variety of visualizations\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# 1. Line Chart - Time Series\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(sales_data['date'], sales_data['store_a'], marker='o', label='Store A')\n",
    "plt.plot(sales_data['date'], sales_data['store_b'], marker='s', label='Store B')\n",
    "plt.plot(sales_data['date'], sales_data['store_c'], marker='^', label='Store C')\n",
    "plt.title('Monthly Sales by Store')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales ($)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# 2. Bar Chart - Comparison\n",
    "plt.subplot(2, 2, 2)\n",
    "monthly_totals = sales_data.groupby('month')['total_sales'].mean().reindex(\n",
    "    ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "     'July', 'August', 'September', 'October', 'November', 'December']\n",
    ")\n",
    "bars = plt.bar(monthly_totals.index, monthly_totals.values, color='skyblue')\n",
    "plt.title('Average Total Sales by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Sales ($)')\n",
    "plt.xticks(rotation=45)\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 1000,\n",
    "             f'{int(height):,}', ha='center', va='bottom', rotation=0)\n",
    "\n",
    "# 3. Scatter Plot - Relationship between variables\n",
    "plt.subplot(2, 2, 3)\n",
    "promotion_color = ['blue' if p == 0 else 'red' for p in sales_data['promotion']]\n",
    "plt.scatter(sales_data['temperature'], sales_data['total_sales'], \n",
    "           c=promotion_color, alpha=0.7, s=100)\n",
    "plt.title('Sales vs. Temperature')\n",
    "plt.xlabel('Temperature (°F)')\n",
    "plt.ylabel('Total Sales ($)')\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "\n",
    "# Add a legend for promotion\n",
    "from matplotlib.lines import Line2D\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='No Promotion'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='With Promotion')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='best')\n",
    "\n",
    "# 4. Heatmap - Correlation matrix\n",
    "plt.subplot(2, 2, 4)\n",
    "corr_matrix = sales_data.drop(['date', 'month', 'promotion'], axis=1).corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create additional visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 5. Pie Chart - Composition\n",
    "plt.subplot(2, 2, 1)\n",
    "store_totals = sales_data[['store_a', 'store_b', 'store_c']].sum()\n",
    "plt.pie(store_totals, labels=store_totals.index, autopct='%1.1f%%', \n",
    "        startangle=90, shadow=True, explode=(0.05, 0, 0))\n",
    "plt.title('Proportion of Sales by Store')\n",
    "\n",
    "# 6. Box Plot - Distribution and Outliers\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.boxplot(data=sales_data[['store_a', 'store_b', 'store_c']])\n",
    "plt.title('Sales Distribution by Store')\n",
    "plt.ylabel('Sales ($)')\n",
    "\n",
    "# 7. Histogram - Distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.hist(sales_data['total_sales'], bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution of Total Monthly Sales')\n",
    "plt.xlabel('Total Sales ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# 8. Stacked Area Chart - Composition over time\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.stackplot(sales_data['date'], \n",
    "              sales_data['store_a'], \n",
    "              sales_data['store_b'], \n",
    "              sales_data['store_c'],\n",
    "              labels=['Store A', 'Store B', 'Store C'],\n",
    "              alpha=0.7)\n",
    "plt.title('Sales Composition Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Sales ($)')\n",
    "plt.legend(loc='upper left')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ecee3d5",
   "metadata": {},
   "source": [
    "## Real-World Applications\n",
    "\n",
    "Data science has transformative applications across industries. Let's explore a real-world example using a publicly available dataset to demonstrate a complete end-to-end workflow:\n",
    "\n",
    "1. Business understanding\n",
    "2. Data collection and preparation\n",
    "3. Exploratory analysis\n",
    "4. Feature engineering\n",
    "5. Model building and evaluation\n",
    "6. Deriving insights\n",
    "\n",
    "For this example, we'll use the California Housing dataset to predict housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17f8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-World Application: California Housing Price Prediction\n",
    "\n",
    "# 1. Business Understanding & Data Collection\n",
    "# Goal: Build a model to predict housing prices in California\n",
    "# This could help real estate companies, homeowners, and buyers make informed decisions\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Load dataset\n",
    "housing = fetch_california_housing()\n",
    "print(\"California Housing Dataset:\")\n",
    "print(f\"Number of samples: {housing.data.shape[0]}\")\n",
    "print(f\"Number of features: {housing.data.shape[1]}\")\n",
    "print(f\"Feature names: {housing.feature_names}\")\n",
    "\n",
    "# 2. Data Preparation\n",
    "# Create DataFrame\n",
    "housing_df = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "housing_df['PRICE'] = housing.target\n",
    "\n",
    "print(\"\\nSample of the dataset:\")\n",
    "print(housing_df.head())\n",
    "\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(housing_df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(housing_df.isnull().sum())\n",
    "\n",
    "# 3. Exploratory Data Analysis\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.subplot(2, 3, 1)\n",
    "corr_matrix = housing_df.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', linewidths=0.5, fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "\n",
    "# Scatter plot: Median Income vs Price\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(housing_df['MedInc'], housing_df['PRICE'], alpha=0.5)\n",
    "plt.title('Price vs. Median Income')\n",
    "plt.xlabel('Median Income ($10,000s)')\n",
    "plt.ylabel('Price ($100,000s)')\n",
    "\n",
    "# Scatter plot: House Age vs Price\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(housing_df['HouseAge'], housing_df['PRICE'], alpha=0.5)\n",
    "plt.title('Price vs. House Age')\n",
    "plt.xlabel('House Age (years)')\n",
    "plt.ylabel('Price ($100,000s)')\n",
    "\n",
    "# Distribution of prices\n",
    "plt.subplot(2, 3, 4)\n",
    "sns.histplot(housing_df['PRICE'], kde=True)\n",
    "plt.title('Distribution of Housing Prices')\n",
    "plt.xlabel('Price ($100,000s)')\n",
    "\n",
    "# Distribution of median income\n",
    "plt.subplot(2, 3, 5)\n",
    "sns.histplot(housing_df['MedInc'], kde=True)\n",
    "plt.title('Distribution of Median Income')\n",
    "plt.xlabel('Median Income ($10,000s)')\n",
    "\n",
    "# Location vs Price (using Latitude and Longitude)\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(housing_df['Longitude'], housing_df['Latitude'], \n",
    "           c=housing_df['PRICE'], cmap='viridis', \n",
    "           s=housing_df['Population']/100, alpha=0.6)\n",
    "plt.title('Housing Prices Across California')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.colorbar(label='Price ($100,000s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Feature Engineering\n",
    "# Create new features\n",
    "housing_df['RoomsPerHousehold'] = housing_df['AveRooms'] / housing_df['AveOccup']\n",
    "housing_df['PopulationPerHousehold'] = housing_df['Population'] / housing_df['Households']\n",
    "housing_df['BedroomsPerRoom'] = housing_df['AveBedrms'] / housing_df['AveRooms']\n",
    "\n",
    "# Check most important features using correlation\n",
    "print(\"\\nCorrelation with PRICE after feature engineering:\")\n",
    "correlations = housing_df.corr()['PRICE'].sort_values(ascending=False)\n",
    "print(correlations)\n",
    "\n",
    "# 5. Model Building and Evaluation\n",
    "# Prepare data for modeling\n",
    "X = housing_df.drop('PRICE', axis=1)\n",
    "y = housing_df['PRICE']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with preprocessing and model\n",
    "models = {\n",
    "    'Linear Regression': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', LinearRegression())\n",
    "    ]),\n",
    "    'Random Forest': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "    ]),\n",
    "    'Gradient Boosting': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('model', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "results = {}\n",
    "for name, pipeline in models.items():\n",
    "    # Train model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{name} Results:\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "\n",
    "# 6. Model Comparison\n",
    "metrics_df = pd.DataFrame(results).T\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(metrics_df)\n",
    "\n",
    "# Visualize model performance\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "metrics_df['RMSE'].plot(kind='bar', color='salmon')\n",
    "plt.title('RMSE by Model (Lower is Better)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "metrics_df['R²'].plot(kind='bar', color='skyblue')\n",
    "plt.title('R² by Model (Higher is Better)')\n",
    "plt.ylabel('R²')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Feature Importance (for best model - Random Forest)\n",
    "best_model = models['Random Forest'].named_steps['model']\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': best_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importances)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances)\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.xlabel('Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Insights and Conclusions\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"1. Median income is the strongest predictor of housing prices\")\n",
    "print(\"2. Location factors (latitude/longitude) significantly impact pricing\")\n",
    "print(\"3. Population density shows correlation with housing prices\")\n",
    "print(\"4. Housing age has a complex relationship with price (potentially non-linear)\")\n",
    "print(\"5. Gradient Boosting provides the best predictive performance for this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93578011",
   "metadata": {},
   "source": [
    "## Ethics in Data Science\n",
    "\n",
    "Ethics is a critical consideration in data science. As data scientists, we have a responsibility to:\n",
    "\n",
    "1. **Protect privacy** and handle sensitive data appropriately\n",
    "2. **Avoid bias** in our data collection and models\n",
    "3. **Ensure fairness** in model predictions across different groups\n",
    "4. **Be transparent** about methodology and limitations\n",
    "5. **Consider societal implications** of our work\n",
    "\n",
    "Let's explore how we can identify and address bias in data and models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbf5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ethics in Data Science: Examining Bias\n",
    "\n",
    "# 1. Create a synthetic dataset with demographic information and credit decisions\n",
    "np.random.seed(42)\n",
    "n_samples = 1000\n",
    "\n",
    "# Generate synthetic data with inherent bias\n",
    "age = np.random.normal(40, 10, n_samples)\n",
    "income = np.random.normal(60000, 20000, n_samples)\n",
    "gender = np.random.choice(['Male', 'Female'], size=n_samples, p=[0.5, 0.5])\n",
    "\n",
    "# Creating bias: females with same qualifications get approved less often\n",
    "credit_score = np.random.normal(700, 100, n_samples)\n",
    "# Artificially lower credit scores for females to introduce bias\n",
    "credit_score[gender == 'Female'] = credit_score[gender == 'Female'] - 30\n",
    "\n",
    "# Generate approval based on credit score with a bias threshold\n",
    "approval_threshold_male = 650\n",
    "approval_threshold_female = 680  # Higher threshold for females (bias)\n",
    "\n",
    "approval = np.zeros(n_samples, dtype=bool)\n",
    "for i in range(n_samples):\n",
    "    if gender[i] == 'Male':\n",
    "        approval[i] = credit_score[i] > approval_threshold_male\n",
    "    else:\n",
    "        approval[i] = credit_score[i] > approval_threshold_female\n",
    "\n",
    "# Create DataFrame\n",
    "credit_df = pd.DataFrame({\n",
    "    'Age': age,\n",
    "    'Income': income,\n",
    "    'Gender': gender,\n",
    "    'CreditScore': credit_score,\n",
    "    'Approved': approval\n",
    "})\n",
    "\n",
    "print(\"Synthetic Credit Approval Dataset:\")\n",
    "print(credit_df.head())\n",
    "\n",
    "# 2. Examine bias in the dataset\n",
    "# Calculate approval rates by gender\n",
    "approval_by_gender = credit_df.groupby('Gender')['Approved'].mean()\n",
    "print(\"\\nApproval Rate by Gender:\")\n",
    "print(approval_by_gender)\n",
    "print(f\"Approval gap: {(approval_by_gender['Male'] - approval_by_gender['Female']) * 100:.2f}%\")\n",
    "\n",
    "# Visualize the bias\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Approval rates by gender\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='Gender', y='Approved', data=credit_df)\n",
    "plt.title('Credit Approval Rate by Gender')\n",
    "plt.ylabel('Approval Rate')\n",
    "\n",
    "# Credit score distributions by gender\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.kdeplot(data=credit_df, x='CreditScore', hue='Gender', fill=True, common_norm=False)\n",
    "plt.title('Credit Score Distribution by Gender')\n",
    "plt.axvline(x=approval_threshold_male, color='blue', linestyle='--', label='Male Threshold')\n",
    "plt.axvline(x=approval_threshold_female, color='orange', linestyle='--', label='Female Threshold')\n",
    "plt.legend()\n",
    "\n",
    "# Approval rates by gender and income bracket\n",
    "plt.subplot(2, 2, 3)\n",
    "credit_df['IncomeGroup'] = pd.qcut(credit_df['Income'], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])\n",
    "sns.barplot(x='IncomeGroup', y='Approved', hue='Gender', data=credit_df)\n",
    "plt.title('Approval Rate by Income Group and Gender')\n",
    "plt.ylabel('Approval Rate')\n",
    "plt.xlabel('Income Group')\n",
    "\n",
    "# Approval rates by gender and age group\n",
    "plt.subplot(2, 2, 4)\n",
    "credit_df['AgeGroup'] = pd.cut(credit_df['Age'], bins=[20, 30, 40, 50, 60, 70], labels=['20-30', '30-40', '40-50', '50-60', '60-70'])\n",
    "sns.barplot(x='AgeGroup', y='Approved', hue='Gender', data=credit_df)\n",
    "plt.title('Approval Rate by Age Group and Gender')\n",
    "plt.ylabel('Approval Rate')\n",
    "plt.xlabel('Age Group')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Build a model and check for bias in predictions\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Prepare data for modeling\n",
    "X = credit_df[['Age', 'Income', 'CreditScore', 'Gender']]\n",
    "y = credit_df['Approved']\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "X_encoded = pd.get_dummies(X, drop_first=True)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Overall model performance\n",
    "print(\"\\nOverall Model Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Check for bias in model predictions\n",
    "test_results = pd.DataFrame({\n",
    "    'Gender': X.loc[X_test.index, 'Gender'],\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred\n",
    "})\n",
    "\n",
    "# Calculate metrics by gender\n",
    "gender_metrics = {}\n",
    "for gender in ['Male', 'Female']:\n",
    "    gender_subset = test_results[test_results['Gender'] == gender]\n",
    "    true_pos = ((gender_subset['Actual'] == True) & (gender_subset['Predicted'] == True)).sum()\n",
    "    false_pos = ((gender_subset['Actual'] == False) & (gender_subset['Predicted'] == True)).sum()\n",
    "    true_neg = ((gender_subset['Actual'] == False) & (gender_subset['Predicted'] == False)).sum()\n",
    "    false_neg = ((gender_subset['Actual'] == True) & (gender_subset['Predicted'] == False)).sum()\n",
    "    \n",
    "    precision = true_pos / (true_pos + false_pos) if (true_pos + false_pos) > 0 else 0\n",
    "    recall = true_pos / (true_pos + false_neg) if (true_pos + false_neg) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    gender_metrics[gender] = {\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-score': f1,\n",
    "        'Approval Rate': gender_subset['Predicted'].mean()\n",
    "    }\n",
    "\n",
    "gender_metrics_df = pd.DataFrame(gender_metrics).T\n",
    "print(\"\\nModel Performance by Gender:\")\n",
    "print(gender_metrics_df)\n",
    "\n",
    "# 4. Techniques to address bias\n",
    "print(\"\\n=== Techniques to Address Bias in Data Science ===\")\n",
    "print(\"1. Data Collection and Preprocessing:\")\n",
    "print(\"   - Ensure representative sampling across all groups\")\n",
    "print(\"   - Collect diverse data from multiple sources\")\n",
    "print(\"   - Careful feature selection to avoid proxies for sensitive attributes\")\n",
    "print(\"   - Apply reweighting techniques for underrepresented groups\")\n",
    "\n",
    "print(\"\\n2. Modeling Approaches:\")\n",
    "print(\"   - Use fairness constraints during model training\")\n",
    "print(\"   - Implement adversarial debiasing techniques\")\n",
    "print(\"   - Test models across different population segments\")\n",
    "print(\"   - Apply fairness-aware algorithms\")\n",
    "\n",
    "print(\"\\n3. Post-Processing Methods:\")\n",
    "print(\"   - Adjust decision thresholds for different groups to ensure equal outcomes\")\n",
    "print(\"   - Implement calibration techniques\")\n",
    "print(\"   - Use model explanation tools to understand and address biases\")\n",
    "\n",
    "print(\"\\n4. Evaluation and Monitoring:\")\n",
    "print(\"   - Regularly audit models for fairness across different demographic groups\")\n",
    "print(\"   - Use disaggregated evaluation metrics\")\n",
    "print(\"   - Monitor model performance and bias over time\")\n",
    "print(\"   - Create feedback loops to continuously improve fairness\")\n",
    "\n",
    "# 5. Example of a fairness-aware model (adjusting thresholds)\n",
    "# Let's demonstrate threshold adjustment to achieve similar approval rates\n",
    "\n",
    "# Extract probabilities instead of binary predictions\n",
    "y_prob = model.predict_proba(X_test)[:, 1]  # Probability of approval\n",
    "\n",
    "# Create results with probabilities\n",
    "results_with_prob = pd.DataFrame({\n",
    "    'Gender': X.loc[X_test.index, 'Gender'],\n",
    "    'Actual': y_test,\n",
    "    'Probability': y_prob\n",
    "})\n",
    "\n",
    "# Find thresholds that equalize approval rates\n",
    "male_results = results_with_prob[results_with_prob['Gender'] == 'Male']\n",
    "female_results = results_with_prob[results_with_prob['Gender'] == 'Female']\n",
    "\n",
    "# Default threshold\n",
    "default_threshold = 0.5\n",
    "male_approval_rate = (male_results['Probability'] >= default_threshold).mean()\n",
    "female_approval_rate = (female_results['Probability'] >= default_threshold).mean()\n",
    "\n",
    "print(\"\\nApproval Rates with Default Threshold (0.5):\")\n",
    "print(f\"Male approval rate: {male_approval_rate:.2%}\")\n",
    "print(f\"Female approval rate: {female_approval_rate:.2%}\")\n",
    "\n",
    "# Find a threshold for females that equalizes approval rates\n",
    "# Simple binary search\n",
    "def find_equalizing_threshold(target_rate, probs, start=0, end=1, precision=0.001):\n",
    "    while end - start > precision:\n",
    "        mid = (start + end) / 2\n",
    "        rate = (probs >= mid).mean()\n",
    "        if rate < target_rate:\n",
    "            end = mid\n",
    "        else:\n",
    "            start = mid\n",
    "    return (start + end) / 2\n",
    "\n",
    "female_adjusted_threshold = find_equalizing_threshold(male_approval_rate, female_results['Probability'])\n",
    "\n",
    "print(\"\\nAdjusted Threshold for Equal Approval Rates:\")\n",
    "print(f\"Male threshold: {default_threshold:.4f}\")\n",
    "print(f\"Female threshold: {female_adjusted_threshold:.4f}\")\n",
    "\n",
    "# Apply adjusted thresholds\n",
    "results_with_prob['Predicted_Adjusted'] = np.where(\n",
    "    results_with_prob['Gender'] == 'Male',\n",
    "    results_with_prob['Probability'] >= default_threshold,\n",
    "    results_with_prob['Probability'] >= female_adjusted_threshold\n",
    ")\n",
    "\n",
    "# Calculate new approval rates\n",
    "adjusted_approval_by_gender = results_with_prob.groupby('Gender')['Predicted_Adjusted'].mean()\n",
    "\n",
    "print(\"\\nApproval Rates After Threshold Adjustment:\")\n",
    "print(adjusted_approval_by_gender)\n",
    "print(f\"Approval gap: {(adjusted_approval_by_gender['Male'] - adjusted_approval_by_gender['Female']) * 100:.2f}%\")\n",
    "\n",
    "# Visualize the effect of threshold adjustment\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Probability distributions by gender\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.kdeplot(data=results_with_prob, x='Probability', hue='Gender', fill=True)\n",
    "plt.axvline(x=default_threshold, color='black', linestyle='--', label='Default Threshold')\n",
    "plt.axvline(x=female_adjusted_threshold, color='red', linestyle='--', label='Adjusted Threshold (Female)')\n",
    "plt.title('Probability Distribution by Gender')\n",
    "plt.legend()\n",
    "\n",
    "# Approval rates before and after adjustment\n",
    "plt.subplot(1, 2, 2)\n",
    "approval_comparison = pd.DataFrame({\n",
    "    'Default Threshold': [male_approval_rate, female_approval_rate],\n",
    "    'Adjusted Threshold': [adjusted_approval_by_gender['Male'], adjusted_approval_by_gender['Female']]\n",
    "}, index=['Male', 'Female'])\n",
    "\n",
    "approval_comparison.plot(kind='bar', width=0.7)\n",
    "plt.title('Approval Rates Before and After Threshold Adjustment')\n",
    "plt.ylabel('Approval Rate')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f4b47c",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "In this introduction to data science, we've covered:\n",
    "\n",
    "1. **What is Data Science**: An interdisciplinary field combining statistics, computer science, and domain expertise to extract insights from data.\n",
    "\n",
    "2. **The Data Science Process**: From problem definition to deployment, understanding the structured workflow helps organize data science projects.\n",
    "\n",
    "3. **Key Components**: Statistics, programming, domain knowledge, and communication are the pillars of effective data science.\n",
    "\n",
    "4. **Types of Data**: Learning to work with different data types is fundamental to data manipulation and analysis.\n",
    "\n",
    "5. **Statistical Foundations**: Understanding basic statistical concepts helps in analyzing data distributions and relationships.\n",
    "\n",
    "6. **Exploratory Data Analysis (EDA)**: Techniques to understand data patterns and relationships before modeling.\n",
    "\n",
    "7. **Machine Learning Overview**: Basic understanding of supervised and unsupervised learning methods.\n",
    "\n",
    "8. **Data Visualization**: Creating effective visual representations of data to communicate insights.\n",
    "\n",
    "9. **Real-World Applications**: Applying data science techniques to solve practical problems.\n",
    "\n",
    "10. **Ethics in Data Science**: Recognizing and addressing ethical considerations like bias and fairness.\n",
    "\n",
    "### Next Steps for Your Data Science Journey:\n",
    "\n",
    "1. **Practice with real-world datasets** on platforms like Kaggle\n",
    "2. **Build a portfolio** of data science projects\n",
    "3. **Learn specialized areas** like NLP, computer vision, or time series analysis\n",
    "4. **Join data science communities** to share knowledge and get feedback\n",
    "5. **Stay updated** with the latest research and tools in the field\n",
    "6. **Contribute to open-source** data science projects\n",
    "7. **Apply data science techniques** to problems you're passionate about\n",
    "\n",
    "Remember that data science is as much about asking the right questions as it is about finding the right answers. The most valuable insights often come from understanding the business context and communicating results effectively."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
