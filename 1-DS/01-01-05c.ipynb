{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdeec9ab",
   "metadata": {},
   "source": [
    "# Loading Data from Various Sources\n",
    "\n",
    "This notebook demonstrates different methods for loading data from various file formats including CSV, Excel, JSON, and remote data sources. We'll cover different options and parameters for each method, as well as how to handle various challenges like different encodings and large files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af249e0b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "We'll start by importing the necessary libraries for loading and manipulating data from various sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28054a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for handling file formats and remote data\n",
    "import json\n",
    "import requests\n",
    "import io\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# For handling Excel files\n",
    "import openpyxl\n",
    "import xlrd\n",
    "\n",
    "# For timing operations\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cab609",
   "metadata": {},
   "source": [
    "## 2. Loading Data from CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) is one of the most common formats for storing tabular data. Pandas provides the powerful `read_csv()` function with many options to handle various CSV formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d3f285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic CSV loading\n",
    "# Let's download the Iris dataset as an example\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
    "iris_df = pd.read_csv(url)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Basic CSV loading:\")\n",
    "print(f\"Shape: {iris_df.shape}\")\n",
    "print(iris_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89013a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with different delimiters\n",
    "# Create a sample semicolon-separated string\n",
    "csv_data = \"\"\"sepal_length;sepal_width;petal_length;petal_width;species\n",
    "5.1;3.5;1.4;0.2;setosa\n",
    "4.9;3.0;1.4;0.2;setosa\n",
    "4.7;3.2;1.3;0.2;setosa\"\"\"\n",
    "\n",
    "# Write to a temporary file\n",
    "with open(\"temp_semicolon.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Read with custom delimiter\n",
    "semi_df = pd.read_csv(\"temp_semicolon.csv\", delimiter=\";\")\n",
    "print(\"CSV with semicolon delimiter:\")\n",
    "print(semi_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV with additional options\n",
    "# Create a sample CSV with header issues and missing values\n",
    "csv_data = \"\"\"# This is a comment\n",
    "# This is another comment\n",
    "col1,col2,col3\n",
    "1,2,3\n",
    "4,,6\n",
    "7,8,\"\"\"\n",
    "\n",
    "# Write to a temporary file\n",
    "with open(\"temp_complex.csv\", \"w\") as f:\n",
    "    f.write(csv_data)\n",
    "\n",
    "# Read with various options\n",
    "complex_df = pd.read_csv(\"temp_complex.csv\", \n",
    "                         comment=\"#\",       # Skip lines starting with #\n",
    "                         skiprows=0,        # Don't skip any non-comment rows\n",
    "                         na_values=[\"\"],    # Treat empty strings as NaN\n",
    "                         )\n",
    "\n",
    "print(\"CSV with comments and missing values:\")\n",
    "print(complex_df)\n",
    "print(\"\\nData types:\")\n",
    "print(complex_df.dtypes)\n",
    "\n",
    "# Clean up temporary files\n",
    "os.remove(\"temp_semicolon.csv\")\n",
    "os.remove(\"temp_complex.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading large CSV files in chunks\n",
    "# Download a larger dataset (COVID-19 data)\n",
    "url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/05-01-2023.csv\"\n",
    "\n",
    "# Read in chunks of 1000 rows\n",
    "chunk_size = 1000\n",
    "chunks = []\n",
    "\n",
    "start_time = time.time()\n",
    "for chunk in pd.read_csv(url, chunksize=chunk_size):\n",
    "    # Process each chunk (for example, filter only certain countries)\n",
    "    filtered_chunk = chunk[chunk['Country_Region'].isin(['US', 'India', 'Brazil', 'United Kingdom'])]\n",
    "    chunks.append(filtered_chunk)\n",
    "\n",
    "# Combine all chunks into a single DataFrame\n",
    "covid_df = pd.concat(chunks)\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Large CSV loaded and processed in chunks in {end_time - start_time:.2f} seconds\")\n",
    "print(f\"Shape: {covid_df.shape}\")\n",
    "print(covid_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67488547",
   "metadata": {},
   "source": [
    "## 3. Loading Data from Excel Files\n",
    "\n",
    "Excel files are widely used in business and can contain multiple sheets of data. Pandas makes it easy to read Excel files with the `read_excel()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a sample Excel file with multiple sheets\n",
    "data1 = {'Name': ['John', 'Anna', 'Peter', 'Linda'],\n",
    "         'Age': [28, 34, 29, 42],\n",
    "         'City': ['New York', 'Paris', 'Berlin', 'London']}\n",
    "\n",
    "data2 = {'Product': ['Laptop', 'Phone', 'Tablet', 'Monitor'],\n",
    "         'Price': [1200, 800, 500, 300],\n",
    "         'Stock': [10, 25, 15, 8]}\n",
    "\n",
    "df1 = pd.DataFrame(data1)\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Write to Excel file with multiple sheets\n",
    "excel_file = \"sample_data.xlsx\"\n",
    "with pd.ExcelWriter(excel_file) as writer:\n",
    "    df1.to_excel(writer, sheet_name='People', index=False)\n",
    "    df2.to_excel(writer, sheet_name='Products', index=False)\n",
    "\n",
    "print(f\"Sample Excel file created: {excel_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b4389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a specific sheet\n",
    "people_df = pd.read_excel(excel_file, sheet_name='People')\n",
    "print(\"Data from 'People' sheet:\")\n",
    "print(people_df)\n",
    "\n",
    "# Reading all sheets as a dictionary of DataFrames\n",
    "all_sheets = pd.read_excel(excel_file, sheet_name=None)\n",
    "print(\"\\nAll sheets loaded as dictionary:\")\n",
    "for sheet_name, df in all_sheets.items():\n",
    "    print(f\"\\nSheet: {sheet_name}\")\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3018f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading specific cells or ranges\n",
    "# Read only specific columns\n",
    "products_subset = pd.read_excel(excel_file, \n",
    "                                sheet_name='Products',\n",
    "                                usecols=['Product', 'Price'])\n",
    "print(\"Specific columns from Products sheet:\")\n",
    "print(products_subset)\n",
    "\n",
    "# Read specific rows\n",
    "products_rows = pd.read_excel(excel_file, \n",
    "                              sheet_name='Products',\n",
    "                              skiprows=1,     # Skip header\n",
    "                              nrows=2)        # Read only 2 rows\n",
    "print(\"\\nSpecific rows from Products sheet:\")\n",
    "print(products_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6e9a2e",
   "metadata": {},
   "source": [
    "## 4. Loading Data from JSON Files\n",
    "\n",
    "JSON (JavaScript Object Notation) is a popular data interchange format. It can represent complex, nested data structures, which pandas can parse into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample JSON data\n",
    "json_data = \"\"\"\n",
    "{\n",
    "    \"employees\": [\n",
    "        {\"name\": \"John\", \"age\": 30, \"city\": \"New York\", \"skills\": [\"Python\", \"SQL\"]},\n",
    "        {\"name\": \"Alice\", \"age\": 25, \"city\": \"San Francisco\", \"skills\": [\"JavaScript\", \"React\"]},\n",
    "        {\"name\": \"Bob\", \"age\": 35, \"city\": \"Chicago\", \"skills\": [\"Java\", \"C++\", \"Python\"]}\n",
    "    ],\n",
    "    \"company\": \"Tech Solutions Inc.\",\n",
    "    \"founded\": 2010\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Write JSON to file\n",
    "with open(\"sample_data.json\", \"w\") as f:\n",
    "    f.write(json_data)\n",
    "\n",
    "# Parse JSON using the json module\n",
    "with open(\"sample_data.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(\"JSON data loaded with json module:\")\n",
    "print(f\"Company: {data['company']}\")\n",
    "print(f\"Number of employees: {len(data['employees'])}\")\n",
    "print(f\"First employee: {data['employees'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6421e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON data into pandas DataFrame\n",
    "# For simple JSON structures\n",
    "simple_df = pd.read_json(\"\"\"\n",
    "[\n",
    "    {\"name\": \"John\", \"age\": 30, \"city\": \"New York\"},\n",
    "    {\"name\": \"Alice\", \"age\": 25, \"city\": \"San Francisco\"},\n",
    "    {\"name\": \"Bob\", \"age\": 35, \"city\": \"Chicago\"}\n",
    "]\n",
    "\"\"\")\n",
    "\n",
    "print(\"Simple JSON loaded into DataFrame:\")\n",
    "print(simple_df)\n",
    "\n",
    "# For nested JSON - we need to normalize it\n",
    "employees_df = pd.json_normalize(data['employees'])\n",
    "print(\"\\nNested JSON normalized into DataFrame:\")\n",
    "print(employees_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb6edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON with nested arrays and objects\n",
    "# Fetch posts from JSONPlaceholder API\n",
    "response = requests.get('https://jsonplaceholder.typicode.com/posts')\n",
    "posts_data = response.json()\n",
    "\n",
    "# Convert to DataFrame\n",
    "posts_df = pd.DataFrame(posts_data)\n",
    "print(\"API JSON data loaded into DataFrame:\")\n",
    "print(posts_df.head())\n",
    "\n",
    "# Get user data for each post\n",
    "response = requests.get('https://jsonplaceholder.typicode.com/users')\n",
    "users_data = response.json()\n",
    "users_df = pd.DataFrame(users_data)\n",
    "\n",
    "# Create a simplified users reference dataframe\n",
    "users_simple = users_df[['id', 'name', 'email']]\n",
    "\n",
    "# Join posts with user info\n",
    "posts_with_users = posts_df.merge(users_simple, left_on='userId', right_on='id', suffixes=('_post', '_user'))\n",
    "print(\"\\nJoined data from multiple JSON sources:\")\n",
    "print(posts_with_users[['id_post', 'title', 'name', 'email']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca60ca1",
   "metadata": {},
   "source": [
    "## 5. Handling Different File Encodings\n",
    "\n",
    "Files can come with different encodings, especially when dealing with international data. Let's see how to handle various encoding issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c854258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data with non-ASCII characters\n",
    "text_data = \"\"\"column1,column2,column3\n",
    "1,2,café\n",
    "4,5,résumé\n",
    "7,8,spaß\n",
    "\"\"\"\n",
    "\n",
    "# Write with different encodings\n",
    "encodings = ['utf-8', 'latin-1', 'cp1252']\n",
    "for encoding in encodings:\n",
    "    with open(f\"encoding_{encoding}.csv\", \"w\", encoding=encoding) as f:\n",
    "        f.write(text_data)\n",
    "    \n",
    "    # Try to read it back\n",
    "    try:\n",
    "        df = pd.read_csv(f\"encoding_{encoding}.csv\")\n",
    "        print(f\"Successfully read with default encoding from file encoded as {encoding}:\")\n",
    "        print(df)\n",
    "    except UnicodeDecodeError:\n",
    "        print(f\"Failed to read with default encoding from file encoded as {encoding}\")\n",
    "        \n",
    "        # Try with explicit encoding\n",
    "        df = pd.read_csv(f\"encoding_{encoding}.csv\", encoding=encoding)\n",
    "        print(f\"Successfully read with explicit {encoding} encoding:\")\n",
    "        print(df)\n",
    "\n",
    "# Clean up files\n",
    "for encoding in encodings:\n",
    "    os.remove(f\"encoding_{encoding}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d40e77",
   "metadata": {},
   "source": [
    "## 6. Working with Remote Data Sources\n",
    "\n",
    "Many datasets are available online through URLs and APIs. Let's explore how to load data directly from remote sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9e6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading directly from URLs\n",
    "url = \"https://raw.githubusercontent.com/fivethirtyeight/data/master/airline-safety/airline-safety.csv\"\n",
    "airlines_df = pd.read_csv(url)\n",
    "\n",
    "print(\"Data loaded directly from GitHub URL:\")\n",
    "print(f\"Shape: {airlines_df.shape}\")\n",
    "print(airlines_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbff3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from an API\n",
    "# Let's use the Open Notify API to get the current position of the ISS\n",
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "iss_data = response.json()\n",
    "\n",
    "print(\"Data from API:\")\n",
    "print(json.dumps(iss_data, indent=4))\n",
    "\n",
    "# Convert to DataFrame\n",
    "iss_df = pd.DataFrame({\n",
    "    'timestamp': [iss_data['timestamp']],\n",
    "    'latitude': [float(iss_data['iss_position']['latitude'])],\n",
    "    'longitude': [float(iss_data['iss_position']['longitude'])]\n",
    "})\n",
    "iss_df['datetime'] = pd.to_datetime(iss_df['timestamp'], unit='s')\n",
    "\n",
    "print(\"\\nConverted to DataFrame:\")\n",
    "print(iss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ed3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data in memory without saving to disk\n",
    "url = \"https://raw.githubusercontent.com/plotly/datasets/master/2014_usa_states.csv\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# If the response was successful, create a file-like object in memory\n",
    "if response.status_code == 200:\n",
    "    # Create a file-like object from the response content\n",
    "    csv_file = io.StringIO(response.text)\n",
    "    \n",
    "    # Read the CSV into a pandas DataFrame\n",
    "    states_df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(\"CSV loaded in memory without saving to disk:\")\n",
    "    print(f\"Shape: {states_df.shape}\")\n",
    "    print(states_df.head())\n",
    "else:\n",
    "    print(f\"Failed to fetch data: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4cc1d8",
   "metadata": {},
   "source": [
    "## 7. Comparing Data Loading Methods\n",
    "\n",
    "Let's compare the performance of different loading methods, especially for larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42930cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download a larger dataset for benchmarking\n",
    "url = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/ggplot2/diamonds.csv\"\n",
    "diamonds_path = \"diamonds.csv\"\n",
    "\n",
    "# Download the file only if it doesn't exist\n",
    "if not os.path.exists(diamonds_path):\n",
    "    response = requests.get(url)\n",
    "    with open(diamonds_path, 'wb') as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"Downloaded {diamonds_path}\")\n",
    "else:\n",
    "    print(f\"{diamonds_path} already exists\")\n",
    "\n",
    "# Get file size\n",
    "file_size = os.path.getsize(diamonds_path) / (1024 * 1024)  # Convert to MB\n",
    "print(f\"File size: {file_size:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f1e2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark different loading methods\n",
    "\n",
    "# Method 1: Standard read_csv\n",
    "start_time = time.time()\n",
    "df_standard = pd.read_csv(diamonds_path)\n",
    "standard_time = time.time() - start_time\n",
    "print(f\"Standard read_csv: {standard_time:.3f} seconds\")\n",
    "\n",
    "# Method 2: Using specific dtypes for columns\n",
    "dtypes = {\n",
    "    'carat': 'float32', \n",
    "    'depth': 'float32', \n",
    "    'table': 'float32', \n",
    "    'price': 'int32', \n",
    "    'x': 'float32',\n",
    "    'y': 'float32',\n",
    "    'z': 'float32'\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "df_dtypes = pd.read_csv(diamonds_path, dtype=dtypes)\n",
    "dtypes_time = time.time() - start_time\n",
    "print(f\"With specified dtypes: {dtypes_time:.3f} seconds\")\n",
    "\n",
    "# Method 3: Using chunking\n",
    "start_time = time.time()\n",
    "chunks = []\n",
    "for chunk in pd.read_csv(diamonds_path, chunksize=5000):\n",
    "    chunks.append(chunk)\n",
    "df_chunked = pd.concat(chunks)\n",
    "chunk_time = time.time() - start_time\n",
    "print(f\"Using chunks: {chunk_time:.3f} seconds\")\n",
    "\n",
    "# Compare memory usage\n",
    "print(\"\\nMemory usage:\")\n",
    "print(f\"Standard: {df_standard.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(f\"With dtypes: {df_dtypes.memory_usage().sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Chunked: {df_chunked.memory_usage().sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7bb38",
   "metadata": {},
   "source": [
    "## 8. Saving Data to Different Formats\n",
    "\n",
    "After loading and processing data, you often need to save it in different formats. Let's see how to convert between formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84255e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the airlines dataset we loaded earlier\n",
    "# First, let's look at the data\n",
    "print(\"Airlines dataset:\")\n",
    "print(airlines_df.head())\n",
    "\n",
    "# Save to CSV\n",
    "airlines_df.to_csv(\"airlines_exported.csv\", index=False)\n",
    "print(\"\\nSaved to airlines_exported.csv\")\n",
    "\n",
    "# Save to Excel\n",
    "airlines_df.to_excel(\"airlines_exported.xlsx\", sheet_name=\"Safety Data\", index=False)\n",
    "print(\"Saved to airlines_exported.xlsx\")\n",
    "\n",
    "# Save to JSON\n",
    "airlines_df.to_json(\"airlines_exported.json\", orient=\"records\")\n",
    "print(\"Saved to airlines_exported.json\")\n",
    "\n",
    "# Save to pickle (Python serialized format)\n",
    "airlines_df.to_pickle(\"airlines_exported.pkl\")\n",
    "print(\"Saved to airlines_exported.pkl\")\n",
    "\n",
    "# Save to parquet (columnar storage format - good for big data)\n",
    "airlines_df.to_parquet(\"airlines_exported.parquet\")\n",
    "print(\"Saved to airlines_exported.parquet\")\n",
    "\n",
    "# Compare file sizes\n",
    "files = [\n",
    "    \"airlines_exported.csv\",\n",
    "    \"airlines_exported.xlsx\", \n",
    "    \"airlines_exported.json\",\n",
    "    \"airlines_exported.pkl\",\n",
    "    \"airlines_exported.parquet\"\n",
    "]\n",
    "\n",
    "print(\"\\nFile size comparison:\")\n",
    "for file in files:\n",
    "    size_kb = os.path.getsize(file) / 1024\n",
    "    print(f\"{file}: {size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5574fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data back from these formats to verify\n",
    "csv_df = pd.read_csv(\"airlines_exported.csv\")\n",
    "excel_df = pd.read_excel(\"airlines_exported.xlsx\")\n",
    "json_df = pd.read_json(\"airlines_exported.json\", orient=\"records\")\n",
    "pickle_df = pd.read_pickle(\"airlines_exported.pkl\")\n",
    "parquet_df = pd.read_parquet(\"airlines_exported.parquet\")\n",
    "\n",
    "# Check that all the data is the same\n",
    "formats = {\n",
    "    \"Original\": airlines_df,\n",
    "    \"CSV\": csv_df,\n",
    "    \"Excel\": excel_df,\n",
    "    \"JSON\": json_df,\n",
    "    \"Pickle\": pickle_df,\n",
    "    \"Parquet\": parquet_df\n",
    "}\n",
    "\n",
    "print(\"Verifying data integrity across formats:\")\n",
    "for name, df in formats.items():\n",
    "    print(f\"{name} shape: {df.shape}\")\n",
    "\n",
    "# Clean up files\n",
    "for file in files:\n",
    "    os.remove(file)\n",
    "os.remove(diamonds_path)\n",
    "os.remove(excel_file)\n",
    "os.remove(\"sample_data.json\")\n",
    "\n",
    "print(\"\\nTemporary files cleaned up\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c18ff0f",
   "metadata": {},
   "source": [
    "## 9. Summary and Best Practices\n",
    "\n",
    "In this notebook, we've explored various methods to load data from different file formats:\n",
    "\n",
    "1. **CSV files** - Using `pd.read_csv()` with various options for delimiters, encodings, and chunking\n",
    "2. **Excel files** - Using `pd.read_excel()` to read specific sheets and ranges\n",
    "3. **JSON files** - Using `pd.read_json()` and `json.load()` for different JSON structures\n",
    "4. **Remote data** - Loading data directly from URLs and APIs\n",
    "5. **Different encodings** - Handling files with various character encodings\n",
    "6. **Performance optimizations** - Using dtypes and chunking for faster loading\n",
    "7. **Saving data** - Converting between different file formats\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- **Specify dtypes** when loading large datasets to reduce memory usage\n",
    "- **Use chunking** for very large files that don't fit in memory\n",
    "- **Handle encodings explicitly** when working with international data\n",
    "- **Consider compression** for large files (e.g., `pd.read_csv(\"file.csv.gz\")`)\n",
    "- **Use appropriate file formats** for your use case (Parquet for large data, CSV for compatibility)\n",
    "- **Include error handling** when loading from remote sources"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
