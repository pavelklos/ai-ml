{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fd2385",
   "metadata": {},
   "source": [
    "# Loading Data from Various Sources (CSV, Excel, JSON)\n",
    "\n",
    "This notebook demonstrates how to load data from different file formats including CSV, Excel, and JSON using pandas and other Python libraries. We'll explore various parameters and options for each format to handle different scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c038674",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b121e0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "# Import core libraries for data loading and processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import requests\n",
    "from io import StringIO\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check pandas version\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dae25e",
   "metadata": {},
   "source": [
    "## Loading Data from CSV Files\n",
    "\n",
    "CSV (Comma-Separated Values) is one of the most common file formats for data storage. Pandas provides the powerful `read_csv()` function to load CSV files with many customization options.\n",
    "\n",
    "Let's explore various ways to load and handle CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29f08821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CSV loading:\n",
      "   id            name  age  salary department\n",
      "0   1      John Smith   34   50000         IT\n",
      "1   2        Jane Doe   28   65000  Marketing\n",
      "2   3     Bob Johnson   45   75000    Finance\n",
      "3   4     Alice Brown   31   55000         HR\n",
      "4   5  Charlie Wilson   29   60000         IT\n"
     ]
    }
   ],
   "source": [
    "# Creating a sample CSV data for demonstration\n",
    "sample_csv = \"\"\"\n",
    "id,name,age,salary,department\n",
    "1,John Smith,34,50000,IT\n",
    "2,Jane Doe,28,65000,Marketing\n",
    "3,Bob Johnson,45,75000,Finance\n",
    "4,Alice Brown,31,55000,HR\n",
    "5,Charlie Wilson,29,60000,IT\n",
    "\"\"\"\n",
    "\n",
    "# Create a file-like object from the string\n",
    "csv_data = StringIO(sample_csv.strip())\n",
    "\n",
    "# Basic CSV reading\n",
    "df_csv = pd.read_csv(csv_data)\n",
    "print(\"Basic CSV loading:\")\n",
    "print(df_csv.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a7b45d",
   "metadata": {},
   "source": [
    "### CSV Loading Options\n",
    "\n",
    "Let's explore various parameters available in `pd.read_csv()`:\n",
    "- Custom delimiters\n",
    "- Skipping rows\n",
    "- Specifying data types\n",
    "- Handling missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9113d99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with tab delimiter:\n",
      "   id            name  age  salary department\n",
      "0   1      John Smith   34   50000         IT\n",
      "1   2        Jane Doe   28   65000  Marketing\n",
      "2   3     Bob Johnson   45   75000    Finance\n",
      "3   4     Alice Brown   31   55000         HR\n",
      "4   5  Charlie Wilson   29   60000         IT\n"
     ]
    }
   ],
   "source": [
    "# Create a sample CSV with different delimiter\n",
    "sample_csv_tab = \"\"\"\n",
    "id\\tname\\tage\\tsalary\\tdepartment\n",
    "1\\tJohn Smith\\t34\\t50000\\tIT\n",
    "2\\tJane Doe\\t28\\t65000\\tMarketing\n",
    "3\\tBob Johnson\\t45\\t75000\\tFinance\n",
    "4\\tAlice Brown\\t31\\t55000\\tHR\n",
    "5\\tCharlie Wilson\\t29\\t60000\\tIT\n",
    "\"\"\"\n",
    "\n",
    "# Create a file-like object\n",
    "csv_tab_data = StringIO(sample_csv_tab.strip())\n",
    "\n",
    "# Reading with custom delimiter\n",
    "df_tab = pd.read_csv(csv_tab_data, delimiter='\\t')\n",
    "print(\"CSV with tab delimiter:\")\n",
    "print(df_tab.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17e78c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV with skiprows and handling missing values:\n",
      "   id         name   age   salary department\n",
      "0   1   John Smith  34.0      NaN         IT\n",
      "1   2     Jane Doe   NaN  65000.0  Marketing\n",
      "2   3  Bob Johnson  45.0  75000.0        NaN\n",
      "3   4  Alice Brown  31.0  55000.0         HR\n",
      "4   5          NaN  29.0  60000.0         IT\n",
      "\n",
      "Missing values count:\n",
      "id            0\n",
      "name          1\n",
      "age           1\n",
      "salary        1\n",
      "department    1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Sample CSV with header at second line and some missing values\n",
    "sample_csv_complex = \"\"\"\n",
    "This is a comment line\n",
    "id,name,age,salary,department\n",
    "1,John Smith,34,,IT\n",
    "2,Jane Doe,,65000,Marketing\n",
    "3,Bob Johnson,45,75000,\n",
    "4,Alice Brown,31,55000,HR\n",
    "5,,29,60000,IT\n",
    "\"\"\"\n",
    "\n",
    "# Create a file-like object\n",
    "csv_complex_data = StringIO(sample_csv_complex.strip())\n",
    "\n",
    "# Reading with skiprows and handling missing values\n",
    "df_complex = pd.read_csv(\n",
    "    csv_complex_data,\n",
    "    skiprows=1,  # Skip the first row\n",
    "    na_values=[\"\", \"NA\", \"N/A\"],  # Define NA values\n",
    "    dtype={\"id\": int, \"name\": str, \"age\": float, \"salary\": float, \"department\": str}  # Define data types\n",
    ")\n",
    "\n",
    "print(\"CSV with skiprows and handling missing values:\")\n",
    "print(df_complex.head())\n",
    "print(\"\\nMissing values count:\")\n",
    "print(df_complex.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "618ffdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris dataset loaded from URL:\n",
      "   sepal_length  sepal_width  petal_length  petal_width species\n",
      "0           5.1          3.5           1.4          0.2  setosa\n",
      "1           4.9          3.0           1.4          0.2  setosa\n",
      "2           4.7          3.2           1.3          0.2  setosa\n",
      "3           4.6          3.1           1.5          0.2  setosa\n",
      "4           5.0          3.6           1.4          0.2  setosa\n",
      "\n",
      "Dataset shape: (150, 5)\n",
      "Dataset columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']\n"
     ]
    }
   ],
   "source": [
    "# Reading CSV from a URL (using the Iris dataset)\n",
    "url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\"\n",
    "\n",
    "try:\n",
    "    df_iris = pd.read_csv(url)\n",
    "    print(\"Iris dataset loaded from URL:\")\n",
    "    print(df_iris.head())\n",
    "    print(f\"\\nDataset shape: {df_iris.shape}\")\n",
    "    print(f\"Dataset columns: {df_iris.columns.tolist()}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading from URL: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f03d7a",
   "metadata": {},
   "source": [
    "## Loading Data from Excel Files\n",
    "\n",
    "Excel files are widely used in business settings. Pandas provides the `read_excel()` function to load data from Excel files, with options to specify sheets, ranges, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "443b4db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Excel data:\n",
      "    Product              Category  Price  Stock Last Updated\n",
      "0    Laptop           Electronics   1200     15   2023-01-01\n",
      "1     Phone           Electronics    800     25   2023-01-02\n",
      "2    Tablet           Electronics    300     40   2023-01-03\n",
      "3   Monitor  Computer Accessories    250     30   2023-01-04\n",
      "4  Keyboard  Computer Accessories    100     45   2023-01-05\n",
      "\n",
      "To load this data from Excel, you would use:\n",
      "df = pd.read_excel('products.xlsx')\n",
      "\n",
      "    Product              Category  Price  Stock Last Updated\n",
      "0    Laptop           Electronics   1200     15   2023-01-01\n",
      "1     Phone           Electronics    800     25   2023-01-02\n",
      "2    Tablet           Electronics    300     40   2023-01-03\n",
      "3   Monitor  Computer Accessories    250     30   2023-01-04\n",
      "4  Keyboard  Computer Accessories    100     45   2023-01-05\n"
     ]
    }
   ],
   "source": [
    "# Since we can't create a real Excel file in this notebook directly,\n",
    "# let's simulate Excel file loading by creating a DataFrame and then\n",
    "# showing how it would be loaded from an Excel file\n",
    "\n",
    "# Create sample data for our simulated Excel file\n",
    "data = {\n",
    "    'Product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard'],\n",
    "    'Category': ['Electronics', 'Electronics', 'Electronics', 'Computer Accessories', 'Computer Accessories'],\n",
    "    'Price': [1200, 800, 300, 250, 100],\n",
    "    'Stock': [15, 25, 40, 30, 45],\n",
    "    'Last Updated': pd.date_range(start='2023-01-01', periods=5, freq='D')\n",
    "}\n",
    "\n",
    "df_excel_data = pd.DataFrame(data)\n",
    "print(\"Sample Excel data:\")\n",
    "print(df_excel_data)\n",
    "\n",
    "# In real scenario, this would be saved as:\n",
    "# df_excel_data.to_excel('products.xlsx', index=False)\n",
    "\n",
    "print(\"\\nTo load this data from Excel, you would use:\")\n",
    "print(\"df = pd.read_excel('products.xlsx')\\n\")\n",
    "\n",
    "# Save and load Excel file\n",
    "df_excel_data.to_excel('products.xlsx', index=False)\n",
    "df = pd.read_excel('products.xlsx')\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb39a832",
   "metadata": {},
   "source": [
    "### Excel Loading Options\n",
    "\n",
    "When loading Excel files, you can:\n",
    "- Specify sheet names or indices\n",
    "- Read specific cell ranges\n",
    "- Handle dates and time formats\n",
    "- Deal with merged cells and formulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89a5952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from a specific sheet:\n",
      "df = pd.read_excel('products.xlsx', sheet_name='Sheet1')\n",
      "\n",
      "Loading from multiple sheets:\n",
      "all_sheets = pd.read_excel('products.xlsx', sheet_name=None)  # Returns a dict of DataFrames\n",
      "\n",
      "Loading a specific range:\n",
      "df = pd.read_excel('products.xlsx', usecols='A:C', skiprows=2, nrows=10)\n",
      "\n",
      "Handling dates:\n",
      "df = pd.read_excel('products.xlsx', parse_dates=['Last Updated'])\n"
     ]
    }
   ],
   "source": [
    "# Here's how you would load from a multi-sheet Excel file\n",
    "print(\"Loading from a specific sheet:\")\n",
    "print(\"df = pd.read_excel('products.xlsx', sheet_name='Sheet1')\")\n",
    "\n",
    "print(\"\\nLoading from multiple sheets:\")\n",
    "print(\"all_sheets = pd.read_excel('products.xlsx', sheet_name=None)  # Returns a dict of DataFrames\")\n",
    "\n",
    "print(\"\\nLoading a specific range:\")\n",
    "print(\"df = pd.read_excel('products.xlsx', usecols='A:C', skiprows=2, nrows=10)\")\n",
    "\n",
    "print(\"\\nHandling dates:\")\n",
    "print(\"df = pd.read_excel('products.xlsx', parse_dates=['Last Updated'])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c36614",
   "metadata": {},
   "source": [
    "## Loading Data from JSON Files\n",
    "\n",
    "JSON (JavaScript Object Notation) is a common data format for web APIs and configuration files. Pandas provides the `read_json()` function to load JSON data directly into DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd0d4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed JSON structure:\n",
      "Keys: ['employees', 'company', 'location']\n",
      "Number of employees: 4\n"
     ]
    }
   ],
   "source": [
    "# Create a sample JSON string\n",
    "sample_json = \"\"\"\n",
    "{\n",
    "  \"employees\": [\n",
    "    {\"id\": 1, \"name\": \"John Smith\", \"department\": \"IT\", \"skills\": [\"Python\", \"SQL\", \"JavaScript\"]},\n",
    "    {\"id\": 2, \"name\": \"Jane Doe\", \"department\": \"Marketing\", \"skills\": [\"SEO\", \"Content Writing\", \"Analytics\"]},\n",
    "    {\"id\": 3, \"name\": \"Bob Johnson\", \"department\": \"Finance\", \"skills\": [\"Excel\", \"Financial Modeling\", \"SQL\"]},\n",
    "    {\"id\": 4, \"name\": \"Alice Brown\", \"department\": \"HR\", \"skills\": [\"Recruiting\", \"Training\", \"Benefits\"]}\n",
    "  ],\n",
    "  \"company\": \"Tech Solutions Inc.\",\n",
    "  \"location\": \"New York\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Parse the JSON\n",
    "parsed_json = json.loads(sample_json)\n",
    "print(\"Parsed JSON structure:\")\n",
    "print(f\"Keys: {list(parsed_json.keys())}\")\n",
    "print(f\"Number of employees: {len(parsed_json['employees'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc8129a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON data as DataFrame:\n",
      "   id         name department                             skills\n",
      "0   1   John Smith         IT          [Python, SQL, JavaScript]\n",
      "1   2     Jane Doe  Marketing  [SEO, Content Writing, Analytics]\n",
      "2   3  Bob Johnson    Finance   [Excel, Financial Modeling, SQL]\n",
      "3   4  Alice Brown         HR   [Recruiting, Training, Benefits]\n",
      "\n",
      "Skills column type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "# Convert the employees list to a DataFrame\n",
    "df_json = pd.DataFrame(parsed_json['employees'])\n",
    "print(\"JSON data as DataFrame:\")\n",
    "print(df_json)\n",
    "\n",
    "# Notice that the skills column contains lists\n",
    "print(\"\\nSkills column type:\", type(df_json['skills'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a71dac25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple JSON loaded with pandas:\n",
      "   id  name  age\n",
      "0   1  John   30\n",
      "1   2  Jane   25\n",
      "2   3   Bob   35\n",
      "\n",
      "For nested JSON, you can use json_normalize:\n",
      "   id         name department                             skills\n",
      "0   1   John Smith         IT          [Python, SQL, JavaScript]\n",
      "1   2     Jane Doe  Marketing  [SEO, Content Writing, Analytics]\n",
      "2   3  Bob Johnson    Finance   [Excel, Financial Modeling, SQL]\n",
      "3   4  Alice Brown         HR   [Recruiting, Training, Benefits]\n"
     ]
    }
   ],
   "source": [
    "# Loading JSON directly with pandas\n",
    "# For a simple JSON array\n",
    "simple_json = \"\"\"\n",
    "[\n",
    "  {\"id\": 1, \"name\": \"John\", \"age\": 30},\n",
    "  {\"id\": 2, \"name\": \"Jane\", \"age\": 25},\n",
    "  {\"id\": 3, \"name\": \"Bob\", \"age\": 35}\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# Load JSON with pandas\n",
    "df_simple_json = pd.read_json(StringIO(simple_json))\n",
    "print(\"Simple JSON loaded with pandas:\")\n",
    "print(df_simple_json)\n",
    "\n",
    "# For more complex nested JSON, you might need to normalize\n",
    "print(\"\\nFor nested JSON, you can use json_normalize:\")\n",
    "df_normalized = pd.json_normalize(parsed_json['employees'])\n",
    "print(df_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12a3f2",
   "metadata": {},
   "source": [
    "### Loading JSON from an API\n",
    "\n",
    "Many data sources are available through APIs that return JSON. Let's see how to load data from a public API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a75b245e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Response:\n",
      "{\n",
      "  \"iss_position\": {\n",
      "    \"longitude\": \"120.3184\",\n",
      "    \"latitude\": \"-35.9747\"\n",
      "  },\n",
      "  \"message\": \"success\",\n",
      "  \"timestamp\": 1744795937\n",
      "}\n",
      "\n",
      "Position as DataFrame:\n",
      "  longitude  latitude\n",
      "0  120.3184  -35.9747\n"
     ]
    }
   ],
   "source": [
    "# Example: Loading data from a public API\n",
    "# Using a public API that doesn't require authentication\n",
    "try:\n",
    "    # Open Notify API - Current location of the International Space Station\n",
    "    response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        print(\"API Response:\")\n",
    "        print(json.dumps(data, indent=2))\n",
    "        \n",
    "        # Convert position data to DataFrame\n",
    "        position = data['iss_position']\n",
    "        df_position = pd.DataFrame([position])\n",
    "        print(\"\\nPosition as DataFrame:\")\n",
    "        print(df_position)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: Status code {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error fetching API data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27395a4f",
   "metadata": {},
   "source": [
    "## Handling Different File Encodings\n",
    "\n",
    "When loading files, especially from international sources, you may encounter encoding issues. Let's see how to handle different file encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fb72222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data with UTF-8 encoding:\n",
      "   id             name  country\n",
      "0   1      José García    Spain\n",
      "1   2     Björn Müller  Germany\n",
      "2   3  Séverine Dupont   France\n",
      "3   4   Николай Иванов   Russia\n",
      "4   5             中村 健    Japan\n",
      "\n",
      "Common encodings to try when you have issues:\n",
      "- utf-8: Universal encoding that works for most modern files\n",
      "- latin-1 (iso-8859-1): Works for Western European languages\n",
      "- cp1252: Windows default for Western languages\n",
      "- utf-16: For some special applications\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with non-ASCII characters\n",
    "sample_data_utf8 = \"\"\"\n",
    "id,name,country\n",
    "1,José García,Spain\n",
    "2,Björn Müller,Germany\n",
    "3,Séverine Dupont,France\n",
    "4,Николай Иванов,Russia\n",
    "5,中村 健,Japan\n",
    "\"\"\"\n",
    "\n",
    "# Create a file-like object\n",
    "data_utf8 = StringIO(sample_data_utf8.strip())\n",
    "\n",
    "# Reading with UTF-8 encoding\n",
    "df_utf8 = pd.read_csv(data_utf8, encoding='utf-8')\n",
    "print(\"Data with UTF-8 encoding:\")\n",
    "print(df_utf8)\n",
    "\n",
    "print(\"\\nCommon encodings to try when you have issues:\")\n",
    "print(\"- utf-8: Universal encoding that works for most modern files\")\n",
    "print(\"- latin-1 (iso-8859-1): Works for Western European languages\")\n",
    "print(\"- cp1252: Windows default for Western languages\")\n",
    "print(\"- utf-16: For some special applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54344d5d",
   "metadata": {},
   "source": [
    "### Detecting Encoding Issues\n",
    "\n",
    "When you encounter encoding errors, here's how to diagnose and fix them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "015017a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When loading files with encoding issues, you can use the try_encodings function above\n",
      "to automatically detect the correct encoding for your file.\n"
     ]
    }
   ],
   "source": [
    "def try_encodings(file_path, encodings=['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']):\n",
    "    \"\"\"Try different encodings for a file\"\"\"\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            # Try to read the first few lines with this encoding\n",
    "            print(f\"Trying {encoding} encoding...\")\n",
    "            result = pd.read_csv(file_path, encoding=encoding, nrows=5)\n",
    "            print(f\"Success with {encoding}!\")\n",
    "            return encoding, result\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed with {encoding} encoding.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Other error with {encoding}: {str(e)}\")\n",
    "    \n",
    "    return None, None\n",
    "\n",
    "# This is how you'd use the function with a real file\n",
    "# best_encoding, sample_data = try_encodings('data.csv')\n",
    "# if best_encoding:\n",
    "#     full_data = pd.read_csv('data.csv', encoding=best_encoding)\n",
    "\n",
    "print(\"When loading files with encoding issues, you can use the try_encodings function above\")\n",
    "print(\"to automatically detect the correct encoding for your file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e023f6",
   "metadata": {},
   "source": [
    "## Working with URLs and Remote Data\n",
    "\n",
    "You can load data directly from URLs without downloading files first using pandas or the requests library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6cfe06b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded from URL:\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "\n",
      "Dataset shape: (891, 12)\n"
     ]
    }
   ],
   "source": [
    "# Loading CSV directly from a URL\n",
    "url_csv = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n",
    "\n",
    "try:\n",
    "    # Direct loading with pandas\n",
    "    df_titanic = pd.read_csv(url_csv)\n",
    "    print(\"Titanic dataset loaded from URL:\")\n",
    "    print(df_titanic.head())\n",
    "    print(f\"\\nDataset shape: {df_titanic.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV from URL: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92482f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titanic dataset loaded using requests:\n",
      "Number of rows: 891\n"
     ]
    }
   ],
   "source": [
    "# Alternative approach: Using requests to download the data first\n",
    "try:\n",
    "    response = requests.get(url_csv)\n",
    "    if response.status_code == 200:\n",
    "        content = StringIO(response.text)\n",
    "        df_titanic2 = pd.read_csv(content)\n",
    "        print(\"Titanic dataset loaded using requests:\")\n",
    "        print(f\"Number of rows: {len(df_titanic2)}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: Status code {response.status_code}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with requests approach: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3b0cf",
   "metadata": {},
   "source": [
    "## Comparing File Formats\n",
    "\n",
    "Let's compare loading speed, file sizes, and use cases for different file formats. We'll also show how to convert between formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20d6bc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated DataFrame with 100000 rows and 5 columns\n",
      "   id    value1    value2 category           timestamp\n",
      "0   1  0.374540  0.580779        D 2020-01-01 00:00:00\n",
      "1   2  0.950714  0.526972        A 2020-01-01 00:10:00\n",
      "2   3  0.731994  0.351037        B 2020-01-01 00:20:00\n",
      "3   4  0.598658  0.493213        B 2020-01-01 00:30:00\n",
      "4   5  0.156019  0.365097        C 2020-01-01 00:40:00\n"
     ]
    }
   ],
   "source": [
    "# Create a sample DataFrame for comparison\n",
    "import numpy as np\n",
    "\n",
    "# Generate a larger dataset for comparison\n",
    "np.random.seed(42)\n",
    "num_rows = 100000\n",
    "data = {\n",
    "    'id': range(1, num_rows + 1),\n",
    "    'value1': np.random.rand(num_rows),\n",
    "    'value2': np.random.rand(num_rows),\n",
    "    'category': np.random.choice(['A', 'B', 'C', 'D'], num_rows),\n",
    "    'timestamp': pd.date_range(start='2020-01-01', periods=num_rows, freq='10min')\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(data)\n",
    "print(f\"Generated DataFrame with {len(comparison_df)} rows and {len(comparison_df.columns)} columns\")\n",
    "print(comparison_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10d70eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Format  File Size (MB)  Write Time (s)  Read Time (s)\n",
      "0     csv            0.63           0.064          0.023\n",
      "1    json            0.92           0.014          0.039\n",
      "2  pickle            0.33           0.003          0.011\n"
     ]
    }
   ],
   "source": [
    "# Function to measure file size and read/write time\n",
    "def format_comparison(df, formats=None):\n",
    "    if formats is None:\n",
    "        formats = ['csv', 'excel', 'json', 'pickle', 'parquet', 'feather']\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for fmt in formats:\n",
    "        file_path = f\"temp_data.{fmt}\"\n",
    "        \n",
    "        # Write to file\n",
    "        write_start = time.time()\n",
    "        \n",
    "        try:\n",
    "            if fmt == 'csv':\n",
    "                df.to_csv(file_path, index=False)\n",
    "            elif fmt == 'excel':\n",
    "                df.to_excel(file_path, index=False)\n",
    "            elif fmt == 'json':\n",
    "                df.to_json(file_path, orient='records')\n",
    "            elif fmt == 'pickle':\n",
    "                df.to_pickle(file_path)\n",
    "            elif fmt == 'parquet':\n",
    "                df.to_parquet(file_path, index=False)\n",
    "            elif fmt == 'feather':\n",
    "                df.to_feather(file_path)\n",
    "            \n",
    "            write_time = time.time() - write_start\n",
    "            \n",
    "            # Get file size\n",
    "            file_size = os.path.getsize(file_path) / (1024 * 1024)  # Size in MB\n",
    "            \n",
    "            # Read from file\n",
    "            read_start = time.time()\n",
    "            \n",
    "            if fmt == 'csv':\n",
    "                _ = pd.read_csv(file_path)\n",
    "            elif fmt == 'excel':\n",
    "                _ = pd.read_excel(file_path)\n",
    "            elif fmt == 'json':\n",
    "                _ = pd.read_json(file_path, orient='records')\n",
    "            elif fmt == 'pickle':\n",
    "                _ = pd.read_pickle(file_path)\n",
    "            elif fmt == 'parquet':\n",
    "                _ = pd.read_parquet(file_path)\n",
    "            elif fmt == 'feather':\n",
    "                _ = pd.read_feather(file_path)\n",
    "                \n",
    "            read_time = time.time() - read_start\n",
    "            \n",
    "            # Clean up\n",
    "            # os.remove(file_path)\n",
    "            \n",
    "            results.append({\n",
    "                'Format': fmt,\n",
    "                'File Size (MB)': round(file_size, 2),\n",
    "                'Write Time (s)': round(write_time, 3),\n",
    "                'Read Time (s)': round(read_time, 3)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {fmt} format: {e}\")\n",
    "            # Clean up if file exists\n",
    "            # if os.path.exists(file_path):\n",
    "            #     os.remove(file_path)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Compare formats (limiting to common formats that don't require extra libraries)\n",
    "try:\n",
    "    # For demonstration, use a smaller subset of the data\n",
    "    sample_df = comparison_df.head(10000)\n",
    "    comparison = format_comparison(sample_df, formats=['csv', 'json', 'pickle'])\n",
    "    print(comparison)\n",
    "except Exception as e:\n",
    "    print(f\"Error during comparison: {e}\")\n",
    "    print(\"Note: Some formats like parquet and feather require additional libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d099d58",
   "metadata": {},
   "source": [
    "### Format Comparison Summary\n",
    "\n",
    "Let's discuss the pros and cons of each format:\n",
    "\n",
    "1. **CSV**\n",
    "   - Pros: Universal compatibility, human-readable, works with many tools\n",
    "   - Cons: Larger file size, slow for large datasets, no schema/type preservation\n",
    "\n",
    "2. **JSON**\n",
    "   - Pros: Great for web APIs, preserves nested structures, human-readable\n",
    "   - Cons: Larger file size than binary formats, slower parsing\n",
    "\n",
    "3. **Excel**\n",
    "   - Pros: User-friendly for non-technical users, supports multiple sheets\n",
    "   - Cons: Very slow for large datasets, large file size, version compatibility issues\n",
    "\n",
    "4. **Pickle**\n",
    "   - Pros: Fast, preserves pandas objects and data types, small file size\n",
    "   - Cons: Python-specific, security concerns, version compatibility issues\n",
    "\n",
    "5. **Parquet**\n",
    "   - Pros: Very efficient storage, columnar storage for analytics, preserves schema\n",
    "   - Cons: Requires additional libraries, not human-readable\n",
    "\n",
    "6. **Feather**\n",
    "   - Pros: Fast read/write, cross-compatible between R and Python\n",
    "   - Cons: Requires additional libraries, not as universally supported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "016a2790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code to convert between formats:\n",
      "\n",
      "# CSV to Excel\n",
      "df = pd.read_csv('data.csv')\n",
      "df.to_excel('data.xlsx', index=False)\n",
      "\n",
      "# Excel to JSON\n",
      "df = pd.read_excel('data.xlsx')\n",
      "df.to_json('data.json', orient='records')\n",
      "\n",
      "# JSON to Parquet\n",
      "df = pd.read_json('data.json')\n",
      "df.to_parquet('data.parquet', index=False)\n",
      "\n",
      "# Parquet to CSV\n",
      "df = pd.read_parquet('data.parquet')\n",
      "df.to_csv('data_new.csv', index=False)\n"
     ]
    }
   ],
   "source": [
    "# Converting between formats \n",
    "print(\"Code to convert between formats:\")\n",
    "\n",
    "print(\"\\n# CSV to Excel\")\n",
    "print(\"df = pd.read_csv('data.csv')\")\n",
    "print(\"df.to_excel('data.xlsx', index=False)\")\n",
    "\n",
    "print(\"\\n# Excel to JSON\")\n",
    "print(\"df = pd.read_excel('data.xlsx')\")\n",
    "print(\"df.to_json('data.json', orient='records')\")\n",
    "\n",
    "print(\"\\n# JSON to Parquet\")\n",
    "print(\"df = pd.read_json('data.json')\")\n",
    "print(\"df.to_parquet('data.parquet', index=False)\")\n",
    "\n",
    "print(\"\\n# Parquet to CSV\")\n",
    "print(\"df = pd.read_parquet('data.parquet')\")\n",
    "print(\"df.to_csv('data_new.csv', index=False)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3412cc",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've learned how to:\n",
    "\n",
    "1. **Load data from CSV files** with various options for separators, headers, and data types\n",
    "2. **Work with Excel files** and handle different sheets and ranges\n",
    "3. **Process JSON data** from files and APIs\n",
    "4. **Handle encoding issues** with different character sets\n",
    "5. **Load data directly from URLs** without downloading files\n",
    "6. **Compare different file formats** for storage efficiency and speed\n",
    "7. **Convert data between formats** for different use cases\n",
    "\n",
    "These skills are fundamental for any data science workflow, as data loading is typically the first step in any analysis or machine learning project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
