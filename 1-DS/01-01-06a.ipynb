{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b64c4977",
   "metadata": {},
   "source": [
    "# Basic Data Cleaning Techniques\n",
    "\n",
    "Data cleaning is one of the most important steps in any data science project. This notebook covers essential techniques for preparing your data for analysis and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59310e1",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Let's start by importing essential libraries for data cleaning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd74362c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set visualization styles\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('Set2')\n",
    "\n",
    "# Display settings for pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.precision', 3)\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1bd5cd",
   "metadata": {},
   "source": [
    "## Loading Sample Data\n",
    "\n",
    "For this demonstration, we'll create sample datasets with common data quality issues like missing values, duplicates, and inconsistent formatting. We'll also load a built-in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8054004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample dataset with common data quality issues\n",
    "def create_sample_data(rows=100):\n",
    "    \"\"\"Create a sample dataframe with various data quality issues\"\"\"\n",
    "    \n",
    "    # Base data with some random values\n",
    "    data = {\n",
    "        'customer_id': np.arange(1, rows+1),\n",
    "        'age': np.random.randint(18, 90, size=rows),\n",
    "        'income': np.random.normal(50000, 15000, size=rows).round(2),\n",
    "        'date_joined': pd.date_range(start='2020-01-01', periods=rows),\n",
    "        'last_purchase': pd.date_range(start='2020-02-01', periods=rows),\n",
    "        'category': np.random.choice(['A', 'B', 'C', 'D'], size=rows),\n",
    "        'email': [f'customer{i}@example.com' for i in range(1, rows+1)],\n",
    "        'address': [f'{i} Main Street' for i in range(1, rows+1)]\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Introduce missing values (approximately 10%)\n",
    "    for col in df.columns:\n",
    "        if col != 'customer_id':  # Keep IDs intact\n",
    "            mask = np.random.random(size=rows) < 0.1\n",
    "            df.loc[mask, col] = np.nan\n",
    "    \n",
    "    # Add duplicate rows (approximately 5%)\n",
    "    dupes = df.sample(int(rows * 0.05)).copy()\n",
    "    df = pd.concat([df, dupes], ignore_index=True)\n",
    "    \n",
    "    # Add some outliers to numeric columns\n",
    "    outlier_idx = np.random.choice(df.index, size=3)\n",
    "    df.loc[outlier_idx, 'age'] = [120, 125, 130]\n",
    "    \n",
    "    outlier_idx = np.random.choice(df.index, size=3)\n",
    "    df.loc[outlier_idx, 'income'] = [200000, 250000, 300000]\n",
    "    \n",
    "    # Add inconsistent formatting to categorical columns\n",
    "    mask = np.random.random(size=len(df)) < 0.1\n",
    "    df.loc[mask, 'category'] = df.loc[mask, 'category'].str.lower()\n",
    "    \n",
    "    # Add extra whitespace to some text fields\n",
    "    mask = np.random.random(size=len(df)) < 0.1\n",
    "    df.loc[mask, 'email'] = '  ' + df.loc[mask, 'email'] + '  '\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate messy data\n",
    "messy_data = create_sample_data(100)\n",
    "messy_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also load a real-world dataset for comparison\n",
    "titanic = sns.load_dataset('titanic')\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec051da",
   "metadata": {},
   "source": [
    "## Identifying Missing Values\n",
    "\n",
    "Missing values are common in real-world datasets. Let's explore methods to detect them using pandas functions and visualize their patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82141a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic check for missing values in our messy data\n",
    "print(\"Missing values per column:\")\n",
    "print(messy_data.isnull().sum())\n",
    "\n",
    "print(\"\\nPercentage of missing values:\")\n",
    "print((messy_data.isnull().sum() / len(messy_data) * 100).round(2))\n",
    "\n",
    "# Check for missing values in the titanic dataset\n",
    "print(\"\\nMissing values in the Titanic dataset:\")\n",
    "print(titanic.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c575834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing values using a heatmap\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(messy_data.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values in Messy Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(titanic.isnull(), cbar=False, cmap='viridis', yticklabels=False)\n",
    "plt.title('Missing Values in Titanic Dataset')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165efc80",
   "metadata": {},
   "source": [
    "The heatmap provides a visual representation of missing values in the dataset. Yellow areas indicate missing values, while purple areas indicate existing values.\n",
    "\n",
    "We can also use the `missingno` library for enhanced visualization of missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70405835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install missingno if needed\n",
    "# !pip install missingno\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "# Matrix visualization of missing values\n",
    "msno.matrix(messy_data)\n",
    "plt.title('Matrix of Missing Values')\n",
    "plt.show()\n",
    "\n",
    "# Correlation of missing values\n",
    "msno.heatmap(messy_data)\n",
    "plt.title('Correlation of Missing Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e31aca8",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "\n",
    "There are several strategies for dealing with missing data:\n",
    "\n",
    "1. Removing rows or columns with missing values\n",
    "2. Filling missing values with statistical measures (mean, median, mode)\n",
    "3. Forward/backward fill (for time series data)\n",
    "4. Using interpolation methods\n",
    "5. Predictive imputation (using models to predict missing values)\n",
    "\n",
    "Let's implement some of these strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d210ac15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of our data to work with\n",
    "df_clean = messy_data.copy()\n",
    "\n",
    "# 1. Remove rows with too many missing values (more than 50%)\n",
    "threshold = len(df_clean.columns) * 0.5\n",
    "df_clean = df_clean.dropna(thresh=threshold)\n",
    "print(f\"Rows after dropping those with >50% missing: {len(df_clean)} (original: {len(messy_data)})\")\n",
    "\n",
    "# 2. Fill numeric columns with median (more robust to outliers than mean)\n",
    "df_clean['age'] = df_clean['age'].fillna(df_clean['age'].median())\n",
    "df_clean['income'] = df_clean['income'].fillna(df_clean['income'].median())\n",
    "\n",
    "# 3. Fill categorical columns with mode\n",
    "df_clean['category'] = df_clean['category'].fillna(df_clean['category'].mode()[0])\n",
    "\n",
    "# 4. For date columns, use forward fill (assuming the last known date)\n",
    "df_clean['date_joined'] = df_clean['date_joined'].fillna(method='ffill')\n",
    "df_clean['last_purchase'] = df_clean['last_purchase'].fillna(method='ffill')\n",
    "\n",
    "# 5. For text columns, fill with appropriate placeholders\n",
    "df_clean['email'] = df_clean['email'].fillna('unknown@example.com')\n",
    "df_clean['address'] = df_clean['address'].fillna('No address provided')\n",
    "\n",
    "# Check if we have any remaining missing values\n",
    "print(\"\\nMissing values after cleaning:\")\n",
    "print(df_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a070e",
   "metadata": {},
   "source": [
    "Let's also demonstrate some more advanced methods for handling missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4052bb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with the Titanic dataset for more practical examples\n",
    "titanic_clean = titanic.copy()\n",
    "\n",
    "# Fill 'age' missing values using interpolation based on other features\n",
    "# First, let's see if there's any correlation between age and other variables\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='pclass', y='age', data=titanic_clean)\n",
    "plt.title('Age Distribution by Passenger Class')\n",
    "plt.show()\n",
    "\n",
    "# Group median age by passenger class and sex (a common approach)\n",
    "age_by_group = titanic_clean.groupby(['pclass', 'sex'])['age'].median()\n",
    "print(\"Median age by passenger class and sex:\")\n",
    "print(age_by_group)\n",
    "\n",
    "# Function to fill missing age based on passenger class and sex\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        return age_by_group[row['pclass'], row['sex']]\n",
    "    return row['age']\n",
    "\n",
    "titanic_clean['age'] = titanic_clean.apply(fill_age, axis=1)\n",
    "\n",
    "# For the 'embarked' column, use the most frequent value\n",
    "most_common_embarked = titanic_clean['embarked'].mode()[0]\n",
    "titanic_clean['embarked'] = titanic_clean['embarked'].fillna(most_common_embarked)\n",
    "\n",
    "# For 'deck' (cabin first letter), it's mostly missing, so we'll create a 'Unknown' category\n",
    "titanic_clean['deck'] = titanic_clean['cabin'].str[0]\n",
    "titanic_clean['deck'] = titanic_clean['deck'].fillna('U')  # U for Unknown\n",
    "\n",
    "print(\"\\nMissing values in cleaned Titanic data:\")\n",
    "print(titanic_clean.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23556a4",
   "metadata": {},
   "source": [
    "## Removing Duplicates\n",
    "\n",
    "Duplicate records can skew your analysis. Let's identify and remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ffcc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates in our messy dataset\n",
    "duplicate_count = messy_data.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Show some duplicate rows if they exist\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nSample of duplicate rows:\")\n",
    "    duplicates = messy_data[messy_data.duplicated(keep='first')]\n",
    "    print(duplicates.head())\n",
    "\n",
    "# Remove duplicates, keeping the first occurrence\n",
    "df_clean = df_clean.drop_duplicates(keep='first')\n",
    "print(f\"\\nRows after removing duplicates: {len(df_clean)}\")\n",
    "\n",
    "# We can also check for duplicates based on specific columns\n",
    "# For example, duplicate customer IDs\n",
    "customer_id_dupes = df_clean.duplicated(subset=['customer_id'], keep=False)\n",
    "print(f\"Rows with duplicate customer IDs: {customer_id_dupes.sum()}\")\n",
    "\n",
    "if customer_id_dupes.sum() > 0:\n",
    "    print(\"\\nSample rows with duplicate customer IDs:\")\n",
    "    print(df_clean[customer_id_dupes].sort_values('customer_id').head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cc94af",
   "metadata": {},
   "source": [
    "## Fixing Data Types\n",
    "\n",
    "Converting columns to appropriate data types is crucial for proper data handling. It improves memory usage and allows for correct operations on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d5c8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current data types\n",
    "print(\"Current data types:\")\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "# Convert columns to appropriate types\n",
    "# 1. Convert date columns to datetime if they're not already\n",
    "df_clean['date_joined'] = pd.to_datetime(df_clean['date_joined'])\n",
    "df_clean['last_purchase'] = pd.to_datetime(df_clean['last_purchase'])\n",
    "\n",
    "# 2. Ensure numeric columns are the right type\n",
    "df_clean['customer_id'] = df_clean['customer_id'].astype('int32')  # smaller int type\n",
    "df_clean['age'] = df_clean['age'].astype('int16')  # smaller int type\n",
    "# Income should be float for precision\n",
    "df_clean['income'] = df_clean['income'].astype('float32')  # smaller float type\n",
    "\n",
    "# 3. Convert categorical variables to category type for efficiency\n",
    "df_clean['category'] = df_clean['category'].astype('category')\n",
    "\n",
    "# Check updated data types\n",
    "print(\"\\nUpdated data types:\")\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "# Check memory usage before and after type conversion\n",
    "original_memory = messy_data.memory_usage(deep=True).sum() / 1024**2  # in MB\n",
    "optimized_memory = df_clean.memory_usage(deep=True).sum() / 1024**2  # in MB\n",
    "\n",
    "print(f\"\\nMemory usage before optimization: {original_memory:.2f} MB\")\n",
    "print(f\"Memory usage after optimization: {optimized_memory:.2f} MB\")\n",
    "print(f\"Memory savings: {(1 - optimized_memory/original_memory) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373a24d",
   "metadata": {},
   "source": [
    "## Detecting and Handling Outliers\n",
    "\n",
    "Outliers can significantly affect statistical analyses and models. Let's identify them using statistical methods and visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributions to spot outliers\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Boxplot for age\n",
    "sns.boxplot(y=df_clean['age'], ax=ax[0])\n",
    "ax[0].set_title('Age Distribution')\n",
    "\n",
    "# Boxplot for income\n",
    "sns.boxplot(y=df_clean['income'], ax=ax[1])\n",
    "ax[1].set_title('Income Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Let's use the IQR method to detect outliers\n",
    "def detect_outliers_iqr(df, column, lower_factor=1.5, upper_factor=1.5):\n",
    "    \"\"\"Detect outliers using the IQR method\"\"\"\n",
    "    q1 = df[column].quantile(0.25)\n",
    "    q3 = df[column].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - (lower_factor * iqr)\n",
    "    upper_bound = q3 + (upper_factor * iqr)\n",
    "    \n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers in age\n",
    "age_outliers, age_lb, age_ub = detect_outliers_iqr(df_clean, 'age')\n",
    "print(f\"Age outliers detected: {len(age_outliers)}\")\n",
    "print(f\"Age bounds: [{age_lb}, {age_ub}]\")\n",
    "print(age_outliers[['customer_id', 'age']].head())\n",
    "\n",
    "# Detect outliers in income\n",
    "income_outliers, income_lb, income_ub = detect_outliers_iqr(df_clean, 'income')\n",
    "print(f\"\\nIncome outliers detected: {len(income_outliers)}\")\n",
    "print(f\"Income bounds: [{income_lb:.2f}, {income_ub:.2f}]\")\n",
    "print(income_outliers[['customer_id', 'income']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e358d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers in age (clipping to a reasonable range)\n",
    "df_clean['age_cleaned'] = df_clean['age'].clip(lower=age_lb, upper=age_ub)\n",
    "\n",
    "# For income, use winsorization (similar to clipping but preserves the distribution better)\n",
    "df_clean['income_cleaned'] = df_clean['income'].clip(lower=income_lb, upper=income_ub)\n",
    "\n",
    "# Visualize the cleaned distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original age\n",
    "sns.boxplot(y=df_clean['age'], ax=axes[0,0])\n",
    "axes[0,0].set_title('Age - Original')\n",
    "\n",
    "# Cleaned age\n",
    "sns.boxplot(y=df_clean['age_cleaned'], ax=axes[0,1])\n",
    "axes[0,1].set_title('Age - After Outlier Handling')\n",
    "\n",
    "# Original income\n",
    "sns.boxplot(y=df_clean['income'], ax=axes[1,0])\n",
    "axes[1,0].set_title('Income - Original')\n",
    "\n",
    "# Cleaned income\n",
    "sns.boxplot(y=df_clean['income_cleaned'], ax=axes[1,1])\n",
    "axes[1,1].set_title('Income - After Outlier Handling')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7e2d9",
   "metadata": {},
   "source": [
    "## String Cleaning and Standardization\n",
    "\n",
    "Text data often needs cleaning to remove whitespace, standardize case, handle special characters, and maintain consistent formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469963a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for inconsistencies in string columns\n",
    "print(\"Category values:\")\n",
    "print(df_clean['category'].value_counts())\n",
    "\n",
    "print(\"\\nSample email values:\")\n",
    "print(df_clean['email'].head(10))\n",
    "\n",
    "# 1. Clean string values\n",
    "# Strip whitespace, standardize case for category\n",
    "df_clean['category'] = df_clean['category'].str.strip().str.upper()\n",
    "\n",
    "# Clean email addresses\n",
    "df_clean['email'] = df_clean['email'].str.strip().str.lower()\n",
    "\n",
    "# Check results after cleaning\n",
    "print(\"\\nCategory values after cleaning:\")\n",
    "print(df_clean['category'].value_counts())\n",
    "\n",
    "# 2. Standardize formats (e.g., create a consistent address format)\n",
    "# Extract and standardize address components\n",
    "# This is a simplified example - real address parsing would be more complex\n",
    "df_clean['address_cleaned'] = df_clean['address'].str.replace('Street', 'St.').str.strip()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nSample addresses after standardization:\")\n",
    "print(pd.DataFrame({\n",
    "    'Original': df_clean['address'].head(),\n",
    "    'Standardized': df_clean['address_cleaned'].head()\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f5a693",
   "metadata": {},
   "source": [
    "## Applying Data Constraints\n",
    "\n",
    "Ensuring data validity through domain-specific constraints is essential for reliable analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply domain constraints to make sure data falls within expected ranges\n",
    "# 1. Age constraints (humans typically live between 0-120 years)\n",
    "df_clean['age_constrained'] = df_clean['age'].clip(0, 120)\n",
    "\n",
    "# 2. Income constraints (non-negative values)\n",
    "df_clean['income_constrained'] = df_clean['income'].clip(lower=0)\n",
    "\n",
    "# 3. Date constraints (join date must be before last purchase)\n",
    "invalid_dates = df_clean['date_joined'] > df_clean['last_purchase']\n",
    "print(f\"Records with invalid dates (join date after purchase date): {invalid_dates.sum()}\")\n",
    "\n",
    "# Fix invalid dates by swapping them\n",
    "if invalid_dates.sum() > 0:\n",
    "    temp = df_clean.loc[invalid_dates, 'date_joined'].copy()\n",
    "    df_clean.loc[invalid_dates, 'date_joined'] = df_clean.loc[invalid_dates, 'last_purchase']\n",
    "    df_clean.loc[invalid_dates, 'last_purchase'] = temp\n",
    "    \n",
    "    # Verify fix\n",
    "    invalid_dates_after = df_clean['date_joined'] > df_clean['last_purchase']\n",
    "    print(f\"Records with invalid dates after fixing: {invalid_dates_after.sum()}\")\n",
    "\n",
    "# 4. Email format validation using a simple regex pattern\n",
    "import re\n",
    "email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "\n",
    "df_clean['valid_email'] = df_clean['email'].str.match(email_pattern)\n",
    "invalid_emails = ~df_clean['valid_email']\n",
    "print(f\"Records with invalid email format: {invalid_emails.sum()}\")\n",
    "\n",
    "if invalid_emails.sum() > 0:\n",
    "    print(\"Sample invalid emails:\")\n",
    "    print(df_clean.loc[invalid_emails, 'email'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f568d",
   "metadata": {},
   "source": [
    "## Data Cleaning Pipeline\n",
    "\n",
    "Now, let's combine all of these cleaning techniques into a reusable data cleaning pipeline function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d136ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_customer_data(df, handle_outliers=True, standardize_strings=True, apply_constraints=True):\n",
    "    \"\"\"\n",
    "    A comprehensive data cleaning pipeline for customer datasets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input DataFrame to clean\n",
    "    handle_outliers : bool\n",
    "        Whether to handle outliers in numeric columns\n",
    "    standardize_strings : bool\n",
    "        Whether to standardize string values\n",
    "    apply_constraints : bool\n",
    "        Whether to apply domain constraints\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # 1. Handle missing values\n",
    "    # Drop rows with too many missing values (>50%)\n",
    "    threshold = len(clean_df.columns) * 0.5\n",
    "    clean_df = clean_df.dropna(thresh=threshold)\n",
    "    \n",
    "    # Fill numeric columns with median\n",
    "    for col in ['age', 'income']:\n",
    "        if col in clean_df.columns:\n",
    "            clean_df[col] = clean_df[col].fillna(clean_df[col].median())\n",
    "    \n",
    "    # Fill categorical columns with mode\n",
    "    for col in ['category']:\n",
    "        if col in clean_df.columns:\n",
    "            clean_df[col] = clean_df[col].fillna(clean_df[col].mode()[0])\n",
    "    \n",
    "    # For date columns, use forward fill\n",
    "    for col in ['date_joined', 'last_purchase']:\n",
    "        if col in clean_df.columns:\n",
    "            clean_df[col] = clean_df[col].fillna(method='ffill')\n",
    "    \n",
    "    # For text columns, fill with appropriate placeholders\n",
    "    if 'email' in clean_df.columns:\n",
    "        clean_df['email'] = clean_df['email'].fillna('unknown@example.com')\n",
    "    if 'address' in clean_df.columns:\n",
    "        clean_df['address'] = clean_df['address'].fillna('No address provided')\n",
    "    \n",
    "    # 2. Remove duplicates\n",
    "    clean_df = clean_df.drop_duplicates()\n",
    "    \n",
    "    # 3. Fix data types\n",
    "    type_conversions = {\n",
    "        'customer_id': 'int32',\n",
    "        'age': 'int16',\n",
    "        'income': 'float32',\n",
    "        'category': 'category',\n",
    "    }\n",
    "    \n",
    "    for col, dtype in type_conversions.items():\n",
    "        if col in clean_df.columns:\n",
    "            clean_df[col] = clean_df[col].astype(dtype)\n",
    "    \n",
    "    # Convert date columns to datetime\n",
    "    for col in ['date_joined', 'last_purchase']:\n",
    "        if col in clean_df.columns:\n",
    "            clean_df[col] = pd.to_datetime(clean_df[col])\n",
    "    \n",
    "    # 4. Handle outliers if requested\n",
    "    if handle_outliers:\n",
    "        # Handle age outliers\n",
    "        if 'age' in clean_df.columns:\n",
    "            q1 = clean_df['age'].quantile(0.25)\n",
    "            q3 = clean_df['age'].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(0, q1 - (1.5 * iqr))  # Age can't be negative\n",
    "            upper_bound = q3 + (1.5 * iqr)\n",
    "            clean_df['age'] = clean_df['age'].clip(lower=lower_bound, upper=upper_bound)\n",
    "        \n",
    "        # Handle income outliers\n",
    "        if 'income' in clean_df.columns:\n",
    "            q1 = clean_df['income'].quantile(0.25)\n",
    "            q3 = clean_df['income'].quantile(0.75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = max(0, q1 - (1.5 * iqr))  # Income can't be negative\n",
    "            upper_bound = q3 + (1.5 * iqr)\n",
    "            clean_df['income'] = clean_df['income'].clip(lower=lower_bound, upper=upper_bound)\n",
    "    \n",
    "    # 5. Standardize strings if requested\n",
    "    if standardize_strings:\n",
    "        # Standardize category values\n",
    "        if 'category' in clean_df.columns:\n",
    "            clean_df['category'] = clean_df['category'].str.strip().str.upper()\n",
    "        \n",
    "        # Clean email addresses\n",
    "        if 'email' in clean_df.columns:\n",
    "            clean_df['email'] = clean_df['email'].str.strip().str.lower()\n",
    "        \n",
    "        # Standardize address format\n",
    "        if 'address' in clean_df.columns:\n",
    "            clean_df['address'] = clean_df['address'].str.strip()\n",
    "    \n",
    "    # 6. Apply constraints if requested\n",
    "    if apply_constraints:\n",
    "        # Age constraints\n",
    "        if 'age' in clean_df.columns:\n",
    "            clean_df['age'] = clean_df['age'].clip(0, 120)\n",
    "        \n",
    "        # Income constraints\n",
    "        if 'income' in clean_df.columns:\n",
    "            clean_df['income'] = clean_df['income'].clip(lower=0)\n",
    "        \n",
    "        # Date constraints (join date must be before last purchase)\n",
    "        if 'date_joined' in clean_df.columns and 'last_purchase' in clean_df.columns:\n",
    "            invalid_dates = clean_df['date_joined'] > clean_df['last_purchase']\n",
    "            if invalid_dates.sum() > 0:\n",
    "                # Fix by setting last_purchase to date_joined (assuming it's the correct one)\n",
    "                clean_df.loc[invalid_dates, 'last_purchase'] = clean_df.loc[invalid_dates, 'date_joined']\n",
    "    \n",
    "    return clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fa03a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our pipeline on the original messy data\n",
    "fully_cleaned_data = clean_customer_data(messy_data)\n",
    "\n",
    "# Check the results\n",
    "print(\"Original data shape:\", messy_data.shape)\n",
    "print(\"Cleaned data shape:\", fully_cleaned_data.shape)\n",
    "\n",
    "# Check for missing values in the cleaned data\n",
    "print(\"\\nMissing values in cleaned data:\")\n",
    "print(fully_cleaned_data.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types in cleaned data:\")\n",
    "print(fully_cleaned_data.dtypes)\n",
    "\n",
    "# Preview the cleaned data\n",
    "print(\"\\nCleaned data preview:\")\n",
    "fully_cleaned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6092c01a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we covered essential data cleaning techniques:\n",
    "\n",
    "1. **Identifying and handling missing values**:\n",
    "   - Detection using `isna()`, `isnull()`, and visualization\n",
    "   - Strategies like dropping, filling, and interpolation\n",
    "\n",
    "2. **Removing duplicates**:\n",
    "   - Identifying duplicates with `duplicated()`\n",
    "   - Removing duplicates using `drop_duplicates()`\n",
    "\n",
    "3. **Fixing data types**:\n",
    "   - Converting to appropriate types for efficient memory usage and operations\n",
    "   - Using category types for categorical data\n",
    "\n",
    "4. **Detecting and handling outliers**:\n",
    "   - Using statistical methods (IQR, z-score)\n",
    "   - Handling using clipping and winsorization\n",
    "\n",
    "5. **String cleaning and standardization**:\n",
    "   - Removing whitespace, standardizing case\n",
    "   - Handling formatting consistency\n",
    "\n",
    "6. **Applying data constraints**:\n",
    "   - Ensuring logical relationships between variables\n",
    "   - Validating formats and ranges\n",
    "\n",
    "7. **Building a data cleaning pipeline**:\n",
    "   - Combining all techniques into a reusable function\n",
    "\n",
    "Clean data is the foundation of good analysis. Investing time in proper data cleaning helps prevent incorrect conclusions and improves the reliability of your models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
