{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c2b39e",
   "metadata": {},
   "source": [
    "# Loading Data from Various Sources (CSV, Excel, JSON)\n",
    "\n",
    "This notebook provides a comprehensive guide on loading data from different file formats commonly used in data science workflows. We'll explore:\n",
    "1. Creating sample data files\n",
    "2. Loading data from CSV files\n",
    "3. Loading data from Excel files \n",
    "4. Loading data from JSON files\n",
    "5. Handling different data sources\n",
    "6. Performing data cleaning\n",
    "7. Saving data back to different formats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30619ab7",
   "metadata": {},
   "source": [
    "## Required Libraries Setup\n",
    "\n",
    "First, let's install and import all necessary libraries. We'll need:\n",
    "- pandas: for data manipulation\n",
    "- numpy: for numerical operations\n",
    "- matplotlib: for visualization\n",
    "- openpyxl: for Excel file handling\n",
    "- json: for JSON handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db0f459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages if not already installed\n",
    "# Uncomment these lines if you need to install packages\n",
    "# !pip install pandas numpy matplotlib openpyxl\n",
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import os\n",
    "\n",
    "# For displaying plots inline\n",
    "%matplotlib inline\n",
    "\n",
    "# Set display options for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809252a",
   "metadata": {},
   "source": [
    "## Creating Sample Data Files\n",
    "\n",
    "Let's create three sample data files containing similar data but in different formats:\n",
    "1. CSV (Comma Separated Values)\n",
    "2. Excel (.xlsx)\n",
    "3. JSON (JavaScript Object Notation)\n",
    "\n",
    "We'll create a dataset about electronics products and their sales information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5333b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data structure\n",
    "data = {\n",
    "    'product_id': [1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010],\n",
    "    'product_name': ['Laptop', 'Smartphone', 'Tablet', 'Smartwatch', 'Headphones', \n",
    "                     'Gaming Console', 'TV', 'Camera', 'Bluetooth Speaker', 'E-reader'],\n",
    "    'category': ['Computers', 'Phones', 'Computers', 'Wearables', 'Audio', \n",
    "                 'Gaming', 'Home Entertainment', 'Photography', 'Audio', 'Reading'],\n",
    "    'price': [1200.50, 899.99, 399.95, 249.50, 159.99, \n",
    "              499.95, 899.99, 649.95, 79.99, 129.95],\n",
    "    'stock': [45, 120, 35, 78, 90, 25, 15, 28, 60, 55],\n",
    "    'rating': [4.6, 4.5, 4.2, 3.9, 4.7, 4.8, 4.3, 4.1, 4.0, 4.4],\n",
    "    'release_date': ['2023-01-15', '2022-09-20', '2023-03-10', '2022-11-05', '2023-02-28',\n",
    "                     '2022-07-12', '2023-04-18', '2022-12-30', '2023-05-22', '2022-08-08']\n",
    "}\n",
    "\n",
    "# Create a pandas DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Display the data\n",
    "print(\"Sample data created:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00b0945",
   "metadata": {},
   "source": [
    "Now that we have our data ready, let's save it in three different formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996fc6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "csv_file = \"c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\electronics_products.csv\"\n",
    "excel_file = \"c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\electronics_products.xlsx\"\n",
    "json_file = \"c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\electronics_products.json\"\n",
    "\n",
    "# Save as CSV\n",
    "df.to_csv(csv_file, index=False)\n",
    "\n",
    "# Save as Excel\n",
    "df.to_excel(excel_file, sheet_name='Products', index=False)\n",
    "\n",
    "# Save as JSON\n",
    "df.to_json(json_file, orient='records', indent=4)\n",
    "\n",
    "print(f\"Files saved successfully at:\\n- {csv_file}\\n- {excel_file}\\n- {json_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328d6506",
   "metadata": {},
   "source": [
    "## Loading CSV Data\n",
    "\n",
    "CSV (Comma Separated Values) is one of the most common formats for storing structured data. Pandas provides the powerful `read_csv()` function with many parameters to handle various CSV file configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc59fc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic reading of a CSV file\n",
    "df_csv = pd.read_csv(csv_file)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Data loaded from CSV file:\")\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c4c90f",
   "metadata": {},
   "source": [
    "### Advanced CSV Loading Options\n",
    "\n",
    "Let's explore some of the parameters that `read_csv()` offers:\n",
    "- delimiter/sep: specify different delimiters\n",
    "- header: specify header row\n",
    "- skiprows: skip specific rows\n",
    "- usecols: select specific columns\n",
    "- dtype: specify column data types\n",
    "- parse_dates: convert columns to datetime\n",
    "- na_values: specify values to be treated as NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db498f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a slightly modified version of our CSV to demonstrate these options\n",
    "# First, add some rows with missing values\n",
    "df_modified = df.copy()\n",
    "df_modified.loc[2, 'price'] = np.nan\n",
    "df_modified.loc[5, 'rating'] = np.nan\n",
    "df_modified.to_csv('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\modified_products.csv', index=False)\n",
    "\n",
    "# Now let's read it with various options\n",
    "df_csv_advanced = pd.read_csv('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\modified_products.csv',\n",
    "                             dtype={'product_id': int, 'stock': int},\n",
    "                             parse_dates=['release_date'],\n",
    "                             na_values=['N/A', 'unknown'])\n",
    "\n",
    "# Display info about the loaded dataframe\n",
    "print(\"Data types after specifying dtypes and parse_dates:\")\n",
    "print(df_csv_advanced.dtypes)\n",
    "\n",
    "# Show rows with missing values\n",
    "print(\"\\nRows with missing values:\")\n",
    "print(df_csv_advanced[df_csv_advanced.isna().any(axis=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dad0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading only specific columns\n",
    "df_csv_subset = pd.read_csv(csv_file, usecols=['product_id', 'product_name', 'price'])\n",
    "\n",
    "print(\"Reading only specific columns:\")\n",
    "df_csv_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4bd27a",
   "metadata": {},
   "source": [
    "## Loading Excel Data\n",
    "\n",
    "Excel files (.xlsx, .xls) are widely used in business and data analysis. Pandas provides the `read_excel()` function to work with Excel files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04544b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic reading of an Excel file\n",
    "df_excel = pd.read_excel(excel_file)\n",
    "\n",
    "print(\"Data loaded from Excel file:\")\n",
    "df_excel.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd307f9a",
   "metadata": {},
   "source": [
    "### Advanced Excel Loading Options\n",
    "\n",
    "Excel files can contain multiple sheets and complex structures. Let's explore some advanced options:\n",
    "- sheet_name: select specific sheet(s)\n",
    "- usecols: select specific columns\n",
    "- skiprows: skip specific rows\n",
    "- header: specify header row\n",
    "- names: specify column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306056ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a more complex Excel file with multiple sheets\n",
    "with pd.ExcelWriter('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\complex_products.xlsx') as writer:\n",
    "    # First sheet with all data\n",
    "    df.to_excel(writer, sheet_name='All Products', index=False)\n",
    "    \n",
    "    # Second sheet with computers only\n",
    "    computers = df[df['category'] == 'Computers']\n",
    "    computers.to_excel(writer, sheet_name='Computers', index=False)\n",
    "    \n",
    "    # Third sheet with audio products\n",
    "    audio = df[df['category'] == 'Audio']\n",
    "    audio.to_excel(writer, sheet_name='Audio Products', index=False)\n",
    "\n",
    "print(\"Complex Excel file created with multiple sheets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39abf654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading specific sheets\n",
    "df_computers = pd.read_excel('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\complex_products.xlsx', \n",
    "                            sheet_name='Computers')\n",
    "\n",
    "print(\"Data from 'Computers' sheet:\")\n",
    "df_computers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247c6022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading multiple sheets at once\n",
    "all_sheets = pd.read_excel('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\complex_products.xlsx', \n",
    "                          sheet_name=None)  # None returns all sheets as dict\n",
    "\n",
    "# Show available sheets\n",
    "print(\"Available sheets:\", list(all_sheets.keys()))\n",
    "\n",
    "# Display data from each sheet\n",
    "for sheet_name, sheet_data in all_sheets.items():\n",
    "    print(f\"\\nData from '{sheet_name}' sheet:\")\n",
    "    print(sheet_data.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5131ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading specific columns and rows\n",
    "df_excel_subset = pd.read_excel('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\complex_products.xlsx',\n",
    "                              sheet_name='All Products',\n",
    "                              usecols=\"B,D,F\",  # Columns B, D, F (product_name, price, rating)\n",
    "                              skiprows=1,       # Skip first row (header)\n",
    "                              header=0)         # Use first row as header\n",
    "\n",
    "print(\"Subset of Excel data with specific columns:\")\n",
    "df_excel_subset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef916ee",
   "metadata": {},
   "source": [
    "## Loading JSON Data\n",
    "\n",
    "JSON (JavaScript Object Notation) is a popular data format used for configuration files and web services. Pandas provides the `read_json()` function to work with JSON data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27214bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic reading of a JSON file\n",
    "df_json = pd.read_json(json_file)\n",
    "\n",
    "print(\"Data loaded from JSON file:\")\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc726c14",
   "metadata": {},
   "source": [
    "### Advanced JSON Loading Options\n",
    "\n",
    "JSON can have various structures, and pandas offers options to handle them:\n",
    "- orient: specify JSON structure ('records', 'columns', 'index', 'split', 'table')\n",
    "- lines: read JSON objects line by line (for JSON Lines format)\n",
    "- convert_dates: convert date strings to datetime objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c43e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create different JSON structures for demonstration\n",
    "\n",
    "# 1. JSON with 'records' orientation (list of objects)\n",
    "with open('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\products_records.json', 'w') as f:\n",
    "    json.dump(df.to_dict(orient='records'), f, indent=2)\n",
    "\n",
    "# 2. JSON with 'columns' orientation (columns as keys)\n",
    "with open('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\products_columns.json', 'w') as f:\n",
    "    json.dump(df.to_dict(orient='columns'), f, indent=2)\n",
    "\n",
    "# 3. JSON Lines format (each line is a valid JSON object)\n",
    "with open('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\products_lines.jsonl', 'w') as f:\n",
    "    for record in df.to_dict(orient='records'):\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(\"Created three different JSON structure files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f160361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON with different orientations\n",
    "df_json_records = pd.read_json('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\products_records.json')\n",
    "print(\"JSON 'records' format loaded:\")\n",
    "print(df_json_records.head(3))\n",
    "\n",
    "df_json_columns = pd.read_json('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\products_columns.json')\n",
    "print(\"\\nJSON 'columns' format loaded:\")\n",
    "print(df_json_columns.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f22f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading JSON lines format\n",
    "df_json_lines = pd.read_json('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\products_lines.jsonl', lines=True)\n",
    "print(\"JSON Lines format loaded:\")\n",
    "print(df_json_lines.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f9bc87",
   "metadata": {},
   "source": [
    "### Working with Nested JSON\n",
    "\n",
    "JSON can have nested structures that may require preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c4d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a nested JSON structure\n",
    "nested_data = []\n",
    "for i, row in df.iterrows():\n",
    "    nested_data.append({\n",
    "        'product_id': row['product_id'],\n",
    "        'product_name': row['product_name'],\n",
    "        'details': {\n",
    "            'category': row['category'],\n",
    "            'price': row['price'],\n",
    "            'stock': row['stock']\n",
    "        },\n",
    "        'meta': {\n",
    "            'rating': row['rating'],\n",
    "            'release_date': row['release_date']\n",
    "        }\n",
    "    })\n",
    "\n",
    "# Save nested JSON\n",
    "with open('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\nested_products.json', 'w') as f:\n",
    "    json.dump(nested_data, f, indent=2)\n",
    "\n",
    "print(\"Created nested JSON structure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73bb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the nested JSON file\n",
    "with open('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\nested_products.json', 'r') as f:\n",
    "    nested_json = json.load(f)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_nested = pd.json_normalize(\n",
    "    nested_json,\n",
    "    sep='_'  # Separator for nested column names\n",
    ")\n",
    "\n",
    "print(\"Nested JSON normalized to DataFrame:\")\n",
    "df_nested.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f93bb7",
   "metadata": {},
   "source": [
    "## Handling Different Data Sources\n",
    "\n",
    "Now that we've loaded data from different sources, let's compare them to ensure consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3aaed8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataframes from different sources are identical\n",
    "csv_equals_excel = df_csv.equals(df_excel)\n",
    "csv_equals_json = df_csv.equals(df_json)\n",
    "\n",
    "print(f\"CSV and Excel data are identical: {csv_equals_excel}\")\n",
    "print(f\"CSV and JSON data are identical: {csv_equals_json}\")\n",
    "\n",
    "if not (csv_equals_excel and csv_equals_json):\n",
    "    print(\"\\nDifferences may exist in data types. Let's check data types from each source:\")\n",
    "    \n",
    "    print(\"\\nCSV data types:\")\n",
    "    print(df_csv.dtypes)\n",
    "    \n",
    "    print(\"\\nExcel data types:\")\n",
    "    print(df_excel.dtypes)\n",
    "    \n",
    "    print(\"\\nJSON data types:\")\n",
    "    print(df_json.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09510e98",
   "metadata": {},
   "source": [
    "### Handling Data Type Issues\n",
    "\n",
    "Different file formats may lead to different data type interpretations. Let's standardize the data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11259e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize data types across dataframes\n",
    "def standardize_dtypes(df):\n",
    "    df_copy = df.copy()\n",
    "    # Convert specific columns to appropriate types\n",
    "    df_copy['product_id'] = df_copy['product_id'].astype(int)\n",
    "    df_copy['price'] = df_copy['price'].astype(float)\n",
    "    df_copy['stock'] = df_copy['stock'].astype(int)\n",
    "    df_copy['rating'] = df_copy['rating'].astype(float)\n",
    "    df_copy['release_date'] = pd.to_datetime(df_copy['release_date'])\n",
    "    return df_copy\n",
    "\n",
    "# Standardize data types for all dataframes\n",
    "df_csv_std = standardize_dtypes(df_csv)\n",
    "df_excel_std = standardize_dtypes(df_excel)\n",
    "df_json_std = standardize_dtypes(df_json)\n",
    "\n",
    "# Check if standardized dataframes are now identical (ignoring index)\n",
    "csv_equals_excel = df_csv_std.reset_index(drop=True).equals(df_excel_std.reset_index(drop=True))\n",
    "csv_equals_json = df_csv_std.reset_index(drop=True).equals(df_json_std.reset_index(drop=True))\n",
    "\n",
    "print(f\"After standardization, CSV and Excel data are identical: {csv_equals_excel}\")\n",
    "print(f\"After standardization, CSV and JSON data are identical: {csv_equals_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3636f",
   "metadata": {},
   "source": [
    "## Data Cleaning Techniques\n",
    "\n",
    "Let's demonstrate some common data cleaning procedures for each data source type:\n",
    "1. Handling missing values\n",
    "2. Removing duplicates\n",
    "3. Converting data types\n",
    "4. Standardizing string values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5706f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a messy dataset to clean\n",
    "messy_data = {\n",
    "    'product_id': [1001, 1002, 1002, 1003, None, 1005],\n",
    "    'product_name': ['Laptop', 'smartphone', 'Smartphone', 'Tablet ', None, 'headphones'],\n",
    "    'category': ['Computers', 'Phones', 'phones', 'Computers', 'Unknown', 'audio'],\n",
    "    'price': ['1200.50', '899.99', '899.99', 'N/A', '159.99', '?'],\n",
    "    'stock': ['45', '120', '120', '35', '', '90'],\n",
    "    'rating': ['4.6', '4.5', '4.5', '4,2', 'pending', '4.7'],\n",
    "    'release_date': ['2023-01-15', '22-09-2022', '22/09/2022', '10-03-2023', '2023', '02/28/2023']\n",
    "}\n",
    "\n",
    "messy_df = pd.DataFrame(messy_data)\n",
    "print(\"Messy data to clean:\")\n",
    "messy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af813769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(messy_df.isnull().sum())\n",
    "\n",
    "# Step 2: Check for duplicates\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(messy_df[messy_df.duplicated(subset=['product_id', 'product_name'], keep=False)])\n",
    "\n",
    "# Step 3: Data types overview\n",
    "print(\"\\nData types before cleaning:\")\n",
    "print(messy_df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcb26a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's clean the data step by step\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "cleaned_df = messy_df.copy()\n",
    "cleaned_df['product_id'] = cleaned_df['product_id'].fillna(9999)  # Fill missing IDs with a placeholder\n",
    "cleaned_df['product_name'] = cleaned_df['product_name'].fillna('Unknown Product')\n",
    "\n",
    "# Step 2: Remove duplicates\n",
    "cleaned_df = cleaned_df.drop_duplicates(subset=['product_id', 'product_name'], keep='first')\n",
    "\n",
    "# Step 3: Fix price column - replace non-numeric values and convert to float\n",
    "cleaned_df['price'] = cleaned_df['price'].replace(['N/A', '?'], np.nan)\n",
    "cleaned_df['price'] = pd.to_numeric(cleaned_df['price'], errors='coerce')\n",
    "\n",
    "# Step 4: Fix stock column - convert to integer\n",
    "cleaned_df['stock'] = pd.to_numeric(cleaned_df['stock'], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# Step 5: Fix rating column - replace comma with dot, convert to float\n",
    "cleaned_df['rating'] = cleaned_df['rating'].str.replace(',', '.', regex=False)\n",
    "cleaned_df['rating'] = pd.to_numeric(cleaned_df['rating'], errors='coerce')\n",
    "\n",
    "# Step 6: Standardize text columns (lowercase, strip spaces)\n",
    "cleaned_df['product_name'] = cleaned_df['product_name'].str.strip().str.title()\n",
    "cleaned_df['category'] = cleaned_df['category'].str.strip().str.title()\n",
    "\n",
    "# Step 7: Parse dates using a custom function\n",
    "def parse_date(date_str):\n",
    "    if pd.isna(date_str):\n",
    "        return pd.NaT\n",
    "    \n",
    "    # Try different date formats\n",
    "    formats = ['%Y-%m-%d', '%d-%m-%Y', '%d/%m/%Y', '%m/%d/%Y', '%Y']\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            return pd.to_datetime(date_str, format=fmt)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    return pd.NaT\n",
    "\n",
    "cleaned_df['release_date'] = cleaned_df['release_date'].apply(parse_date)\n",
    "\n",
    "# Display cleaned data\n",
    "print(\"Data after cleaning:\")\n",
    "cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbe4f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types after cleaning\n",
    "print(\"Data types after cleaning:\")\n",
    "print(cleaned_df.dtypes)\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary statistics after cleaning:\")\n",
    "print(cleaned_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d86851b",
   "metadata": {},
   "source": [
    "## Saving Data to Different Formats\n",
    "\n",
    "After processing and cleaning our data, we often need to save it back to disk in various formats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7956d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned data to CSV\n",
    "cleaned_df.to_csv('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\cleaned_products.csv', index=False)\n",
    "\n",
    "# Save cleaned data to Excel with some formatting\n",
    "with pd.ExcelWriter('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\cleaned_products.xlsx', engine='openpyxl') as writer:\n",
    "    cleaned_df.to_excel(writer, sheet_name='Cleaned Products', index=False)\n",
    "    # You can add more sheets or formatting here if needed\n",
    "\n",
    "# Save cleaned data to JSON in different formats\n",
    "cleaned_df.to_json('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\cleaned_products_records.json', \n",
    "                  orient='records', indent=2)\n",
    "\n",
    "cleaned_df.to_json('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\cleaned_products_columns.json', \n",
    "                  orient='columns', indent=2)\n",
    "\n",
    "print(\"Cleaned data saved to various formats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881c841f",
   "metadata": {},
   "source": [
    "### Advanced Export Options\n",
    "\n",
    "Let's explore some advanced options for saving data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0424f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Save CSV with specific options\n",
    "cleaned_df.to_csv('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\cleaned_products_advanced.csv', \n",
    "                 index=False,\n",
    "                 sep=';',  # Use semicolon as delimiter\n",
    "                 date_format='%Y-%m-%d',  # Specify date format\n",
    "                 float_format='%.2f')  # Format floats to 2 decimal places\n",
    "\n",
    "# 2. Save Excel with formatting\n",
    "from openpyxl.styles import Font, PatternFill, Alignment, Border, Side\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# Create a new Excel file with openpyxl for more formatting control\n",
    "import openpyxl\n",
    "wb = openpyxl.Workbook()\n",
    "ws = wb.active\n",
    "ws.title = \"Formatted Products\"\n",
    "\n",
    "# Add the dataframe\n",
    "for r_idx, row in enumerate(dataframe_to_rows(cleaned_df, index=False, header=True), 1):\n",
    "    for c_idx, value in enumerate(row, 1):\n",
    "        ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "# Apply formatting to header row\n",
    "header_fill = PatternFill(start_color=\"4472C4\", end_color=\"4472C4\", fill_type=\"solid\")\n",
    "header_font = Font(color=\"FFFFFF\", bold=True)\n",
    "\n",
    "for cell in ws[1]:\n",
    "    cell.fill = header_fill\n",
    "    cell.font = header_font\n",
    "    cell.alignment = Alignment(horizontal=\"center\")\n",
    "\n",
    "# Auto adjust column width\n",
    "for col in ws.columns:\n",
    "    max_length = 0\n",
    "    column = col[0].column_letter\n",
    "    for cell in col:\n",
    "        if cell.value:\n",
    "            max_length = max(max_length, len(str(cell.value)))\n",
    "    adjusted_width = max_length + 2\n",
    "    ws.column_dimensions[column].width = adjusted_width\n",
    "\n",
    "# Save the workbook\n",
    "wb.save('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\formatted_products.xlsx')\n",
    "\n",
    "print(\"Advanced formatted Excel file saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Save filtered data\n",
    "# For example, save only high-priced items to a separate file\n",
    "high_priced = cleaned_df[cleaned_df['price'] > 500]\n",
    "high_priced.to_json('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\high_priced_products.json', orient='records', indent=2)\n",
    "\n",
    "# 4. Save data as HTML table\n",
    "html_table = cleaned_df.to_html(index=False, classes='table table-striped')\n",
    "with open('c:\\\\Users\\\\pavel\\\\projects\\\\ai-ml\\\\products_table.html', 'w') as f:\n",
    "    f.write('''\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Products Table</title>\n",
    "        <link rel=\"stylesheet\" href=\"https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css\">\n",
    "    </head>\n",
    "    <body class=\"container mt-5\">\n",
    "        <h2>Electronics Products Data</h2>\n",
    "    ''' + html_table + '''\n",
    "    </body>\n",
    "    </html>\n",
    "    ''')\n",
    "\n",
    "print(\"Additional export formats created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185ef28",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've covered:\n",
    "\n",
    "1. **Creating Sample Data Files** - We created CSV, Excel, and JSON files containing similar data.\n",
    "\n",
    "2. **Loading CSV Data** - We explored various parameters of `pd.read_csv()` to handle different CSV formats.\n",
    "\n",
    "3. **Loading Excel Data** - We demonstrated how to work with Excel files, including handling multiple sheets using `pd.read_excel()`.\n",
    "\n",
    "4. **Loading JSON Data** - We examined different JSON structures and how to load them with `pd.read_json()` and `json_normalize()`.\n",
    "\n",
    "5. **Handling Different Data Sources** - We compared data loaded from different sources and addressed inconsistencies.\n",
    "\n",
    "6. **Data Cleaning Techniques** - We applied common data cleaning operations to fix messy data.\n",
    "\n",
    "7. **Saving Data to Different Formats** - We explored various options to export data with specific formatting.\n",
    "\n",
    "These skills are essential for any data scientist, as most data analysis projects begin with loading and preprocessing data from various sources."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
