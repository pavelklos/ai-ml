{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9394902",
   "metadata": {},
   "source": [
    "# Simple Data Transformation\n",
    "\n",
    "This notebook demonstrates various techniques for transforming data in Python using pandas, NumPy, and other libraries. We'll cover simple transformations that are essential for data preprocessing and feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdaf200",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024b2dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Allow to display all columns\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994bc44f",
   "metadata": {},
   "source": [
    "### Create sample datasets for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400011de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple dataset for demonstration purposes\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "# Create a basic dataframe with different data types\n",
    "df = pd.DataFrame({\n",
    "    'id': range(1, 11),\n",
    "    'name': ['John Smith', 'Jane Doe', 'Bob Johnson', 'Maria Garcia', 'Wei Chen', \n",
    "             'Ahmed Ali', 'Sara Patel', 'Tom Wilson', 'Anna Kim', 'Luis Rodriguez'],\n",
    "    'age': np.random.randint(18, 65, 10),\n",
    "    'income': np.random.randint(30000, 120000, 10),\n",
    "    'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD'], 10),\n",
    "    'score': np.random.uniform(0, 100, 10).round(2),\n",
    "    'registered': np.random.choice([True, False], 10),\n",
    "    'join_date': pd.date_range(start='2020-01-01', periods=10, freq='M')\n",
    "})\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(\"Sample dataframe:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48865f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another dataset with some missing values and outliers\n",
    "df_messy = pd.DataFrame({\n",
    "    'product': ['Laptop', 'Phone', 'Tablet', 'Monitor', 'Keyboard', 'Mouse', 'Printer', 'Speaker', 'Headphones', 'Camera'],\n",
    "    'price': [1200, 800, 500, 300, np.nan, 50, 150, 120, 80, 450],\n",
    "    'stock': [10, 25, 15, np.nan, 30, 40, 5, np.nan, 20, 10],\n",
    "    'rating': [4.5, 4.2, 3.8, 4.0, 3.5, 4.7, np.nan, 3.9, 4.1, 4.3],\n",
    "    'category': ['Electronics', 'Electronics', 'Electronics', 'Electronics', 'Accessories', \n",
    "                'Accessories', 'Electronics', 'Accessories', 'Accessories', 'Electronics'],\n",
    "    'last_updated': pd.date_range(start='2023-01-01', periods=10, freq='3D')\n",
    "})\n",
    "\n",
    "print(\"Dataset with missing values:\")\n",
    "df_messy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910f21e9",
   "metadata": {},
   "source": [
    "## 2. Basic Data Transformations\n",
    "\n",
    "Let's start with some basic transformations like adding/removing columns, renaming columns, and changing data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774f48a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Adding a new column based on existing ones\n",
    "df['income_tier'] = pd.cut(df['income'], \n",
    "                          bins=[0, 40000, 80000, 120000], \n",
    "                          labels=['Low', 'Medium', 'High'])\n",
    "\n",
    "# 2. Removing a column\n",
    "df_reduced = df.drop('id', axis=1)\n",
    "\n",
    "# 3. Renaming columns\n",
    "df_renamed = df.rename(columns={'name': 'full_name', 'age': 'years'})\n",
    "\n",
    "# 4. Changing data types\n",
    "df['age'] = df['age'].astype(float)\n",
    "df['score'] = df['score'].astype(int)\n",
    "\n",
    "# 5. Create a copy of the dataframe for next examples\n",
    "df_transformed = df.copy()\n",
    "\n",
    "# Display the results\n",
    "print(\"Dataframe with new income_tier column:\")\n",
    "print(df[['name', 'income', 'income_tier']].head())\n",
    "print(\"\\nDataframe with 'id' column removed:\")\n",
    "print(df_reduced.columns.tolist())\n",
    "print(\"\\nDataframe with renamed columns:\")\n",
    "print(df_renamed.columns.tolist())\n",
    "print(\"\\nDataframe with changed data types:\")\n",
    "print(df[['age', 'score']].dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1110a4",
   "metadata": {},
   "source": [
    "### Basic Arithmetic Operations on Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d76c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column via arithmetic operation\n",
    "df_transformed['income_after_tax'] = df_transformed['income'] * 0.7  # 30% tax\n",
    "\n",
    "# Combine columns with different operations\n",
    "df_transformed['score_weight'] = df_transformed['score'] * 0.2 + df_transformed['age'] * 0.1\n",
    "\n",
    "# Apply conditional transformations\n",
    "df_transformed['bonus'] = np.where(df_transformed['score'] > 70, 1000, 0)\n",
    "\n",
    "# Display results\n",
    "print(\"Dataframe with arithmetic operations:\")\n",
    "df_transformed[['name', 'income', 'income_after_tax', 'score', 'age', 'score_weight', 'bonus']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d952452",
   "metadata": {},
   "source": [
    "## 3. Mathematical Transformations\n",
    "\n",
    "Mathematical transformations are useful for normalizing data distributions, feature engineering, and preparing data for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194947e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DataFrame for mathematical transformations\n",
    "df_math = df.copy()\n",
    "\n",
    "# Log transformation (useful for right-skewed data)\n",
    "df_math['income_log'] = np.log(df_math['income'])\n",
    "\n",
    "# Square root transformation (another way to handle right skew)\n",
    "df_math['income_sqrt'] = np.sqrt(df_math['income'])\n",
    "\n",
    "# Square transformation\n",
    "df_math['age_squared'] = df_math['age'] ** 2\n",
    "\n",
    "# Exponential transformation\n",
    "df_math['score_exp'] = np.exp(df_math['score'] / 50)  # Scaled down to avoid overflow\n",
    "\n",
    "# Trigonometric transformations (useful for cyclical data)\n",
    "df_math['sine_age'] = np.sin(df_math['age'] * np.pi / 50)\n",
    "\n",
    "# Visualization of transformations for income\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(df_math['income'], kde=True)\n",
    "plt.title('Original Income Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(df_math['income_log'], kde=True)\n",
    "plt.title('Log-transformed Income')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(df_math['income_sqrt'], kde=True)\n",
    "plt.title('Square Root-transformed Income')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.scatterplot(x='age', y='age_squared', data=df_math)\n",
    "plt.title('Age vs Age Squared')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display results\n",
    "df_math[['income', 'income_log', 'income_sqrt', 'age', 'age_squared', 'score', 'score_exp']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5562c",
   "metadata": {},
   "source": [
    "### Polynomial Features Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e092f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating polynomial features (useful for linear models to capture non-linear relationships)\n",
    "df_math['age_squared'] = df_math['age'] ** 2\n",
    "df_math['age_cubed'] = df_math['age'] ** 3\n",
    "\n",
    "# Interaction terms\n",
    "df_math['age_income_interaction'] = df_math['age'] * df_math['income'] / 10000  # Scaled for readability\n",
    "\n",
    "# Display results\n",
    "df_math[['age', 'income', 'age_squared', 'age_cubed', 'age_income_interaction']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f235eda0",
   "metadata": {},
   "source": [
    "## 4. String Transformations\n",
    "\n",
    "String manipulation is essential for text cleaning, feature extraction, and preparing categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0eef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DataFrame for string transformations\n",
    "df_str = df.copy()\n",
    "\n",
    "# Convert strings to lowercase\n",
    "df_str['name_lower'] = df_str['name'].str.lower()\n",
    "\n",
    "# Split a string and extract first and last name\n",
    "df_str[['first_name', 'last_name']] = df_str['name'].str.split(' ', expand=True)\n",
    "\n",
    "# Replace specific content in strings\n",
    "df_str['education_cleaned'] = df_str['education'].str.replace('School', 'Diploma')\n",
    "\n",
    "# Extract specific patterns (e.g., extracting initials)\n",
    "df_str['initials'] = df_str['first_name'].str[0] + df_str['last_name'].str[0]\n",
    "\n",
    "# String concatenation\n",
    "df_str['full_info'] = df_str['name'] + ' | ' + df_str['education'] + ' | Age: ' + df_str['age'].astype(str)\n",
    "\n",
    "# String methods with apply and lambda\n",
    "df_str['name_length'] = df_str['name'].apply(lambda x: len(x))\n",
    "\n",
    "# Display results\n",
    "df_str[['name', 'name_lower', 'first_name', 'last_name', 'initials', 'education', \n",
    "       'education_cleaned', 'full_info', 'name_length']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b6c378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced string operations\n",
    "\n",
    "# Pattern extraction using regex\n",
    "df_str['has_vowel_start'] = df_str['first_name'].str.match(r'^[aeiouAEIOU]')\n",
    "\n",
    "# Count specific characters\n",
    "df_str['vowel_count'] = df_str['name'].apply(lambda x: sum(1 for char in x.lower() if char in 'aeiou'))\n",
    "\n",
    "# Padding strings\n",
    "df_str['id_padded'] = df_str['id'].astype(str).str.zfill(3)\n",
    "\n",
    "# Extract specific parts of strings (e.g., domain from email)\n",
    "sample_emails = pd.Series(['john.smith@example.com', 'jane.doe@company.org', 'bob.johnson@test.net'])\n",
    "domains = sample_emails.str.extract(r'@([^.]+)')\n",
    "\n",
    "print(\"Pattern extraction examples:\")\n",
    "print(domains)\n",
    "\n",
    "# Display results\n",
    "df_str[['name', 'has_vowel_start', 'vowel_count', 'id', 'id_padded']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319ab4ce",
   "metadata": {},
   "source": [
    "## 5. Categorical Data Transformations\n",
    "\n",
    "Converting categorical variables into numerical representations is crucial for machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d8ef24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DataFrame for categorical transformations\n",
    "df_cat = df.copy()\n",
    "\n",
    "# One-hot encoding (good for nominal categories with no inherent order)\n",
    "education_dummies = pd.get_dummies(df_cat['education'], prefix='edu')\n",
    "df_cat = pd.concat([df_cat, education_dummies], axis=1)\n",
    "\n",
    "# Label encoding (good for ordinal categories)\n",
    "le = LabelEncoder()\n",
    "education_mapping = {'High School': 0, 'Bachelor': 1, 'Master': 2, 'PhD': 3}\n",
    "df_cat['education_encoded'] = df_cat['education'].map(education_mapping)\n",
    "\n",
    "# Binary encoding (for boolean values)\n",
    "df_cat['registered_int'] = df_cat['registered'].astype(int)\n",
    "\n",
    "# Ordinal encoding with custom mapping\n",
    "income_mapping = {'Low': 1, 'Medium': 2, 'High': 3}\n",
    "df_cat['income_tier_encoded'] = df_cat['income_tier'].map(income_mapping)\n",
    "\n",
    "# Display the transformations\n",
    "print(\"One-hot encoding for education:\")\n",
    "print(df_cat[['education', 'edu_High School', 'edu_Bachelor', 'edu_Master', 'edu_PhD']].head())\n",
    "print(\"\\nLabel encoding for education:\")\n",
    "print(df_cat[['education', 'education_encoded']].head())\n",
    "print(\"\\nBinary encoding for registered:\")\n",
    "print(df_cat[['registered', 'registered_int']].head())\n",
    "print(\"\\nOrdinal encoding for income tier:\")\n",
    "print(df_cat[['income_tier', 'income_tier_encoded']].drop(df_cat[df_cat['income_tier'].isna()].index).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d94213b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using scikit-learn for encoding\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "\n",
    "# Reshape the data for sklearn transformers\n",
    "education_array = df['education'].values.reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding using scikit-learn\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "education_onehot = onehot_encoder.fit_transform(education_array)\n",
    "education_onehot_df = pd.DataFrame(\n",
    "    education_onehot, \n",
    "    columns=[f'edu_sk_{cat}' for cat in onehot_encoder.categories_[0]],\n",
    "    index=df.index\n",
    ")\n",
    "\n",
    "# Ordinal encoding using scikit-learn\n",
    "ordinal_encoder = OrdinalEncoder(categories=[['High School', 'Bachelor', 'Master', 'PhD']])\n",
    "education_ordinal = ordinal_encoder.fit_transform(education_array)\n",
    "education_ordinal_df = pd.DataFrame(education_ordinal, columns=['education_ordinal'], index=df.index)\n",
    "\n",
    "# Display the scikit-learn transformations\n",
    "print(\"Scikit-learn One-hot encoding:\")\n",
    "print(pd.concat([df['education'], education_onehot_df], axis=1).head())\n",
    "\n",
    "print(\"\\nScikit-learn Ordinal encoding:\")\n",
    "print(pd.concat([df['education'], education_ordinal_df], axis=1).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262579b6",
   "metadata": {},
   "source": [
    "## 6. Custom Transformations with apply() and map()\n",
    "\n",
    "Creating custom transformations using pandas .apply() and .map() functions allows for complex logic and conditional transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012b8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DataFrame for custom transformations\n",
    "df_custom = df.copy()\n",
    "\n",
    "# Simple lambda function with apply\n",
    "df_custom['age_category'] = df_custom['age'].apply(\n",
    "    lambda x: 'Young' if x < 30 else ('Middle-aged' if x < 50 else 'Senior')\n",
    ")\n",
    "\n",
    "# Custom function with multiple conditions\n",
    "def salary_bonus(row):\n",
    "    if row['education'] == 'PhD' and row['age'] < 40:\n",
    "        return row['income'] * 0.2\n",
    "    elif row['education'] in ['Master', 'Bachelor'] and row['age'] < 35:\n",
    "        return row['income'] * 0.15\n",
    "    else:\n",
    "        return row['income'] * 0.1\n",
    "\n",
    "df_custom['bonus_amount'] = df_custom.apply(salary_bonus, axis=1)\n",
    "\n",
    "# Using map with a dictionary\n",
    "title_map = {\n",
    "    'PhD': 'Dr.',\n",
    "    'Master': 'MSc.',\n",
    "    'Bachelor': 'BSc.',\n",
    "    'High School': 'Mr./Ms.'\n",
    "}\n",
    "df_custom['title'] = df_custom['education'].map(title_map)\n",
    "\n",
    "# Combining string operations with apply\n",
    "df_custom['formal_name'] = df_custom.apply(\n",
    "    lambda row: f\"{row['title']} {row['name']}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Display results\n",
    "print(\"Custom transformations results:\")\n",
    "df_custom[['name', 'age', 'age_category', 'education', 'income', \n",
    "          'bonus_amount', 'title', 'formal_name']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914a253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex custom transformations\n",
    "\n",
    "# Function that evaluates multiple columns\n",
    "def calculate_score(row):\n",
    "    base_score = row['score']\n",
    "    \n",
    "    # Age adjustment\n",
    "    if row['age'] < 30:\n",
    "        age_factor = 1.1\n",
    "    elif row['age'] < 50:\n",
    "        age_factor = 1.0\n",
    "    else:\n",
    "        age_factor = 0.9\n",
    "        \n",
    "    # Education adjustment\n",
    "    edu_factors = {\n",
    "        'High School': 0.9,\n",
    "        'Bachelor': 1.0,\n",
    "        'Master': 1.1,\n",
    "        'PhD': 1.2\n",
    "    }\n",
    "    edu_factor = edu_factors[row['education']]\n",
    "    \n",
    "    # Registration bonus\n",
    "    reg_bonus = 5 if row['registered'] else 0\n",
    "    \n",
    "    final_score = base_score * age_factor * edu_factor + reg_bonus\n",
    "    return round(final_score, 1)\n",
    "\n",
    "df_custom['adjusted_score'] = df_custom.apply(calculate_score, axis=1)\n",
    "\n",
    "# Display results of complex transformation\n",
    "print(\"Complex custom transformation results:\")\n",
    "df_custom[['name', 'age', 'education', 'score', 'registered', 'adjusted_score']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196ed725",
   "metadata": {},
   "source": [
    "## 7. Date and Time Transformations\n",
    "\n",
    "Date and time transformations are essential for time series analysis and feature engineering based on time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3dd6e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy DataFrame for date transformations\n",
    "df_date = df.copy()\n",
    "\n",
    "# Extract components from datetime\n",
    "df_date['join_year'] = df_date['join_date'].dt.year\n",
    "df_date['join_month'] = df_date['join_date'].dt.month\n",
    "df_date['join_day'] = df_date['join_date'].dt.day\n",
    "df_date['join_dayofweek'] = df_date['join_date'].dt.dayofweek\n",
    "df_date['join_quarter'] = df_date['join_date'].dt.quarter\n",
    "\n",
    "# Create time-based features\n",
    "df_date['days_since_join'] = (pd.Timestamp.now() - df_date['join_date']).dt.days\n",
    "df_date['is_weekend'] = df_date['join_date'].dt.dayofweek >= 5\n",
    "\n",
    "# Format datetime as string\n",
    "df_date['join_date_formatted'] = df_date['join_date'].dt.strftime('%B %d, %Y')\n",
    "\n",
    "# Extract month name\n",
    "df_date['join_month_name'] = df_date['join_date'].dt.month_name()\n",
    "\n",
    "# Create a reference date and calculate difference\n",
    "reference_date = pd.Timestamp('2020-06-15')\n",
    "df_date['days_from_reference'] = (df_date['join_date'] - reference_date).dt.days\n",
    "\n",
    "# Display results\n",
    "print(\"Date transformations results:\")\n",
    "df_date[['name', 'join_date', 'join_year', 'join_month', 'join_month_name', \n",
    "        'join_day', 'join_dayofweek', 'join_quarter', 'days_since_join', \n",
    "        'is_weekend', 'join_date_formatted', 'days_from_reference']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b70eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More advanced date transformations\n",
    "\n",
    "# Create a date range for demonstration\n",
    "date_range = pd.date_range(start='2022-01-01', end='2022-12-31', freq='W')\n",
    "date_df = pd.DataFrame({'date': date_range})\n",
    "\n",
    "# Create cyclical features for time (useful for machine learning with time data)\n",
    "date_df['day_of_year'] = date_df['date'].dt.dayofyear\n",
    "date_df['month_sin'] = np.sin(2 * np.pi * date_df['date'].dt.month / 12)\n",
    "date_df['month_cos'] = np.cos(2 * np.pi * date_df['date'].dt.month / 12)\n",
    "date_df['day_sin'] = np.sin(2 * np.pi * date_df['date'].dt.dayofyear / 365)\n",
    "date_df['day_cos'] = np.cos(2 * np.pi * date_df['date'].dt.dayofyear / 365)\n",
    "\n",
    "# Fiscal quarter (assuming fiscal year starts in April)\n",
    "date_df['fiscal_quarter'] = (date_df['date'].dt.month - 4) % 12 // 3 + 1\n",
    "\n",
    "# Business days between dates\n",
    "from pandas.tseries.offsets import BDay\n",
    "date_df['business_days_since_start'] = date_df['date'].apply(\n",
    "    lambda x: len(pd.date_range(start='2022-01-01', end=x, freq=BDay)) - 1\n",
    ")\n",
    "\n",
    "# Age binning based on timestamp\n",
    "def timestamp_to_age_bucket(timestamp):\n",
    "    age_days = (pd.Timestamp.now() - timestamp).days\n",
    "    if age_days < 90:\n",
    "        return \"Recent (< 3 months)\"\n",
    "    elif age_days < 180:\n",
    "        return \"Medium (3-6 months)\"\n",
    "    else:\n",
    "        return \"Old (> 6 months)\"\n",
    "\n",
    "date_df['age_bucket'] = date_df['date'].apply(timestamp_to_age_bucket)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Advanced date transformations:\")\n",
    "date_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8624353",
   "metadata": {},
   "source": [
    "## 8. Scaling and Normalization\n",
    "\n",
    "Scaling and normalization are crucial for many machine learning algorithms to ensure features are on similar scales and distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92adfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy numeric columns for scaling\n",
    "numeric_df = df[['age', 'income', 'score']].copy()\n",
    "\n",
    "# Min-Max Scaling (normalization) - scales values to [0,1] range\n",
    "scaler_minmax = MinMaxScaler()\n",
    "numeric_df_minmax = pd.DataFrame(\n",
    "    scaler_minmax.fit_transform(numeric_df), \n",
    "    columns=[col + '_minmax' for col in numeric_df.columns],\n",
    "    index=numeric_df.index\n",
    ")\n",
    "\n",
    "# Standardization (Z-score normalization) - mean=0, std=1\n",
    "scaler_standard = StandardScaler()\n",
    "numeric_df_standard = pd.DataFrame(\n",
    "    scaler_standard.fit_transform(numeric_df),\n",
    "    columns=[col + '_standard' for col in numeric_df.columns],\n",
    "    index=numeric_df.index\n",
    ")\n",
    "\n",
    "# Robust Scaling - scales using median and quantiles, robust to outliers\n",
    "scaler_robust = RobustScaler()\n",
    "numeric_df_robust = pd.DataFrame(\n",
    "    scaler_robust.fit_transform(numeric_df),\n",
    "    columns=[col + '_robust' for col in numeric_df.columns],\n",
    "    index=numeric_df.index\n",
    ")\n",
    "\n",
    "# Manual scaling methods\n",
    "numeric_df_manual = numeric_df.copy()\n",
    "# Manual min-max scaling\n",
    "numeric_df_manual['age_manual_minmax'] = (numeric_df_manual['age'] - numeric_df_manual['age'].min()) / (numeric_df_manual['age'].max() - numeric_df_manual['age'].min())\n",
    "# Manual standardization\n",
    "numeric_df_manual['income_manual_standard'] = (numeric_df_manual['income'] - numeric_df_manual['income'].mean()) / numeric_df_manual['income'].std()\n",
    "# Log transformation and then scaling\n",
    "numeric_df_manual['income_log_scaled'] = np.log1p(numeric_df_manual['income'])  # log1p avoids issues with zero values\n",
    "numeric_df_manual['income_log_scaled'] = (numeric_df_manual['income_log_scaled'] - numeric_df_manual['income_log_scaled'].min()) / (numeric_df_manual['income_log_scaled'].max() - numeric_df_manual['income_log_scaled'].min())\n",
    "\n",
    "# Combine all scaled dataframes\n",
    "all_scaled = pd.concat([numeric_df, numeric_df_minmax, numeric_df_standard, numeric_df_robust, \n",
    "                       numeric_df_manual[['age_manual_minmax', 'income_manual_standard', 'income_log_scaled']]], axis=1)\n",
    "\n",
    "# Display the results\n",
    "print(\"Scaling and normalization results:\")\n",
    "all_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8c4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the effect of different scaling methods on the income feature\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.histplot(numeric_df['income'], kde=True)\n",
    "plt.title('Original Income Distribution')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.histplot(numeric_df_minmax['income_minmax'], kde=True)\n",
    "plt.title('Min-Max Scaled Income')\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(numeric_df_standard['income_standard'], kde=True)\n",
    "plt.title('Standardized Income')\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.histplot(numeric_df_robust['income_robust'], kde=True)\n",
    "plt.title('Robust Scaled Income')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize a comparison of all scaling methods for the income feature\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(all_scaled['income'], label='Original')\n",
    "plt.plot(all_scaled['income_minmax'], label='Min-Max')\n",
    "plt.plot(all_scaled['income_standard'], label='Standard')\n",
    "plt.plot(all_scaled['income_robust'], label='Robust')\n",
    "plt.plot(all_scaled['income_manual_standard'], label='Manual Standard')\n",
    "plt.plot(all_scaled['income_log_scaled'], label='Log Scaled')\n",
    "plt.title('Comparison of Scaling Methods for Income')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24834432",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored a variety of data transformation techniques:\n",
    "\n",
    "1. **Basic Data Transformations**: Adding/removing columns, renaming, changing data types\n",
    "2. **Mathematical Transformations**: Log, square root, polynomial transformations\n",
    "3. **String Transformations**: Text cleaning, pattern extraction, string manipulation\n",
    "4. **Categorical Data Transformations**: One-hot encoding, label encoding, ordinal encoding\n",
    "5. **Custom Transformations**: Using apply() and map() for complex transformations\n",
    "6. **Date and Time Transformations**: Extracting components, formatting, calculating differences\n",
    "7. **Scaling and Normalization**: Min-max scaling, standardization, robust scaling\n",
    "\n",
    "These transformations are essential steps in the data preprocessing pipeline and help prepare data for analysis and modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
