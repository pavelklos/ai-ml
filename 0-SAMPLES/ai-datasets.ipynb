{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b3a362",
   "metadata": {},
   "source": [
    "# Best Online Datasets for AI Domains\n",
    "\n",
    "This notebook explores valuable online datasets for different AI domains, including:\n",
    "- Data Science\n",
    "- Machine Learning\n",
    "- Data Visualization\n",
    "- Deep Learning\n",
    "- Python Programming\n",
    "\n",
    "We'll walk through dataset sources, examples, and how to access them for your projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d491f",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "First, let's install and import the necessary libraries for handling and exploring datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d969cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages if needed\n",
    "# Uncomment and run if required\n",
    "# !pip install pandas numpy matplotlib seaborn requests plotly kaggle scikit-learn tensorflow\n",
    "\n",
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import plotly.express as px\n",
    "\n",
    "# Configure displays\n",
    "pd.set_option('display.max_columns', None)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "print(\"Setup completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4908ab1f",
   "metadata": {},
   "source": [
    "## Data Science (DS) Datasets\n",
    "\n",
    "Data Science datasets are fundamental for statistical analysis, data cleaning, and exploratory data analysis. Here are some excellent sources and examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025328eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to display dataset sources\n",
    "def display_data_sources(domain, sources):\n",
    "    \"\"\"Display dataset sources for a specific AI domain\"\"\"\n",
    "    print(f\"=== {domain} Dataset Sources ===\")\n",
    "    for source in sources:\n",
    "        print(f\"- {source['name']}: {source['url']}\")\n",
    "        print(f\"  Description: {source['description']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Data Science dataset sources\n",
    "ds_sources = [\n",
    "    {\n",
    "        \"name\": \"Kaggle\",\n",
    "        \"url\": \"https://www.kaggle.com/datasets\",\n",
    "        \"description\": \"Thousands of datasets for various data science problems.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"UCI Machine Learning Repository\",\n",
    "        \"url\": \"https://archive.ics.uci.edu/ml/index.php\",\n",
    "        \"description\": \"Collection of databases, domain theories, and data generators used for empirical analysis of machine learning algorithms.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"data.gov\",\n",
    "        \"url\": \"https://www.data.gov/\",\n",
    "        \"description\": \"US government's open data portal with over 200,000 datasets.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Google Dataset Search\",\n",
    "        \"url\": \"https://datasetsearch.research.google.com/\",\n",
    "        \"description\": \"Search engine for datasets.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"AWS Open Data Registry\",\n",
    "        \"url\": \"https://registry.opendata.aws/\",\n",
    "        \"description\": \"Public datasets available through AWS resources.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "display_data_sources(\"Data Science\", ds_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18eb74c0",
   "metadata": {},
   "source": [
    "### Example: Exploring a Data Science Dataset\n",
    "\n",
    "Let's explore the Iris dataset, a classic dataset for data science practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset from seaborn\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = [iris.target_names[i] for i in iris.target]\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Iris Dataset Preview:\")\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic exploratory data analysis\n",
    "print(\"Dataset Shape:\", iris_df.shape)\n",
    "print(\"\\nDataset Info:\")\n",
    "iris_df.info()\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "iris_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of the Iris dataset\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Create a scatter plot\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.scatterplot(x='sepal length (cm)', y='sepal width (cm)', \n",
    "                hue='species', data=iris_df)\n",
    "plt.title('Sepal Dimensions by Species')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.scatterplot(x='petal length (cm)', y='petal width (cm)', \n",
    "                hue='species', data=iris_df)\n",
    "plt.title('Petal Dimensions by Species')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383e7cd1",
   "metadata": {},
   "source": [
    "## Machine Learning (ML) Datasets\n",
    "\n",
    "Machine Learning datasets are designed for training and testing ML models. These datasets are suitable for classification, regression, and clustering problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e75f428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning dataset sources\n",
    "ml_sources = [\n",
    "    {\n",
    "        \"name\": \"Scikit-learn built-in datasets\",\n",
    "        \"url\": \"https://scikit-learn.org/stable/datasets/index.html\",\n",
    "        \"description\": \"Clean datasets for classification, regression, clustering, and manifold learning.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"OpenML\",\n",
    "        \"url\": \"https://www.openml.org/\",\n",
    "        \"description\": \"Open platform for sharing ML datasets, tasks, and experiments.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"TensorFlow Datasets\",\n",
    "        \"url\": \"https://www.tensorflow.org/datasets\",\n",
    "        \"description\": \"Collection of ready-to-use datasets for ML research.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Quandl\",\n",
    "        \"url\": \"https://www.quandl.com/\",\n",
    "        \"description\": \"Financial, economic, and alternative datasets for ML in finance.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MLData.org\",\n",
    "        \"url\": \"http://mldata.org/\",\n",
    "        \"description\": \"Repository for machine learning data.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "display_data_sources(\"Machine Learning\", ml_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc2f22d",
   "metadata": {},
   "source": [
    "### Example: Exploring a Machine Learning Dataset\n",
    "\n",
    "Let's explore the Boston Housing dataset, commonly used for regression problems:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75873441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston Housing dataset\n",
    "from sklearn.datasets import load_boston\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  # Suppress warnings\n",
    "\n",
    "# Note: The Boston housing dataset has been removed from scikit-learn due to ethical concerns\n",
    "# We're still including it for educational purposes\n",
    "try:\n",
    "    boston = load_boston()\n",
    "    boston_df = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "    boston_df['PRICE'] = boston.target\n",
    "\n",
    "    # Display dataset info\n",
    "    print(\"Boston Housing Dataset - First 5 rows:\")\n",
    "    print(boston_df.head())\n",
    "    print(\"\\nFeatures description:\")\n",
    "    print(boston.DESCR[:1000] + \"...\")  # Display partial description\n",
    "    \n",
    "except:\n",
    "    print(\"The Boston Housing dataset is no longer available in scikit-learn.\")\n",
    "    print(\"Let's use the California Housing dataset instead.\")\n",
    "    \n",
    "    from sklearn.datasets import fetch_california_housing\n",
    "    california = fetch_california_housing()\n",
    "    california_df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "    california_df['PRICE'] = california.target\n",
    "    \n",
    "    # Display dataset info\n",
    "    print(\"California Housing Dataset - First 5 rows:\")\n",
    "    print(california_df.head())\n",
    "    print(\"\\nFeatures description:\")\n",
    "    print(california.DESCR[:1000] + \"...\")  # Display partial description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb50d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic analysis of housing dataset\n",
    "try:\n",
    "    # For Boston dataset\n",
    "    housing_df = boston_df\n",
    "    target_feature = 'PRICE'\n",
    "except:\n",
    "    # For California dataset\n",
    "    housing_df = california_df\n",
    "    target_feature = 'PRICE'\n",
    "\n",
    "# Statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13588d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = housing_df.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of most correlated feature with target\n",
    "most_correlated = correlation_matrix[target_feature].drop(target_feature).abs().idxmax()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.regplot(x=most_correlated, y=target_feature, data=housing_df)\n",
    "plt.title(f'Relationship between {most_correlated} and {target_feature}')\n",
    "plt.xlabel(most_correlated)\n",
    "plt.ylabel(target_feature)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248cc85",
   "metadata": {},
   "source": [
    "## Data Visualization (DV) Datasets\n",
    "\n",
    "These datasets are particularly suited for creating compelling visualizations with temporal, geographical, and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd23f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization dataset sources\n",
    "dv_sources = [\n",
    "    {\n",
    "        \"name\": \"Gapminder\",\n",
    "        \"url\": \"https://www.gapminder.org/data/\",\n",
    "        \"description\": \"Data on global development trends, perfect for temporal and geographical visualizations.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Our World in Data\",\n",
    "        \"url\": \"https://ourworldindata.org/\",\n",
    "        \"description\": \"Research and data on global problems and their solutions.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"FiveThirtyEight\",\n",
    "        \"url\": \"https://github.com/fivethirtyeight/data\",\n",
    "        \"description\": \"Data and code behind FiveThirtyEight articles and graphics.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"The Pudding\",\n",
    "        \"url\": \"https://github.com/the-pudding/data\",\n",
    "        \"description\": \"Datasets from visual essays on The Pudding.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tableau Public Sample Data\",\n",
    "        \"url\": \"https://public.tableau.com/en-us/s/resources\",\n",
    "        \"description\": \"Curated datasets for creating visualizations.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "display_data_sources(\"Data Visualization\", dv_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db0cab",
   "metadata": {},
   "source": [
    "### Example: Exploring a Data Visualization Dataset\n",
    "\n",
    "Let's explore the Gapminder dataset, which is excellent for creating visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71fc6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the gapminder dataset from plotly express\n",
    "try:\n",
    "    gapminder = px.data.gapminder()\n",
    "    print(\"Gapminder Dataset - First 5 rows:\")\n",
    "    display(gapminder.head())\n",
    "except:\n",
    "    # Alternative approach if plotly's built-in dataset is not available\n",
    "    !pip install -q plotly\n",
    "    import plotly.express as px\n",
    "    gapminder = px.data.gapminder()\n",
    "    print(\"Gapminder Dataset - First 5 rows:\")\n",
    "    display(gapminder.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dda8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the Gapminder dataset\n",
    "print(\"Dataset Shape:\", gapminder.shape)\n",
    "print(\"\\nCountries in the dataset:\", gapminder['country'].nunique())\n",
    "print(\"Years in the dataset:\", gapminder['year'].unique())\n",
    "print(\"Continents in the dataset:\", gapminder['continent'].unique())\n",
    "\n",
    "# Summary statistics\n",
    "gapminder.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e83949e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations with the Gapminder dataset\n",
    "\n",
    "# 1. GDP per capita vs life expectancy for 2007, colored by continent\n",
    "fig1 = px.scatter(\n",
    "    gapminder[gapminder['year'] == 2007], \n",
    "    x=\"gdpPercap\", y=\"lifeExp\", \n",
    "    size=\"pop\", color=\"continent\",\n",
    "    hover_name=\"country\", log_x=True,\n",
    "    size_max=60,\n",
    "    title=\"GDP per capita vs Life Expectancy (2007)\"\n",
    ")\n",
    "fig1.show()\n",
    "\n",
    "# 2. Population growth over time for each continent\n",
    "fig2 = px.line(\n",
    "    gapminder.groupby(['year', 'continent'])['pop'].sum().reset_index(),\n",
    "    x=\"year\", y=\"pop\", color=\"continent\",\n",
    "    title=\"Population Growth by Continent (1952-2007)\"\n",
    ")\n",
    "fig2.show()\n",
    "\n",
    "# 3. Life expectancy over time by continent\n",
    "fig3 = px.box(\n",
    "    gapminder, \n",
    "    x=\"year\", y=\"lifeExp\", color=\"continent\", \n",
    "    notched=True,\n",
    "    title=\"Life Expectancy Distribution by Continent (1952-2007)\"\n",
    ")\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e969b",
   "metadata": {},
   "source": [
    "## Deep Learning (DL) Datasets\n",
    "\n",
    "These datasets are specialized for training deep learning models, particularly in computer vision, natural language processing, and speech recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f128127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep Learning dataset sources\n",
    "dl_sources = [\n",
    "    {\n",
    "        \"name\": \"TensorFlow Datasets\",\n",
    "        \"url\": \"https://www.tensorflow.org/datasets/catalog/overview\",\n",
    "        \"description\": \"Collection of datasets ready to use with TensorFlow.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Hugging Face Datasets\",\n",
    "        \"url\": \"https://huggingface.co/datasets\",\n",
    "        \"description\": \"Datasets for NLP tasks like text classification, question answering, etc.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"ImageNet\",\n",
    "        \"url\": \"https://www.image-net.org/\",\n",
    "        \"description\": \"Image database organized according to the WordNet hierarchy.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"COCO (Common Objects in Context)\",\n",
    "        \"url\": \"https://cocodataset.org/\",\n",
    "        \"description\": \"Dataset for object detection, segmentation, and captioning.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"AudioSet\",\n",
    "        \"url\": \"https://research.google.com/audioset/\",\n",
    "        \"description\": \"Large-scale dataset of manually annotated audio events.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "display_data_sources(\"Deep Learning\", dl_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4204af",
   "metadata": {},
   "source": [
    "### Example: Exploring a Deep Learning Dataset\n",
    "\n",
    "Let's explore MNIST, a classic dataset for deep learning image classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d93718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "try:\n",
    "    from tensorflow.keras.datasets import mnist\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    print(\"MNIST Dataset loaded successfully\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")\n",
    "    \n",
    "    # If TensorFlow is not available, let's try to use sklearn's version\n",
    "except ImportError:\n",
    "    print(\"TensorFlow not available, loading MNIST from scikit-learn...\")\n",
    "    from sklearn.datasets import fetch_openml\n",
    "    \n",
    "    # Load data from OpenML\n",
    "    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, parser='auto')\n",
    "    X = np.array(X).reshape(-1, 28, 28)\n",
    "    y = np.array(y, dtype=int)\n",
    "    \n",
    "    # Split the data\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(\"MNIST Dataset loaded successfully from scikit-learn\")\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e668fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some examples from the MNIST dataset\n",
    "plt.figure(figsize=(10, 5))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[i]}\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle('MNIST Examples', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print class distribution\n",
    "unique_values, counts = np.unique(y_train, return_counts=True)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(unique_values, counts)\n",
    "plt.xlabel('Digit')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Digits in MNIST Training Set')\n",
    "plt.xticks(unique_values)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903fa79f",
   "metadata": {},
   "source": [
    "## Python Programming (PY) Datasets\n",
    "\n",
    "These datasets are suitable for Python programming practice and learning data manipulation techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2bf6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Programming friendly dataset sources\n",
    "py_sources = [\n",
    "    {\n",
    "        \"name\": \"Python's built-in datasets\",\n",
    "        \"url\": \"https://docs.python.org/3/library/csv.html\",\n",
    "        \"description\": \"Simple datasets that come with Python libraries like csv, sqlite3, etc.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"GitHub Repositories\",\n",
    "        \"url\": \"https://github.com/awesomedata/awesome-public-datasets\",\n",
    "        \"description\": \"Curated list of public datasets organized by topic.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Public APIs\",\n",
    "        \"url\": \"https://github.com/public-apis/public-apis\",\n",
    "        \"description\": \"A collective list of free APIs for use in software and web development.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Pandas Documentation Datasets\",\n",
    "        \"url\": \"https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\",\n",
    "        \"description\": \"Datasets used in Pandas documentation examples.\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Seaborn Example Datasets\",\n",
    "        \"url\": \"https://seaborn.pydata.org/generated/seaborn.load_dataset.html\",\n",
    "        \"description\": \"Datasets built into the Seaborn visualization library.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "display_data_sources(\"Python Programming\", py_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43137332",
   "metadata": {},
   "source": [
    "### Example: Exploring a Python Programming Dataset\n",
    "\n",
    "Let's explore a dataset from Seaborn that's excellent for Python programming practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b50be54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Titanic dataset from Seaborn\n",
    "try:\n",
    "    titanic = sns.load_dataset('titanic')\n",
    "    print(\"Titanic Dataset loaded successfully\")\n",
    "except:\n",
    "    # Alternative: download directly from GitHub if seaborn is not available\n",
    "    url = \"https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv\"\n",
    "    titanic = pd.read_csv(url)\n",
    "    print(\"Titanic Dataset loaded successfully from GitHub\")\n",
    "\n",
    "# Display the first few rows\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f69ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic information about the Titanic dataset\n",
    "print(\"Dataset Shape:\", titanic.shape)\n",
    "print(\"\\nBasic Information:\")\n",
    "titanic.info()\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(titanic.isna().sum())\n",
    "\n",
    "print(\"\\nSurvival Rate:\")\n",
    "survival_rate = titanic['survived'].mean() * 100\n",
    "print(f\"{survival_rate:.2f}% of passengers survived\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12110640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python data manipulation examples with the Titanic dataset\n",
    "\n",
    "# Example 1: Groupby operations\n",
    "print(\"Survival Rate by Class:\")\n",
    "class_survival = titanic.groupby('class')['survived'].mean().sort_values(ascending=False)\n",
    "print(class_survival)\n",
    "\n",
    "print(\"\\nSurvival Rate by Gender:\")\n",
    "gender_survival = titanic.groupby('sex')['survived'].mean()\n",
    "print(gender_survival)\n",
    "\n",
    "print(\"\\nSurvival Rate by Age Group:\")\n",
    "# Create age groups\n",
    "titanic['age_group'] = pd.cut(titanic['age'], bins=[0, 12, 18, 35, 60, 100], \n",
    "                             labels=['Child', 'Teenager', 'Young Adult', 'Adult', 'Senior'])\n",
    "age_survival = titanic.groupby('age_group')['survived'].mean().sort_values(ascending=False)\n",
    "print(age_survival)\n",
    "\n",
    "# Example 2: Data visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Survival by class\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.barplot(x='class', y='survived', data=titanic)\n",
    "plt.title('Survival Rate by Class')\n",
    "\n",
    "# Survival by gender\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.barplot(x='sex', y='survived', data=titanic)\n",
    "plt.title('Survival Rate by Gender')\n",
    "\n",
    "# Age distribution\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.histplot(data=titanic, x='age', hue='survived', bins=30, multiple='stack')\n",
    "plt.title('Age Distribution by Survival')\n",
    "\n",
    "# Fare vs Age with survival\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.scatterplot(x='age', y='fare', hue='survived', size='survived', \n",
    "                sizes={0: 20, 1: 60}, alpha=0.7, data=titanic)\n",
    "plt.title('Fare vs Age with Survival')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31b051c",
   "metadata": {},
   "source": [
    "## Accessing and Loading Datasets\n",
    "\n",
    "Different data sources require different methods for accessing and loading datasets. Let's explore common approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f1d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to demonstrate different ways to load datasets\n",
    "def demonstrate_data_loading():\n",
    "    print(\"=== METHODS FOR LOADING DATASETS ===\\n\")\n",
    "    \n",
    "    # Method 1: Loading data from built-in libraries\n",
    "    print(\"1. From Built-in Libraries:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Scikit-learn\")\n",
    "    print(\"from sklearn.datasets import load_iris\")\n",
    "    print(\"iris = load_iris()\")\n",
    "    print(\"iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\")\n",
    "    print(\"\\n# Seaborn\")\n",
    "    print(\"import seaborn as sns\")\n",
    "    print(\"titanic = sns.load_dataset('titanic')\")\n",
    "    print(\"```\\n\")\n",
    "    \n",
    "    # Method 2: Loading from URLs\n",
    "    print(\"2. From URLs:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# Pandas directly from URL\")\n",
    "    print(\"url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\")\n",
    "    print(\"df = pd.read_csv(url)\")\n",
    "    print(\"```\\n\")\n",
    "    \n",
    "    # Method 3: From APIs\n",
    "    print(\"3. From APIs:\")\n",
    "    print(\"```python\")\n",
    "    print(\"import requests\")\n",
    "    print(\"import json\")\n",
    "    print(\"\\n# Example: Open Notify API (ISS location)\")\n",
    "    print(\"response = requests.get('http://api.open-notify.org/iss-now.json')\")\n",
    "    print(\"data = response.json()\")\n",
    "    print(\"```\\n\")\n",
    "    \n",
    "    # Method 4: Kaggle API\n",
    "    print(\"4. From Kaggle API:\")\n",
    "    print(\"```python\")\n",
    "    print(\"# First, configure your Kaggle API credentials\")\n",
    "    print(\"# !kaggle datasets download -d USERNAME/DATASET-SLUG\")\n",
    "    print(\"\\n# Example:\")\n",
    "    print(\"import kaggle\")\n",
    "    print(\"kaggle.api.authenticate()\")\n",
    "    print(\"kaggle.api.dataset_download_files('ronitf/heart-disease-uci', path='.', unzip=True)\")\n",
    "    print(\"df = pd.read_csv('heart.csv')\")\n",
    "    print(\"```\\n\")\n",
    "    \n",
    "    # Method 5: TensorFlow datasets\n",
    "    print(\"5. From TensorFlow Datasets:\")\n",
    "    print(\"```python\")\n",
    "    print(\"import tensorflow_datasets as tfds\")\n",
    "    print(\"mnist, info = tfds.load('mnist', with_info=True, as_supervised=True)\")\n",
    "    print(\"train_dataset, test_dataset = mnist['train'], mnist['test']\")\n",
    "    print(\"```\\n\")\n",
    "    \n",
    "    # Method 6: Hugging Face datasets\n",
    "    print(\"6. From Hugging Face Datasets:\")\n",
    "    print(\"```python\")\n",
    "    print(\"from datasets import load_dataset\")\n",
    "    print(\"dataset = load_dataset('glue', 'sst2')\")\n",
    "    print(\"train_dataset = dataset['train']\")\n",
    "    print(\"```\\n\")\n",
    "    \n",
    "    # Method 7: Database connections\n",
    "    print(\"7. From Databases:\")\n",
    "    print(\"```python\")\n",
    "    print(\"import sqlite3\")\n",
    "    print(\"conn = sqlite3.connect('database.db')\")\n",
    "    print(\"df = pd.read_sql_query('SELECT * FROM table_name', conn)\")\n",
    "    print(\"conn.close()\")\n",
    "    print(\"```\\n\")\n",
    "\n",
    "demonstrate_data_loading()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37a5f8c",
   "metadata": {},
   "source": [
    "### Practical Example: Loading Data from an API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's demonstrate loading data from a public API\n",
    "try:\n",
    "    # Using Open-Meteo API to get weather data for New York\n",
    "    url = \"https://api.open-meteo.com/v1/forecast?latitude=40.71&longitude=-74.01&daily=temperature_2m_max,temperature_2m_min,precipitation_sum&timezone=America%2FNew_York\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    weather_data = response.json()\n",
    "    \n",
    "    # Convert to pandas dataframe\n",
    "    if 'daily' in weather_data:\n",
    "        daily_data = weather_data['daily']\n",
    "        weather_df = pd.DataFrame({\n",
    "            'date': daily_data['time'],\n",
    "            'max_temp': daily_data['temperature_2m_max'],\n",
    "            'min_temp': daily_data['temperature_2m_min'],\n",
    "            'precipitation': daily_data['precipitation_sum']\n",
    "        })\n",
    "        \n",
    "        print(\"New York Weather Forecast:\")\n",
    "        display(weather_df.head())\n",
    "        \n",
    "        # Simple visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(weather_df['date'], weather_df['max_temp'], 'r-', label='Max Temperature')\n",
    "        plt.plot(weather_df['date'], weather_df['min_temp'], 'b-', label='Min Temperature')\n",
    "        plt.fill_between(weather_df['date'], weather_df['min_temp'], weather_df['max_temp'], alpha=0.2)\n",
    "        plt.bar(weather_df['date'], weather_df['precipitation'], alpha=0.3, color='blue', label='Precipitation')\n",
    "        plt.title('New York Weather Forecast')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Temperature (°C) / Precipitation (mm)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Could not retrieve weather data.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing API: {e}\")\n",
    "    print(\"API connections may be restricted in this environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b1761f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has explored a variety of high-quality datasets for different AI domains:\n",
    "\n",
    "1. **Data Science Datasets**: We explored sources like Kaggle, UCI ML Repository, and data.gov for statistical analysis and EDA.\n",
    "\n",
    "2. **Machine Learning Datasets**: We examined datasets from scikit-learn, OpenML, and other sources for training ML models.\n",
    "\n",
    "3. **Data Visualization Datasets**: We used datasets from Gapminder and other sources that are excellent for creating compelling visualizations.\n",
    "\n",
    "4. **Deep Learning Datasets**: We reviewed specialized datasets for computer vision, NLP, and other deep learning applications.\n",
    "\n",
    "5. **Python Programming Datasets**: We explored datasets that are useful for practicing Python data manipulation skills.\n",
    "\n",
    "Additionally, we demonstrated various methods for accessing and loading datasets from different sources, from built-in libraries to APIs.\n",
    "\n",
    "These datasets and resources should provide a solid foundation for any AI, machine learning, or data science project you undertake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
