{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0116131b",
   "metadata": {},
   "source": [
    "# Data Science Project Framework\n",
    "A comprehensive framework for data science projects that includes all essential steps from data loading to evaluation and visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2966a8b",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b46450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries for data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Import machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    mean_squared_error, r2_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Common ML algorithms\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960165b7",
   "metadata": {},
   "source": [
    "## Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b59dd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "# Replace 'your_dataset.csv' with the actual file path\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# For demonstration purposes, let's use a sample dataset from sklearn\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target\n",
    "df['target_name'] = df['target'].map({i: name for i, name in enumerate(iris.target_names)})\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "display(df.head())\n",
    "\n",
    "# Basic information about the dataset\n",
    "print(\"\\nBasic information about the dataset:\")\n",
    "df.info()\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical summary:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "# Check the distribution of target variable (if applicable)\n",
    "print(\"\\nTarget variable distribution:\")\n",
    "if 'target' in df.columns:\n",
    "    display(df['target_name'].value_counts())\n",
    "    \n",
    "# Correlation matrix\n",
    "print(\"\\nCorrelation matrix:\")\n",
    "corr = df.select_dtypes(include=['float64', 'int64']).corr()\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2ca95d",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c85b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify numerical and categorical columns\n",
    "def identify_column_types(df):\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
    "    return numerical_cols, categorical_cols\n",
    "\n",
    "# Identify column types\n",
    "numerical_cols, categorical_cols = identify_column_types(df)\n",
    "print(f\"Numerical columns: {numerical_cols}\")\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Handle missing values if any\n",
    "# For numerical columns: impute with mean\n",
    "# For categorical columns: impute with most frequent value\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Define the target variable\n",
    "# For this example, we use 'target' from iris dataset\n",
    "X = df.drop(['target', 'target_name'], axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.25, random_state=42, stratify=y if 'target' in df.columns else None\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# If using OneHotEncoder, transform the sparse matrix to dense array if needed\n",
    "# X_train_processed = X_train_processed.toarray()\n",
    "# X_test_processed = X_test_processed.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9969a0",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of models to try (adapt based on your problem type)\n",
    "# Here we'll set up for a classification task assuming the iris dataset\n",
    "\n",
    "# Check the type of problem\n",
    "is_classification = len(np.unique(y)) < 10  # Arbitrary threshold\n",
    "problem_type = \"Classification\" if is_classification else \"Regression\"\n",
    "print(f\"Detected problem type: {problem_type}\")\n",
    "\n",
    "# Initialize models based on problem type\n",
    "if is_classification:\n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'SVM': SVC(probability=True, random_state=42),\n",
    "        'KNN': KNeighborsClassifier()\n",
    "    }\n",
    "else:\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(random_state=42),\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "        'SVR': SVR(),\n",
    "        'KNN': KNeighborsRegressor()\n",
    "    }\n",
    "\n",
    "print(f\"Models to be evaluated: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16c47ba",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb6b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model training and cross-validation function\n",
    "def train_and_evaluate_models(models, X_train, y_train, X_test, y_test, cv=5):\n",
    "    results = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        \n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(model, X_train, y_train, cv=cv)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'cv_scores': cv_scores,\n",
    "            'cv_mean': np.mean(cv_scores),\n",
    "            'cv_std': np.std(cv_scores),\n",
    "            'test_predictions': y_pred\n",
    "        }\n",
    "        \n",
    "        print(f\"{name} - Cross-validation Mean: {np.mean(cv_scores):.4f}, Std: {np.std(cv_scores):.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Train models\n",
    "cv_results = train_and_evaluate_models(models, X_train_processed, y_train, X_test_processed, y_test)\n",
    "\n",
    "# Find the best model based on cross-validation scores\n",
    "best_model_name = max(cv_results, key=lambda x: cv_results[x]['cv_mean'])\n",
    "print(f\"\\nBest model based on cross-validation: {best_model_name} with score {cv_results[best_model_name]['cv_mean']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa7324",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71d549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for model evaluation based on problem type\n",
    "def evaluate_model(y_true, y_pred, problem_type=\"Classification\"):\n",
    "    results = {}\n",
    "    \n",
    "    if problem_type == \"Classification\":\n",
    "        # Classification metrics\n",
    "        results['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "        \n",
    "        # For multi-class, use 'weighted' average\n",
    "        results['precision'] = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        results['recall'] = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        results['f1'] = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        results['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Classification report\n",
    "        results['classification_report'] = classification_report(y_true, y_pred)\n",
    "    else:\n",
    "        # Regression metrics\n",
    "        results['mse'] = mean_squared_error(y_true, y_pred)\n",
    "        results['rmse'] = np.sqrt(results['mse'])\n",
    "        results['r2'] = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Evaluate all models\n",
    "for name, result in cv_results.items():\n",
    "    print(f\"\\nEvaluating {name}:\")\n",
    "    eval_results = evaluate_model(y_test, result['test_predictions'], problem_type)\n",
    "    \n",
    "    if problem_type == \"Classification\":\n",
    "        print(f\"Accuracy: {eval_results['accuracy']:.4f}\")\n",
    "        print(f\"Precision: {eval_results['precision']:.4f}\")\n",
    "        print(f\"Recall: {eval_results['recall']:.4f}\")\n",
    "        print(f\"F1 Score: {eval_results['f1']:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(eval_results['confusion_matrix'])\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(eval_results['classification_report'])\n",
    "    else:\n",
    "        print(f\"MSE: {eval_results['mse']:.4f}\")\n",
    "        print(f\"RMSE: {eval_results['rmse']:.4f}\")\n",
    "        print(f\"R²: {eval_results['r2']:.4f}\")\n",
    "        \n",
    "    # Store evaluation results\n",
    "    cv_results[name]['evaluation'] = eval_results\n",
    "\n",
    "# Get the best model\n",
    "best_model = cv_results[best_model_name]['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31dff0",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62560f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cross-validation scores comparison\n",
    "cv_means = [result['cv_mean'] for result in cv_results.values()]\n",
    "cv_stds = [result['cv_std'] for result in cv_results.values()]\n",
    "model_names = list(cv_results.keys())\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.bar(model_names, cv_means, yerr=cv_stds, capsize=10)\n",
    "plt.title('Model Comparison: Cross-validation Scores')\n",
    "plt.ylabel('Mean CV Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    importances = best_model.feature_importances_\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.title(f'Feature Importances from {best_model_name}')\n",
    "    plt.bar(range(len(importances)), importances[indices], align='center')\n",
    "    plt.xticks(range(len(importances)), [feature_names[i] for i in indices], rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Confusion Matrix visualization (for classification)\n",
    "if problem_type == \"Classification\":\n",
    "    confusion_mat = cv_results[best_model_name]['evaluation']['confusion_matrix']\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=iris.target_names,\n",
    "                yticklabels=iris.target_names)\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4. ROC Curve (for binary classification)\n",
    "if problem_type == \"Classification\" and len(np.unique(y)) == 2:\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for name, result in cv_results.items():\n",
    "        if hasattr(result['model'], 'predict_proba'):\n",
    "            # Get probabilities for positive class\n",
    "            y_score = result['model'].predict_proba(X_test_processed)[:, 1]\n",
    "            fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            plt.plot(fpr, tpr, lw=2, label=f'{name} (AUC = {roc_auc:.4f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# 5. Scatter plot of actual vs predicted (for regression)\n",
    "if problem_type == \"Regression\":\n",
    "    y_pred = best_model.predict(X_test_processed)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title(f'Actual vs. Predicted Values - {best_model_name}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Learning curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    best_model, X_train_processed, y_train, cv=5, n_jobs=-1, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10)\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "train_std = np.std(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std = np.std(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, color='blue', marker='o', markersize=5, label='Training score')\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.15, color='blue')\n",
    "plt.plot(train_sizes, test_mean, color='green', linestyle='--', marker='s', markersize=5, label='Validation score')\n",
    "plt.fill_between(train_sizes, test_mean + test_std, test_mean - test_std, alpha=0.15, color='green')\n",
    "plt.title(f'Learning Curve - {best_model_name}')\n",
    "plt.xlabel('Training Size')\n",
    "plt.ylabel('Score')\n",
    "plt.grid()\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e087071e",
   "metadata": {},
   "source": [
    "## Summary and Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30627da4",
   "metadata": {},
   "source": [
    "### Summary of Findings:\n",
    "\n",
    "- Best performing model: [Insert best model name with its performance metrics]\n",
    "- Key features: [Insert top features if feature importance was available]\n",
    "- Model evaluation: [Summarize key evaluation metrics]\n",
    "\n",
    "### Potential Next Steps:\n",
    "\n",
    "1. **Model Improvement**:\n",
    "   - Hyperparameter tuning\n",
    "   - Feature engineering\n",
    "   - Try ensemble methods or more advanced algorithms\n",
    "\n",
    "2. **Further Analysis**:\n",
    "   - Investigate specific misclassifications or prediction errors\n",
    "   - Perform deeper feature analysis\n",
    "   - Consider additional data sources\n",
    "\n",
    "3. **Deployment**:\n",
    "   - Serialize the model for production\n",
    "   - Create an API for model inference\n",
    "   - Set up monitoring for model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d602a5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model for later use\n",
    "import joblib\n",
    "\n",
    "# Uncomment to save the model\n",
    "# joblib.dump(best_model, 'best_model.pkl')\n",
    "# print(f\"Saved best model ({best_model_name}) to 'best_model.pkl'\")\n",
    "\n",
    "# Create a simple function to make new predictions\n",
    "def make_prediction(data, model=best_model, preprocessor=preprocessor):\n",
    "    \"\"\"\n",
    "    Make predictions on new data using the trained model\n",
    "    \n",
    "    Parameters:\n",
    "    data: pandas DataFrame - New data to predict\n",
    "    model: trained model object\n",
    "    preprocessor: fitted preprocessor\n",
    "    \n",
    "    Returns:\n",
    "    array of predictions\n",
    "    \"\"\"\n",
    "    # Preprocess the data\n",
    "    processed_data = preprocessor.transform(data)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(processed_data)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Example usage:\n",
    "# new_data = pd.DataFrame(...)\n",
    "# predictions = make_prediction(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
